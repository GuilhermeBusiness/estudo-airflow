[2024-02-10T18:22:31.582+0000] {processor.py:153} INFO - Started process (PID=110306) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:22:31.583+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:22:31.584+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:22:31.584+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:22:31.588+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:22:31.586+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 4
    from
        ^
SyntaxError: invalid syntax
[2024-02-10T18:22:31.589+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:22:31.611+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.035 seconds
[2024-02-10T18:22:32.203+0000] {processor.py:153} INFO - Started process (PID=110309) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:22:32.204+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:22:32.206+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:22:32.205+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:22:32.209+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:22:32.207+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 4
    from airflow
               ^
SyntaxError: invalid syntax
[2024-02-10T18:22:32.210+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:22:32.242+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.044 seconds
[2024-02-10T18:22:36.508+0000] {processor.py:153} INFO - Started process (PID=110341) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:22:36.509+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:22:36.510+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:22:36.510+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:22:36.512+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:22:36.511+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 4
    from airflow.providers.
                          ^
SyntaxError: invalid syntax
[2024-02-10T18:22:36.513+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:22:36.543+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.039 seconds
[2024-02-10T18:22:40.726+0000] {processor.py:153} INFO - Started process (PID=110361) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:22:40.727+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:22:40.727+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:22:40.727+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:22:40.731+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:22:40.730+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 4
    from airflow.providers.http.
                               ^
SyntaxError: invalid syntax
[2024-02-10T18:22:40.731+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:22:40.764+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.042 seconds
[2024-02-10T18:22:43.486+0000] {processor.py:153} INFO - Started process (PID=110370) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:22:43.487+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:22:43.488+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:22:43.488+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:22:43.491+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:22:43.490+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 4
    from airflow.providers.http.sensors.
                                       ^
SyntaxError: invalid syntax
[2024-02-10T18:22:43.491+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:22:43.518+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.038 seconds
[2024-02-10T18:22:45.319+0000] {processor.py:153} INFO - Started process (PID=110373) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:22:45.321+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:22:45.321+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:22:45.321+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:22:45.324+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:22:45.323+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 4
    from airflow.providers.http.sensors.http
                                           ^
SyntaxError: invalid syntax
[2024-02-10T18:22:45.324+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:22:45.348+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.035 seconds
[2024-02-10T18:22:49.099+0000] {processor.py:153} INFO - Started process (PID=110396) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:22:49.103+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:22:49.108+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:22:49.107+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:22:49.120+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:22:49.116+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 4
    from airflow.providers.http.sensors.http import
                                                   ^
SyntaxError: invalid syntax
[2024-02-10T18:22:49.122+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:22:49.184+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.102 seconds
[2024-02-10T18:22:50.903+0000] {processor.py:153} INFO - Started process (PID=110410) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:22:50.908+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:22:50.911+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:22:50.910+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:22:50.971+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:22:50.970+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 4, in <module>
    from airflow.providers.http.sensors.http import Htt
ImportError: cannot import name 'Htt' from 'airflow.providers.http.sensors.http' (/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/http/sensors/http.py)
[2024-02-10T18:22:50.972+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:22:51.010+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.121 seconds
[2024-02-10T18:22:51.938+0000] {processor.py:153} INFO - Started process (PID=110411) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:22:51.939+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:22:51.940+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:22:51.939+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:22:51.953+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:22:51.970+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.038 seconds
[2024-02-10T18:22:55.376+0000] {processor.py:153} INFO - Started process (PID=110434) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:22:55.377+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:22:55.379+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:22:55.379+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:22:55.383+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:22:55.381+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 5
    import
          ^
SyntaxError: invalid syntax
[2024-02-10T18:22:55.384+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:22:55.421+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.052 seconds
[2024-02-10T18:23:00.489+0000] {processor.py:153} INFO - Started process (PID=110446) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:23:00.491+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:23:00.498+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:00.498+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:00.530+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:00.583+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.101 seconds
[2024-02-10T18:23:02.157+0000] {processor.py:153} INFO - Started process (PID=110460) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:23:02.158+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:23:02.160+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:02.159+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:02.162+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:02.161+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 7
    dag =
         ^
SyntaxError: invalid syntax
[2024-02-10T18:23:02.163+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:02.192+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.042 seconds
[2024-02-10T18:23:05.010+0000] {processor.py:153} INFO - Started process (PID=110475) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:23:05.018+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:23:05.019+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:05.018+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:05.056+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:05.055+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 7, in <module>
    dag = DAG()
TypeError: __init__() missing 1 required positional argument: 'dag_id'
[2024-02-10T18:23:05.065+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:05.106+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.105 seconds
[2024-02-10T18:23:06.479+0000] {processor.py:153} INFO - Started process (PID=110481) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:23:06.480+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:23:06.482+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:06.481+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:06.517+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:06.509+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 7, in <module>
    dag = DAG('')
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 443, in __init__
    validate_key(dag_id)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/helpers.py", line 56, in validate_key
    f"The key {k!r} has to be made of alphanumeric characters, dashes, "
airflow.exceptions.AirflowException: The key '' has to be made of alphanumeric characters, dashes, dots and underscores exclusively
[2024-02-10T18:23:06.518+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:06.618+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.151 seconds
[2024-02-10T18:23:08.623+0000] {processor.py:153} INFO - Started process (PID=110494) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:23:08.624+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:23:08.626+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:08.625+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:08.659+0000] {processor.py:753} INFO - DAG(s) dict_keys(['h']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:08.917+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:08.917+0000] {manager.py:504} INFO - Created Permission View: can delete on DAG:h
[2024-02-10T18:23:08.966+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:08.965+0000] {manager.py:504} INFO - Created Permission View: can read on DAG:h
[2024-02-10T18:23:08.984+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:08.984+0000] {manager.py:504} INFO - Created Permission View: can edit on DAG:h
[2024-02-10T18:23:08.985+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:08.984+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:23:09.040+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:09.039+0000] {dag.py:2711} INFO - Creating ORM DAG for h
[2024-02-10T18:23:09.088+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:09.087+0000] {dag.py:3441} INFO - Setting next_dagrun for h to None, run_after=None
[2024-02-10T18:23:09.166+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.548 seconds
[2024-02-10T18:23:15.142+0000] {processor.py:153} INFO - Started process (PID=110512) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:23:15.152+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:23:15.154+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:15.153+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:15.198+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:15.439+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:15.439+0000] {manager.py:504} INFO - Created Permission View: can delete on DAG:httpsensor
[2024-02-10T18:23:15.472+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:15.472+0000] {manager.py:504} INFO - Created Permission View: can read on DAG:httpsensor
[2024-02-10T18:23:15.509+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:15.509+0000] {manager.py:504} INFO - Created Permission View: can edit on DAG:httpsensor
[2024-02-10T18:23:15.510+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:15.510+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:23:15.571+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:15.571+0000] {dag.py:2711} INFO - Creating ORM DAG for httpsensor
[2024-02-10T18:23:15.626+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:15.625+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:23:15.691+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.556 seconds
[2024-02-10T18:23:17.686+0000] {processor.py:153} INFO - Started process (PID=110525) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:23:17.687+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:23:17.689+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:17.688+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:17.720+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:17.718+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 7, in <module>
    dag = DAG('httpsensor',desc)
NameError: name 'desc' is not defined
[2024-02-10T18:23:17.720+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:17.774+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.108 seconds
[2024-02-10T18:23:19.966+0000] {processor.py:153} INFO - Started process (PID=110542) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:23:19.967+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:23:19.969+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:19.968+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:20.026+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:20.024+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 7, in <module>
    dag = DAG('httpsensor',descri)
NameError: name 'descri' is not defined
[2024-02-10T18:23:20.027+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:20.090+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.137 seconds
[2024-02-10T18:23:21.360+0000] {processor.py:153} INFO - Started process (PID=110552) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:23:21.362+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:23:21.363+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:21.363+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:21.368+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:21.366+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 7
    dag = DAG('httpsensor',description=)
                                       ^
SyntaxError: invalid syntax
[2024-02-10T18:23:21.368+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:21.426+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.077 seconds
[2024-02-10T18:23:24.442+0000] {processor.py:153} INFO - Started process (PID=110567) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:23:24.443+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:23:24.444+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:24.444+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:24.463+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:24.462+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 7, in <module>
    dag = DAG('httpsensor',description=htt)
NameError: name 'htt' is not defined
[2024-02-10T18:23:24.464+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:24.497+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.063 seconds
[2024-02-10T18:23:26.223+0000] {processor.py:153} INFO - Started process (PID=110573) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:23:26.225+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:23:26.226+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:26.226+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:26.268+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:26.289+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:26.289+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:23:26.345+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:26.344+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:23:26.686+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:23:26.696+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:26.686+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET last_parsed_time=%(last_parsed_time)s, description=%(description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 23, 26, 344544, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:23:26.702+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:26.697+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:23:26.704+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET last_parsed_time=%(last_parsed_time)s, description=%(description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 23, 26, 344544, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:23:32.785+0000] {processor.py:153} INFO - Started process (PID=110598) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:23:32.786+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:23:32.787+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:32.787+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:32.809+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:32.831+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:32.830+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:23:32.865+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:32.865+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:23:33.152+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:23:33.155+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:33.152+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET last_parsed_time=%(last_parsed_time)s, description=%(description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 23, 32, 865475, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:23:33.156+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:33.155+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:23:33.157+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET last_parsed_time=%(last_parsed_time)s, description=%(description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 23, 32, 865475, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:23:36.788+0000] {processor.py:153} INFO - Started process (PID=110628) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:23:36.789+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:23:36.791+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:36.791+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:36.829+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:36.856+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:36.856+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:23:36.905+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:36.905+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:23:37.231+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:23:37.236+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:37.232+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET last_parsed_time=%(last_parsed_time)s, description=%(description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 23, 36, 904563, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:23:37.236+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:37.236+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:23:37.238+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET last_parsed_time=%(last_parsed_time)s, description=%(description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 23, 36, 904563, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:23:39.278+0000] {processor.py:153} INFO - Started process (PID=110638) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:23:39.279+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:23:39.280+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:39.280+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:39.315+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:39.335+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:39.334+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:23:39.390+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:39.389+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:23:39.814+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:23:39.818+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:39.814+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET last_parsed_time=%(last_parsed_time)s, description=%(description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 23, 39, 389627, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:23:39.819+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:39.819+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:23:39.822+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET last_parsed_time=%(last_parsed_time)s, description=%(description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 23, 39, 389627, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:23:42.678+0000] {processor.py:153} INFO - Started process (PID=110654) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:23:42.679+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:23:42.680+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:42.680+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:42.696+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:42.694+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 9
    sche)
    ^
SyntaxError: positional argument follows keyword argument
[2024-02-10T18:23:42.696+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:42.758+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.100 seconds
[2024-02-10T18:23:47.311+0000] {processor.py:153} INFO - Started process (PID=110675) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:23:47.312+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:23:47.313+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:47.313+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:47.343+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:47.460+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:47.460+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:23:47.516+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:47.516+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:23:47.937+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:23:47.943+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:47.938+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 23, 47, 516001, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:23:47.943+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:47.943+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:23:47.945+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 23, 47, 516001, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:23:48.990+0000] {processor.py:153} INFO - Started process (PID=110689) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:23:48.991+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:23:48.993+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:48.993+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:49.029+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:49.143+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:49.143+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:23:49.212+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:49.212+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:23:49.246+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:23:49.249+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:49.247+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 23, 49, 211651, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:23:49.250+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:49.250+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:23:49.251+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 23, 49, 211651, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:23:50.881+0000] {processor.py:153} INFO - Started process (PID=110695) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:23:50.883+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:23:50.884+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:50.884+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:50.920+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:51.055+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:51.055+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:23:51.119+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:51.119+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:23:51.381+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:23:51.386+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:51.382+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 23, 51, 118918, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:23:51.387+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:51.386+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:23:51.388+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 23, 51, 118918, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:23:56.481+0000] {processor.py:153} INFO - Started process (PID=110720) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:23:56.482+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:23:56.483+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:56.483+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:56.486+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:56.485+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 10
    star)
    ^
SyntaxError: positional argument follows keyword argument
[2024-02-10T18:23:56.487+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:23:56.517+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.040 seconds
[2024-02-10T18:23:59.996+0000] {processor.py:153} INFO - Started process (PID=110730) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:23:59.998+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:24:00.000+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:23:59.999+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:00.019+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:00.017+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 10, in <module>
    start_date=date)
NameError: name 'date' is not defined
[2024-02-10T18:24:00.019+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:00.052+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.061 seconds
[2024-02-10T18:24:01.871+0000] {processor.py:153} INFO - Started process (PID=110744) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:24:01.872+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:24:01.874+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:01.873+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:01.895+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:01.892+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 10, in <module>
    start_date=datetime)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 467, in __init__
    tz = pendulum.instance(start_date, tz=tzinfo).timezone
  File "/home/airflow/.local/lib/python3.7/site-packages/pendulum/__init__.py", line 174, in instance
    raise ValueError("instance() only accepts datetime objects.")
ValueError: instance() only accepts datetime objects.
[2024-02-10T18:24:01.895+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:01.926+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.059 seconds
[2024-02-10T18:24:04.652+0000] {processor.py:153} INFO - Started process (PID=110759) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:24:04.654+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:24:04.655+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:04.655+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:04.678+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:04.676+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 10, in <module>
    start_date=datetime())
TypeError: function missing required argument 'year' (pos 1)
[2024-02-10T18:24:04.678+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:04.720+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.074 seconds
[2024-02-10T18:24:07.911+0000] {processor.py:153} INFO - Started process (PID=110782) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:24:07.912+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:24:07.913+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:07.913+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:07.944+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:08.023+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:08.023+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:24:08.063+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:08.063+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:24:08.410+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:24:08.414+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:08.410+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 24, 8, 63390, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:24:08.415+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:08.414+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:24:08.416+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 24, 8, 63390, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:24:10.465+0000] {processor.py:153} INFO - Started process (PID=110785) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:24:10.466+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:24:10.467+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:10.467+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:10.496+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:10.638+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:10.638+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:24:10.690+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:10.689+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:24:10.759+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:24:10.768+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:10.760+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 24, 10, 689580, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:24:10.769+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:10.769+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:24:10.770+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 24, 10, 689580, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:24:14.028+0000] {processor.py:153} INFO - Started process (PID=110793) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:24:14.029+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:24:14.030+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:14.030+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:14.035+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:14.032+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 11
    catchup=)
            ^
SyntaxError: invalid syntax
[2024-02-10T18:24:14.035+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:14.072+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.051 seconds
[2024-02-10T18:24:16.642+0000] {processor.py:153} INFO - Started process (PID=110818) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:24:16.643+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:24:16.644+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:16.644+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:16.678+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:16.803+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:16.803+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:24:16.854+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:16.854+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:24:16.962+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:24:16.965+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:16.962+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 24, 16, 854084, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:24:16.965+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:16.965+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:24:16.966+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 24, 16, 854084, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:24:18.707+0000] {processor.py:153} INFO - Started process (PID=110824) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:24:18.708+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:24:18.710+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:18.710+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:18.748+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:18.828+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:18.827+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:24:18.857+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:18.857+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:24:19.107+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:24:19.110+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:19.107+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 24, 18, 857263, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:24:19.110+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:19.110+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:24:19.111+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 24, 18, 857263, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:24:21.408+0000] {processor.py:153} INFO - Started process (PID=110844) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:24:21.409+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:24:21.410+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:21.410+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:21.431+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:21.429+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 13, in <module>
    che
NameError: name 'che' is not defined
[2024-02-10T18:24:21.431+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:21.456+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.053 seconds
[2024-02-10T18:24:27.273+0000] {processor.py:153} INFO - Started process (PID=110860) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:24:27.274+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:24:27.275+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:27.275+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:27.279+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:27.278+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 13
    check_api =
               ^
SyntaxError: invalid syntax
[2024-02-10T18:24:27.280+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:27.380+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.118 seconds
[2024-02-10T18:24:30.212+0000] {processor.py:153} INFO - Started process (PID=110875) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:24:30.224+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:24:30.226+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:30.225+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:30.291+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:30.507+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:30.507+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:24:30.599+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:30.598+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:24:30.730+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:24:30.735+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:30.731+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 24, 30, 598594, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:24:30.736+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:30.736+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:24:30.750+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 24, 30, 598594, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:24:34.622+0000] {processor.py:153} INFO - Started process (PID=110904) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:24:34.624+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:24:34.625+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:34.625+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:34.648+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:34.646+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 14, in <module>
    tas
NameError: name 'tas' is not defined
[2024-02-10T18:24:34.648+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:34.678+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.062 seconds
[2024-02-10T18:24:37.690+0000] {processor.py:153} INFO - Started process (PID=110914) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:24:37.691+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:24:37.693+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:37.693+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:37.718+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:37.716+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 14, in <module>
    task_id
NameError: name 'task_id' is not defined
[2024-02-10T18:24:37.718+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:37.743+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.065 seconds
[2024-02-10T18:24:41.112+0000] {processor.py:153} INFO - Started process (PID=110931) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:24:41.113+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:24:41.114+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:41.114+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:41.139+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:41.138+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 14, in <module>
    task_id=c
NameError: name 'c' is not defined
[2024-02-10T18:24:41.140+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:41.171+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.063 seconds
[2024-02-10T18:24:41.719+0000] {processor.py:153} INFO - Started process (PID=110938) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:24:41.720+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:24:41.721+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:41.721+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:41.725+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:41.724+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 15
    )
    ^
SyntaxError: invalid syntax
[2024-02-10T18:24:41.726+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:41.764+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.052 seconds
[2024-02-10T18:24:44.002+0000] {processor.py:153} INFO - Started process (PID=110942) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:24:44.003+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:24:44.004+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:44.004+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:44.034+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:44.030+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 14, in <module>
    task_id=""
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 377, in apply_defaults
    raise AirflowException(f"missing keyword argument {missing_args.pop()!r}")
airflow.exceptions.AirflowException: missing keyword argument 'endpoint'
[2024-02-10T18:24:44.034+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:44.070+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.073 seconds
[2024-02-10T18:24:46.412+0000] {processor.py:153} INFO - Started process (PID=110958) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:24:46.413+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:24:46.414+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:46.414+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:46.446+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:46.444+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 14, in <module>
    task_id="check"
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 377, in apply_defaults
    raise AirflowException(f"missing keyword argument {missing_args.pop()!r}")
airflow.exceptions.AirflowException: missing keyword argument 'endpoint'
[2024-02-10T18:24:46.448+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:46.483+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.075 seconds
[2024-02-10T18:24:49.853+0000] {processor.py:153} INFO - Started process (PID=110980) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:24:49.854+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:24:49.855+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:49.855+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:49.876+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:49.875+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 14, in <module>
    task_id="check_api",
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 377, in apply_defaults
    raise AirflowException(f"missing keyword argument {missing_args.pop()!r}")
airflow.exceptions.AirflowException: missing keyword argument 'endpoint'
[2024-02-10T18:24:49.877+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:49.905+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.058 seconds
[2024-02-10T18:24:52.973+0000] {processor.py:153} INFO - Started process (PID=110988) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:24:52.974+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:24:52.975+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:52.975+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:52.980+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:52.978+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 15
    htt
    ^
SyntaxError: positional argument follows keyword argument
[2024-02-10T18:24:52.980+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:53.018+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.051 seconds
[2024-02-10T18:24:55.742+0000] {processor.py:153} INFO - Started process (PID=111006) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:24:55.743+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:24:55.744+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:55.744+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:55.764+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:55.762+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 15, in <module>
    http_conn_id=''
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 377, in apply_defaults
    raise AirflowException(f"missing keyword argument {missing_args.pop()!r}")
airflow.exceptions.AirflowException: missing keyword argument 'endpoint'
[2024-02-10T18:24:55.765+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:55.792+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.056 seconds
[2024-02-10T18:24:57.763+0000] {processor.py:153} INFO - Started process (PID=111007) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:24:57.764+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:24:57.765+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:57.765+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:57.790+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:24:57.788+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 15, in <module>
    http_conn_id='c'
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 377, in apply_defaults
    raise AirflowException(f"missing keyword argument {missing_args.pop()!r}")
airflow.exceptions.AirflowException: missing keyword argument 'endpoint'
[2024-02-10T18:24:57.791+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:24:57.825+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.068 seconds
[2024-02-10T18:25:01.760+0000] {processor.py:153} INFO - Started process (PID=111030) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:25:01.762+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:25:01.763+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:01.763+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:01.794+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:01.792+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 15, in <module>
    http_conn_id='conn'
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 377, in apply_defaults
    raise AirflowException(f"missing keyword argument {missing_args.pop()!r}")
airflow.exceptions.AirflowException: missing keyword argument 'endpoint'
[2024-02-10T18:25:01.795+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:01.862+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.111 seconds
[2024-02-10T18:25:03.765+0000] {processor.py:153} INFO - Started process (PID=111045) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:25:03.766+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:25:03.768+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:03.767+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:03.796+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:03.793+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 15, in <module>
    http_conn_id='connection'
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 377, in apply_defaults
    raise AirflowException(f"missing keyword argument {missing_args.pop()!r}")
airflow.exceptions.AirflowException: missing keyword argument 'endpoint'
[2024-02-10T18:25:03.796+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:03.836+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.077 seconds
[2024-02-10T18:25:05.580+0000] {processor.py:153} INFO - Started process (PID=111050) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:25:05.581+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:25:05.582+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:05.582+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:05.617+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:05.614+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 15, in <module>
    http_conn_id='connection',
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 377, in apply_defaults
    raise AirflowException(f"missing keyword argument {missing_args.pop()!r}")
airflow.exceptions.AirflowException: missing keyword argument 'endpoint'
[2024-02-10T18:25:05.619+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:05.669+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.102 seconds
[2024-02-10T18:25:08.070+0000] {processor.py:153} INFO - Started process (PID=111060) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:25:08.074+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:25:08.079+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:08.078+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:08.088+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:08.084+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 17
    )
    ^
SyntaxError: invalid syntax
[2024-02-10T18:25:08.089+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:08.131+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.080 seconds
[2024-02-10T18:25:11.243+0000] {processor.py:153} INFO - Started process (PID=111072) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:25:11.244+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:25:11.245+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:11.245+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:11.283+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:11.419+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:11.419+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:25:11.473+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:11.472+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:25:11.992+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:25:11.997+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:11.993+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 25, 11, 472707, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:25:11.998+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:11.998+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:25:12.010+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 25, 11, 472707, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:25:13.285+0000] {processor.py:153} INFO - Started process (PID=111080) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:25:13.286+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:25:13.287+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:13.287+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:13.385+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:13.794+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:13.793+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:25:13.884+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:13.884+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:25:14.270+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:25:14.275+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:14.271+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 25, 13, 884301, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:25:14.276+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:14.276+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:25:14.278+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 25, 13, 884301, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:25:14.808+0000] {processor.py:153} INFO - Started process (PID=111088) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:25:14.809+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:25:14.810+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:14.810+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:14.840+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:14.966+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:14.965+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:25:14.997+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:14.997+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:25:15.245+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:25:15.250+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:15.246+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 25, 14, 997297, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:25:15.251+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:15.250+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:25:15.252+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 25, 14, 997297, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:25:17.170+0000] {processor.py:153} INFO - Started process (PID=111109) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:25:17.171+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:25:17.172+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:17.172+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:17.176+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:17.175+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 17
    pok
    ^
SyntaxError: positional argument follows keyword argument
[2024-02-10T18:25:17.177+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:17.208+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.046 seconds
[2024-02-10T18:25:18.178+0000] {processor.py:153} INFO - Started process (PID=111113) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:25:18.178+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:25:18.179+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:18.179+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:18.182+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:18.181+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 17
    poke
    ^
SyntaxError: positional argument follows keyword argument
[2024-02-10T18:25:18.183+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:18.212+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.041 seconds
[2024-02-10T18:25:20.604+0000] {processor.py:153} INFO - Started process (PID=111127) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:25:20.605+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:25:20.606+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:20.606+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:20.610+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:20.609+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 17
    poke_in
    ^
SyntaxError: positional argument follows keyword argument
[2024-02-10T18:25:20.611+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:20.647+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.050 seconds
[2024-02-10T18:25:23.555+0000] {processor.py:153} INFO - Started process (PID=111139) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:25:23.556+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:25:23.557+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:23.557+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:23.561+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:23.560+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 17
    poke_interval
    ^
SyntaxError: positional argument follows keyword argument
[2024-02-10T18:25:23.562+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:23.593+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.044 seconds
[2024-02-10T18:25:27.729+0000] {processor.py:153} INFO - Started process (PID=111159) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:25:27.730+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:25:27.732+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:27.731+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:27.736+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:27.734+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 18
    )
    ^
SyntaxError: invalid syntax
[2024-02-10T18:25:27.737+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:27.768+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.044 seconds
[2024-02-10T18:25:29.578+0000] {processor.py:153} INFO - Started process (PID=111167) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:25:29.579+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:25:29.580+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:29.580+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:29.609+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:29.691+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:29.691+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:25:29.730+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:29.730+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:25:29.783+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:25:29.795+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:29.784+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 25, 29, 730395, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:25:29.797+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:29.796+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:25:29.799+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 25, 29, 730395, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:25:32.475+0000] {processor.py:153} INFO - Started process (PID=111188) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:25:32.477+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:25:32.478+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:32.478+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:32.501+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:32.577+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:32.577+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:25:32.606+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:32.606+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:25:32.907+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:25:32.912+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:32.907+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 25, 32, 605960, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:25:32.913+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:32.913+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:25:32.914+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 25, 32, 605960, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:25:37.263+0000] {processor.py:153} INFO - Started process (PID=111205) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:25:37.264+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:25:37.265+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:37.265+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:37.269+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:37.268+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 18
    timeo
    ^
SyntaxError: positional argument follows keyword argument
[2024-02-10T18:25:37.270+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:37.305+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.052 seconds
[2024-02-10T18:25:38.710+0000] {processor.py:153} INFO - Started process (PID=111219) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:25:38.711+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:25:38.712+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:38.712+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:38.715+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:38.714+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 18
    timeout
    ^
SyntaxError: positional argument follows keyword argument
[2024-02-10T18:25:38.715+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:38.744+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.039 seconds
[2024-02-10T18:25:43.974+0000] {processor.py:153} INFO - Started process (PID=111237) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:25:43.976+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:25:43.976+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:43.976+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:43.979+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:43.978+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 18
    timeout
    ^
SyntaxError: positional argument follows keyword argument
[2024-02-10T18:25:43.979+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:44.004+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.035 seconds
[2024-02-10T18:25:48.088+0000] {processor.py:153} INFO - Started process (PID=111268) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:25:48.091+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:25:48.093+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:48.092+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:48.134+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:48.288+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:48.288+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:25:48.354+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:48.353+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:25:48.707+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:25:48.713+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:48.708+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 25, 48, 353672, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:25:48.714+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:48.714+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:25:48.716+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 25, 48, 353672, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:25:51.507+0000] {processor.py:153} INFO - Started process (PID=111282) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:25:51.508+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:25:51.509+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:51.508+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:51.512+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:51.511+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 19
    dag
    ^
SyntaxError: positional argument follows keyword argument
[2024-02-10T18:25:51.513+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:51.593+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.092 seconds
[2024-02-10T18:25:53.847+0000] {processor.py:153} INFO - Started process (PID=111294) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:25:53.848+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:25:53.849+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:53.849+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:53.924+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:25:54.179+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:54.179+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:25:54.246+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:54.245+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:25:54.700+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:25:54.717+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:54.701+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 25, 54, 245689, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:25:54.718+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:25:54.718+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:25:54.720+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 25, 54, 245689, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:26:00.359+0000] {processor.py:153} INFO - Started process (PID=111328) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:26:00.360+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:26:00.361+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:00.361+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:26:00.383+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:26:00.470+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:00.470+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:26:00.501+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:00.500+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:26:00.981+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:26:00.984+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:00.981+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 26, 0, 500641, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:26:00.985+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:00.985+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:26:00.986+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 26, 0, 500641, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:26:06.105+0000] {processor.py:153} INFO - Started process (PID=111356) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:26:06.106+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:26:06.107+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:06.106+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:26:06.134+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:26:06.232+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:06.232+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:26:06.271+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:06.271+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:26:06.322+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:26:06.326+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:06.322+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 26, 6, 270742, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:26:06.327+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:06.327+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:26:06.328+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 26, 6, 270742, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:26:11.723+0000] {processor.py:153} INFO - Started process (PID=111379) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:26:11.724+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:26:11.726+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:11.726+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:26:11.760+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:26:11.956+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:11.955+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:26:12.048+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:12.047+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:26:12.523+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:26:12.527+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:12.524+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 26, 12, 47609, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:26:12.541+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:12.541+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:26:12.543+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 26, 12, 47609, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:26:18.362+0000] {processor.py:153} INFO - Started process (PID=111416) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:26:18.363+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:26:18.364+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:18.364+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:26:18.388+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:26:18.482+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:18.482+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:26:18.513+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:18.513+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:26:18.856+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:26:18.859+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:18.856+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 26, 18, 513053, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:26:18.860+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:18.860+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:26:18.861+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 26, 18, 513053, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:26:23.931+0000] {processor.py:153} INFO - Started process (PID=111441) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:26:23.932+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:26:23.933+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:23.933+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:26:23.954+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:26:24.072+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:24.072+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:26:24.130+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:24.130+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:26:24.464+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:26:24.472+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:24.465+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 26, 24, 129811, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:26:24.476+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:24.476+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:26:24.478+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 26, 24, 129811, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:26:30.130+0000] {processor.py:153} INFO - Started process (PID=111478) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:26:30.131+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:26:30.132+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:30.132+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:26:30.159+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:26:30.241+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:30.241+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:26:30.272+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:30.272+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:26:30.534+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:26:30.537+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:30.534+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 26, 30, 272251, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:26:30.538+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:30.538+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:26:30.539+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 26, 30, 272251, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:26:35.876+0000] {processor.py:153} INFO - Started process (PID=111503) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:26:35.879+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:26:35.881+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:35.881+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:26:35.912+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:26:35.989+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:35.989+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:26:36.024+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:36.023+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:26:36.521+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:26:36.525+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:36.521+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 26, 36, 23776, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:26:36.526+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:36.526+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:26:36.527+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 26, 36, 23776, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:26:42.761+0000] {processor.py:153} INFO - Started process (PID=111537) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:26:42.762+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:26:42.764+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:42.763+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:26:42.814+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:26:42.991+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:42.990+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:26:43.062+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:43.062+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:26:43.453+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:26:43.467+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:43.457+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 26, 43, 61823, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:26:43.471+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:43.471+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:26:43.477+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 26, 43, 61823, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:26:48.732+0000] {processor.py:153} INFO - Started process (PID=111564) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:26:48.733+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:26:48.734+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:48.734+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:26:48.767+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:26:48.906+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:48.906+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:26:48.962+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:48.962+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:26:49.126+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:26:49.131+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:49.127+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 26, 48, 962074, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:26:49.132+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:49.131+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:26:49.133+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 26, 48, 962074, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:26:54.248+0000] {processor.py:153} INFO - Started process (PID=111580) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:26:54.269+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:26:54.270+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:54.270+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:26:54.386+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:26:54.944+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:54.944+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:26:55.123+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:55.122+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:26:55.627+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:26:55.647+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:55.627+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 26, 55, 122495, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:26:55.648+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:55.648+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:26:55.661+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 26, 55, 122495, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:26:58.375+0000] {processor.py:153} INFO - Started process (PID=111604) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:26:58.376+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:26:58.410+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:58.409+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:26:58.513+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:26:58.811+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:58.810+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:26:58.941+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:58.940+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:26:59.068+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:26:59.072+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:59.068+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 26, 58, 940575, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:26:59.081+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:26:59.081+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:26:59.083+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 26, 58, 940575, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:27:04.225+0000] {processor.py:153} INFO - Started process (PID=111629) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:27:04.226+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:27:04.227+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:04.227+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:27:04.296+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:27:04.582+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:04.582+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:27:04.738+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:04.738+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:27:04.839+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:27:04.844+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:04.840+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 27, 4, 737753, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:27:04.846+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:04.846+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:27:04.847+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 27, 4, 737753, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:27:11.381+0000] {processor.py:153} INFO - Started process (PID=111653) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:27:11.382+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:27:11.383+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:11.383+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:27:11.443+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:27:11.849+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:11.849+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:27:12.074+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:12.074+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:27:12.575+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:27:12.579+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:12.575+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 27, 12, 73934, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:27:12.584+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:12.584+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:27:12.591+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 27, 12, 73934, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:27:20.222+0000] {processor.py:153} INFO - Started process (PID=111686) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:27:20.224+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:27:20.227+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:20.226+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:27:20.265+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:27:20.422+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:20.421+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:27:20.475+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:20.475+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:27:20.891+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:27:20.896+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:20.892+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 27, 20, 474916, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:27:20.897+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:20.897+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:27:20.899+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 27, 20, 474916, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:27:26.936+0000] {processor.py:153} INFO - Started process (PID=111709) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:27:26.940+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:27:26.941+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:26.941+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:27:26.982+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:27:27.306+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:27.305+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:27:27.387+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:27.387+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:27:27.631+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:27:27.636+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:27.631+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 27, 27, 387356, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:27:27.637+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:27.637+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:27:27.638+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 27, 27, 387356, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:27:33.200+0000] {processor.py:153} INFO - Started process (PID=111731) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:27:33.201+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:27:33.203+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:33.202+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:27:33.268+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:27:33.592+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:33.592+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:27:33.732+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:33.732+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:27:34.240+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:27:34.253+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:34.240+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 27, 33, 732212, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:27:34.254+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:34.254+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:27:34.258+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 27, 33, 732212, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:27:39.928+0000] {processor.py:153} INFO - Started process (PID=111768) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:27:39.934+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:27:39.938+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:39.937+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:27:40.040+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:27:40.319+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:40.318+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:27:40.459+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:40.459+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:27:40.577+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:27:40.584+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:40.578+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 27, 40, 458700, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:27:40.585+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:40.585+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:27:40.587+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 27, 40, 458700, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:27:45.656+0000] {processor.py:153} INFO - Started process (PID=111792) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:27:45.657+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:27:45.658+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:45.658+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:27:45.688+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:27:45.859+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:45.859+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:27:45.928+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:45.928+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:27:45.998+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:27:46.002+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:45.998+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 27, 45, 928419, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:27:46.002+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:46.002+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:27:46.003+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 27, 45, 928419, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:27:51.784+0000] {processor.py:153} INFO - Started process (PID=111814) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:27:51.785+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:27:51.785+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:51.785+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:27:51.812+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:27:51.912+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:51.912+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:27:51.954+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:51.954+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:27:52.401+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:27:52.412+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:52.402+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 27, 51, 954251, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:27:52.414+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:52.414+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:27:52.418+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 27, 51, 954251, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:27:58.822+0000] {processor.py:153} INFO - Started process (PID=111851) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:27:58.823+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:27:58.825+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:58.824+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:27:58.897+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:27:59.116+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:59.116+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:27:59.180+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:59.180+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:27:59.646+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:27:59.666+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:59.661+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 27, 59, 179963, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:27:59.667+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:27:59.667+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:27:59.674+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 27, 59, 179963, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:28:05.136+0000] {processor.py:153} INFO - Started process (PID=111872) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:28:05.161+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:28:05.162+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:05.162+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:28:05.193+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:28:05.320+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:05.320+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:28:05.364+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:05.364+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:28:05.509+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:28:05.514+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:05.510+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 28, 5, 363769, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:28:05.515+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:05.515+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:28:05.516+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 28, 5, 363769, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:28:11.490+0000] {processor.py:153} INFO - Started process (PID=111906) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:28:11.491+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:28:11.493+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:11.492+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:28:11.532+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:28:11.728+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:11.728+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:28:11.787+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:11.787+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:28:12.283+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:28:12.288+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:12.283+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 28, 11, 787348, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:28:12.289+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:12.288+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:28:12.290+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 28, 11, 787348, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:28:17.790+0000] {processor.py:153} INFO - Started process (PID=111935) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:28:17.791+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:28:17.793+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:17.793+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:28:17.831+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:28:17.973+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:17.973+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:28:18.038+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:18.037+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:28:18.389+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:28:18.394+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:18.390+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 28, 18, 37383, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:28:18.394+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:18.394+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:28:18.396+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 28, 18, 37383, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:28:23.715+0000] {processor.py:153} INFO - Started process (PID=111960) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:28:23.716+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:28:23.718+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:23.718+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:28:23.749+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:28:23.878+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:23.878+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:28:23.918+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:23.918+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:28:24.282+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:28:24.287+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:24.283+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 28, 23, 918154, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:28:24.288+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:24.288+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:28:24.289+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 28, 23, 918154, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:28:29.841+0000] {processor.py:153} INFO - Started process (PID=111997) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:28:29.842+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:28:29.843+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:29.843+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:28:29.877+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:28:29.971+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:29.971+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:28:30.016+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:30.015+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:28:30.108+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:28:30.112+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:30.108+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 28, 30, 15378, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:28:30.113+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:30.112+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:28:30.114+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 28, 30, 15378, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:28:35.143+0000] {processor.py:153} INFO - Started process (PID=112017) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:28:35.143+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:28:35.144+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:35.144+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:28:35.166+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:28:35.252+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:35.252+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:28:35.289+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:35.289+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:28:35.447+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:28:35.453+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:35.447+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 28, 35, 289020, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:28:35.453+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:35.453+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:28:35.455+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 28, 35, 289020, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:28:40.715+0000] {processor.py:153} INFO - Started process (PID=112051) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:28:40.716+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:28:40.717+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:40.717+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:28:40.752+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:28:40.850+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:40.850+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:28:40.897+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:40.896+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:28:41.019+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:28:41.025+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:41.020+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 28, 40, 896477, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:28:41.026+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:41.026+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:28:41.028+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 28, 40, 896477, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:28:46.656+0000] {processor.py:153} INFO - Started process (PID=112077) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:28:46.666+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:28:46.667+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:46.667+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:28:46.723+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:46.721+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 24, in <module>
    process
NameError: name 'process' is not defined
[2024-02-10T18:28:46.723+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:28:46.765+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.120 seconds
[2024-02-10T18:28:48.078+0000] {processor.py:153} INFO - Started process (PID=112081) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:28:48.079+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:28:48.080+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:48.080+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:28:48.107+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:48.105+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 24, in <module>
    process_data
NameError: name 'process_data' is not defined
[2024-02-10T18:28:48.107+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:28:48.180+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.110 seconds
[2024-02-10T18:28:50.179+0000] {processor.py:153} INFO - Started process (PID=112091) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:28:50.180+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:28:50.182+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:50.181+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:28:50.216+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:50.214+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 24, in <module>
    process_data = Python
NameError: name 'Python' is not defined
[2024-02-10T18:28:50.220+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:28:50.273+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.099 seconds
[2024-02-10T18:28:51.770+0000] {processor.py:153} INFO - Started process (PID=112101) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:28:51.771+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:28:51.773+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:51.773+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:28:51.815+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:28:52.022+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:52.022+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:28:52.181+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:52.181+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:28:52.491+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:28:52.495+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:52.491+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 28, 52, 169297, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:28:52.496+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:52.496+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:28:52.498+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 28, 52, 169297, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:28:52.786+0000] {processor.py:153} INFO - Started process (PID=112105) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:28:52.787+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:28:52.789+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:52.788+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:28:52.815+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:52.813+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 24, in <module>
    process_data = PythonOperator()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 377, in apply_defaults
    raise AirflowException(f"missing keyword argument {missing_args.pop()!r}")
airflow.exceptions.AirflowException: missing keyword argument 'python_callable'
[2024-02-10T18:28:52.816+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:28:52.865+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.086 seconds
[2024-02-10T18:28:55.334+0000] {processor.py:153} INFO - Started process (PID=112113) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:28:55.335+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:28:55.336+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:55.336+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:28:55.362+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:55.361+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 24, in <module>
    process_data = PythonOperator(tas)
NameError: name 'tas' is not defined
[2024-02-10T18:28:55.363+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:28:55.413+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.086 seconds
[2024-02-10T18:28:56.639+0000] {processor.py:153} INFO - Started process (PID=112128) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:28:56.640+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:28:56.642+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:56.641+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:28:56.671+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:56.670+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 24, in <module>
    process_data = PythonOperator(task)
NameError: name 'task' is not defined
[2024-02-10T18:28:56.672+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:28:56.707+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.075 seconds
[2024-02-10T18:28:57.511+0000] {processor.py:153} INFO - Started process (PID=112136) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:28:57.512+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:28:57.514+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:57.514+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:28:57.541+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:28:57.540+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 24, in <module>
    process_data = PythonOperator(task_id)
NameError: name 'task_id' is not defined
[2024-02-10T18:28:57.542+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:28:57.576+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.072 seconds
[2024-02-10T18:29:00.203+0000] {processor.py:153} INFO - Started process (PID=112146) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:29:00.205+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:29:00.207+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:00.206+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:00.235+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:00.233+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 24, in <module>
    process_data = PythonOperator(task_id="")
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 377, in apply_defaults
    raise AirflowException(f"missing keyword argument {missing_args.pop()!r}")
airflow.exceptions.AirflowException: missing keyword argument 'python_callable'
[2024-02-10T18:29:00.236+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:00.268+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.073 seconds
[2024-02-10T18:29:02.167+0000] {processor.py:153} INFO - Started process (PID=112158) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:29:02.168+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:29:02.170+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:02.169+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:02.202+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:02.196+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 24, in <module>
    process_data = PythonOperator(task_id="proce")
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 377, in apply_defaults
    raise AirflowException(f"missing keyword argument {missing_args.pop()!r}")
airflow.exceptions.AirflowException: missing keyword argument 'python_callable'
[2024-02-10T18:29:02.203+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:02.235+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.074 seconds
[2024-02-10T18:29:05.009+0000] {processor.py:153} INFO - Started process (PID=112172) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:29:05.011+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:29:05.013+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:05.012+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:05.041+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:05.039+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 24, in <module>
    process_data = PythonOperator(task_id="process_data")
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 377, in apply_defaults
    raise AirflowException(f"missing keyword argument {missing_args.pop()!r}")
airflow.exceptions.AirflowException: missing keyword argument 'python_callable'
[2024-02-10T18:29:05.042+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:05.072+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.078 seconds
[2024-02-10T18:29:08.825+0000] {processor.py:153} INFO - Started process (PID=112182) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:29:08.827+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:29:08.828+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:08.828+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:08.833+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:08.831+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 24
    process_data = PythonOperator(task_id="process_data",python)
                                                        ^
SyntaxError: positional argument follows keyword argument
[2024-02-10T18:29:08.834+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:08.888+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.070 seconds
[2024-02-10T18:29:11.173+0000] {processor.py:153} INFO - Started process (PID=112203) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:29:11.174+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:29:11.175+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:11.175+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:11.181+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:11.180+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 24
    process_data = PythonOperator(task_id="process_data",python_call)
                                                        ^
SyntaxError: positional argument follows keyword argument
[2024-02-10T18:29:11.182+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:11.237+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.071 seconds
[2024-02-10T18:29:13.043+0000] {processor.py:153} INFO - Started process (PID=112214) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:29:13.045+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:29:13.045+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:13.045+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:13.048+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:13.047+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 24
    process_data = PythonOperator(task_id="process_data",python_callable)
                                                        ^
SyntaxError: positional argument follows keyword argument
[2024-02-10T18:29:13.049+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:13.073+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.035 seconds
[2024-02-10T18:29:18.808+0000] {processor.py:153} INFO - Started process (PID=112235) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:29:18.812+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:29:18.814+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:18.814+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:18.841+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:18.839+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 24, in <module>
    process_data = PythonOperator(task_id="process_data",python_callable=query)
NameError: name 'query' is not defined
[2024-02-10T18:29:18.841+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:18.894+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.099 seconds
[2024-02-10T18:29:20.316+0000] {processor.py:153} INFO - Started process (PID=112240) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:29:20.316+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:29:20.317+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:20.317+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:20.334+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:20.332+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 24, in <module>
    process_data = PythonOperator(task_id="process_data",python_callable=query_api)
NameError: name 'query_api' is not defined
[2024-02-10T18:29:20.335+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:20.362+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.051 seconds
[2024-02-10T18:29:26.209+0000] {processor.py:153} INFO - Started process (PID=112265) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:29:26.210+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:29:26.211+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:26.211+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:26.237+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:26.235+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 24, in <module>
    process_data = PythonOperator(task_id="process_data",python_callable=query_api, dag=dag)
NameError: name 'query_api' is not defined
[2024-02-10T18:29:26.237+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:26.268+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.066 seconds
[2024-02-10T18:29:28.277+0000] {processor.py:153} INFO - Started process (PID=112276) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:29:28.281+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:29:28.282+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:28.281+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:28.302+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:28.301+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 24, in <module>
    process_data = PythonOperator(task_id="process_data",python_callable=query_api,dag=dag)
NameError: name 'query_api' is not defined
[2024-02-10T18:29:28.302+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:28.326+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.055 seconds
[2024-02-10T18:29:30.497+0000] {processor.py:153} INFO - Started process (PID=112290) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:29:30.498+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:29:30.500+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:30.500+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:30.538+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:30.532+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 24, in <module>
    process_data = PythonOperator(task_id="process_data",python_callable=query_api,dag=dag)
NameError: name 'query_api' is not defined
[2024-02-10T18:29:30.539+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:30.576+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.087 seconds
[2024-02-10T18:29:36.377+0000] {processor.py:153} INFO - Started process (PID=112312) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:29:36.383+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:29:36.386+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:36.385+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:36.390+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:36.388+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 13
    def
       ^
SyntaxError: invalid syntax
[2024-02-10T18:29:36.391+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:36.446+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.077 seconds
[2024-02-10T18:29:40.101+0000] {processor.py:153} INFO - Started process (PID=112330) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:29:40.102+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:29:40.103+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:40.102+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:40.107+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:40.106+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 13
    def query_api
                ^
SyntaxError: invalid syntax
[2024-02-10T18:29:40.107+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:40.132+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.035 seconds
[2024-02-10T18:29:41.114+0000] {processor.py:153} INFO - Started process (PID=112331) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:29:41.115+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:29:41.117+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:41.116+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:41.120+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:41.118+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 13
    def query_api()
                  ^
SyntaxError: invalid syntax
[2024-02-10T18:29:41.120+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:41.146+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.040 seconds
[2024-02-10T18:29:42.597+0000] {processor.py:153} INFO - Started process (PID=112348) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:29:42.598+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:29:42.599+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:42.599+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:42.604+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:42.602+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 17
    check_api = HttpSensor(
            ^
IndentationError: expected an indented block
[2024-02-10T18:29:42.605+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:42.640+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.049 seconds
[2024-02-10T18:29:47.048+0000] {processor.py:153} INFO - Started process (PID=112371) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:29:47.050+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:29:47.051+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:47.051+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:47.084+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:47.213+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:47.212+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:29:47.272+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:47.272+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:29:47.736+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:29:47.739+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:47.736+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 29, 47, 271862, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:29:47.740+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:47.740+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:29:47.741+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 29, 47, 271862, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:29:50.275+0000] {processor.py:153} INFO - Started process (PID=112386) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:29:50.276+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:29:50.277+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:50.277+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:50.340+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:50.498+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:50.498+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:29:50.626+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:50.626+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:29:51.120+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:29:51.125+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:51.121+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 29, 50, 625798, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:29:51.126+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:51.126+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:29:51.135+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 29, 50, 625798, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:29:54.140+0000] {processor.py:153} INFO - Started process (PID=112397) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:29:54.143+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:29:54.146+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:54.146+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:54.192+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:54.340+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:54.340+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:29:54.396+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:54.396+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:29:54.480+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:29:54.483+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:54.480+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 29, 54, 395912, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:29:54.483+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:54.483+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:29:54.484+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 29, 54, 395912, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:29:55.309+0000] {processor.py:153} INFO - Started process (PID=112403) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:29:55.310+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:29:55.311+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:55.311+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:55.337+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:55.426+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:55.426+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:29:55.456+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:55.456+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:29:55.922+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:29:55.925+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:55.922+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 29, 55, 456518, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:29:55.926+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:55.926+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:29:55.927+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 29, 55, 456518, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:29:57.880+0000] {processor.py:153} INFO - Started process (PID=112430) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:29:57.881+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:29:57.881+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:57.881+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:57.911+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:29:58.000+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:58.000+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:29:58.038+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:58.038+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:29:58.450+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:29:58.453+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:58.451+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 29, 58, 38317, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:29:58.454+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:29:58.454+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:29:58.455+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 29, 58, 38317, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:30:01.978+0000] {processor.py:153} INFO - Started process (PID=112441) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:30:01.979+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:30:01.980+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:01.980+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:30:02.049+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:30:02.275+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:02.274+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:30:02.328+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:02.327+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:30:02.440+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:30:02.450+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:02.440+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 30, 2, 327398, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:30:02.457+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:02.457+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:30:02.459+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 30, 2, 327398, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:30:03.745+0000] {processor.py:153} INFO - Started process (PID=112451) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:30:03.748+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:30:03.749+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:03.749+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:30:03.784+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:30:03.989+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:03.988+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:30:04.048+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:04.048+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:30:04.482+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:30:04.487+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:04.482+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 30, 4, 47761, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:30:04.488+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:04.488+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:30:04.489+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 30, 4, 47761, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:30:05.586+0000] {processor.py:153} INFO - Started process (PID=112462) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:30:05.587+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:30:05.588+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:05.588+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:30:05.627+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:30:05.755+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:05.754+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:30:05.797+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:05.797+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:30:06.307+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:30:06.310+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:06.307+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 30, 5, 797111, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:30:06.310+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:06.310+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:30:06.311+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 30, 5, 797111, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:30:08.056+0000] {processor.py:153} INFO - Started process (PID=112468) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:30:08.057+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:30:08.058+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:08.058+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:30:08.081+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:30:08.163+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:08.162+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:30:08.200+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:08.200+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:30:08.354+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:30:08.366+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:08.355+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 30, 8, 200251, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:30:08.368+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:08.367+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:30:08.370+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 30, 8, 200251, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:30:12.189+0000] {processor.py:153} INFO - Started process (PID=112488) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:30:12.190+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:30:12.191+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:12.191+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:30:12.214+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:30:12.292+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:12.291+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:30:12.319+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:12.319+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:30:12.427+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:30:12.440+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:12.428+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 30, 12, 319643, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:30:12.442+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:12.442+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:30:12.445+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 30, 12, 319643, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:30:16.234+0000] {processor.py:153} INFO - Started process (PID=112517) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:30:16.261+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:30:16.262+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:16.262+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:30:16.316+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:30:16.477+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:16.477+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:30:16.532+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:16.532+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:30:16.787+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:30:16.792+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:16.788+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 30, 16, 532148, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:30:16.793+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:16.792+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:30:16.806+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 30, 16, 532148, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:30:21.908+0000] {processor.py:153} INFO - Started process (PID=112543) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:30:21.910+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:30:21.911+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:21.911+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:30:21.943+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:30:22.068+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:22.068+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:30:22.112+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:22.112+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:30:22.179+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:30:22.185+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:22.179+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 30, 22, 112036, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:30:22.186+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:22.186+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:30:22.188+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 30, 22, 112036, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:30:27.612+0000] {processor.py:153} INFO - Started process (PID=112564) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:30:27.613+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:30:27.614+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:27.614+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:30:27.638+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:30:27.755+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:27.755+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:30:27.790+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:27.790+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:30:27.970+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:30:27.975+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:27.970+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 30, 27, 790545, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:30:27.975+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:27.975+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:30:27.977+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 30, 27, 790545, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:30:34.074+0000] {processor.py:153} INFO - Started process (PID=112594) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:30:34.075+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:30:34.076+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:34.075+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:30:34.122+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:30:34.306+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:34.305+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:30:34.402+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:34.401+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:30:34.736+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:30:34.741+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:34.736+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 30, 34, 401699, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:30:34.741+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:34.741+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:30:34.750+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 30, 34, 401699, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:30:37.173+0000] {processor.py:153} INFO - Started process (PID=112607) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:30:37.174+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:30:37.174+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:37.174+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:30:37.199+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:30:37.294+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:37.294+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:30:37.345+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:37.344+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:30:37.779+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:30:37.784+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:37.780+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 30, 37, 344588, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:30:37.784+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:37.784+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:30:37.786+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 30, 37, 344588, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:30:39.489+0000] {processor.py:153} INFO - Started process (PID=112619) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:30:39.490+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:30:39.491+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:39.491+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:30:39.515+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:30:39.635+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:39.635+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:30:39.672+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:39.672+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:30:40.045+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:30:40.052+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:40.045+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 30, 39, 671810, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:30:40.054+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:40.054+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:30:40.056+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 30, 39, 671810, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:30:45.600+0000] {processor.py:153} INFO - Started process (PID=112644) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:30:45.601+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:30:45.603+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:45.602+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:30:45.634+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:30:45.728+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:45.728+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:30:45.790+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:45.790+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:30:46.106+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:30:46.122+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:46.106+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 30, 45, 790377, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:30:46.122+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:46.122+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:30:46.123+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 30, 45, 790377, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:30:50.520+0000] {processor.py:153} INFO - Started process (PID=112663) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:30:50.557+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:30:50.558+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:50.558+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:30:50.586+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:50.560+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 15
    print(response.)
                   ^
SyntaxError: invalid syntax
[2024-02-10T18:30:50.587+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:30:50.719+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.229 seconds
[2024-02-10T18:30:55.503+0000] {processor.py:153} INFO - Started process (PID=112686) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:30:55.504+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:30:55.506+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:55.505+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:30:55.548+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:30:55.745+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:55.745+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:30:55.814+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:55.814+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:30:56.343+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:30:56.347+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:56.343+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 30, 55, 814197, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:30:56.348+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:56.348+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:30:56.349+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 30, 55, 814197, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:30:57.278+0000] {processor.py:153} INFO - Started process (PID=112695) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:30:57.279+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:30:57.280+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:57.280+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:30:57.308+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:30:57.443+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:57.442+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:30:57.506+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:57.506+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:30:57.725+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:30:57.728+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:57.725+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 30, 57, 506145, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:30:57.729+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:30:57.728+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:30:57.730+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 30, 57, 506145, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:31:03.386+0000] {processor.py:153} INFO - Started process (PID=112721) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:31:03.387+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:31:03.388+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:03.388+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:31:03.420+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:31:03.575+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:03.574+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:31:03.634+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:03.634+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:31:03.937+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:31:03.944+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:03.938+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 31, 3, 634197, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:31:03.945+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:03.945+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:31:03.947+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 31, 3, 634197, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:31:09.863+0000] {processor.py:153} INFO - Started process (PID=112756) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:31:09.864+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:31:09.865+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:09.865+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:31:09.889+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:31:10.001+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:10.001+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:31:10.047+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:10.047+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:31:10.503+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:31:10.508+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:10.504+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 31, 10, 46972, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:31:10.509+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:10.509+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:31:10.510+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 31, 10, 46972, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:31:15.806+0000] {processor.py:153} INFO - Started process (PID=112783) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:31:15.807+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:31:15.809+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:15.808+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:31:15.843+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:31:15.954+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:15.953+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:31:15.998+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:15.997+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:31:16.060+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:31:16.064+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:16.060+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 31, 15, 997481, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:31:16.065+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:16.064+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:31:16.066+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 31, 15, 997481, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:31:21.226+0000] {processor.py:153} INFO - Started process (PID=112799) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:31:21.227+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:31:21.228+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:21.228+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:31:21.268+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:31:21.411+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:21.411+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:31:21.461+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:21.460+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:31:21.859+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:31:21.863+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:21.859+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 31, 21, 460583, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:31:21.863+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:21.863+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:31:21.864+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 31, 21, 460583, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:31:27.356+0000] {processor.py:153} INFO - Started process (PID=112835) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:31:27.357+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:31:27.358+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:27.358+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:31:27.380+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:31:27.464+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:27.464+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:31:27.497+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:27.497+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:31:27.911+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:31:27.921+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:27.912+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 31, 27, 496771, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:31:27.923+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:27.922+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:31:27.925+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 31, 27, 496771, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:31:33.716+0000] {processor.py:153} INFO - Started process (PID=112860) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:31:33.717+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:31:33.718+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:33.718+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:31:33.758+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:31:33.922+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:33.921+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:31:33.998+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:33.997+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:31:34.535+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:31:34.541+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:34.536+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 31, 33, 997452, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:31:34.544+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:34.543+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:31:34.549+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 31, 33, 997452, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:31:37.627+0000] {processor.py:153} INFO - Started process (PID=112887) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:31:37.628+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:31:37.629+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:37.629+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:31:37.676+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:31:37.814+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:37.814+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:31:37.866+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:37.866+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:31:38.216+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:31:38.224+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:38.216+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 31, 37, 865954, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:31:38.229+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:38.229+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:31:38.231+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 31, 37, 865954, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:31:40.297+0000] {processor.py:153} INFO - Started process (PID=112896) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:31:40.298+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:31:40.299+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:40.299+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:31:40.323+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:31:40.435+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:40.435+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:31:40.492+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:40.492+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:31:40.672+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:31:40.677+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:40.673+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 31, 40, 491878, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:31:40.678+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:40.677+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:31:40.870+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 31, 40, 491878, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:31:47.320+0000] {processor.py:153} INFO - Started process (PID=112921) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:31:47.329+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:31:47.331+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:47.330+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:31:47.418+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:31:47.683+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:47.683+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:31:47.779+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:47.779+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:31:48.511+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:31:48.514+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:48.511+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 31, 47, 779206, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:31:48.514+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:48.514+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:31:48.515+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 31, 47, 779206, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:31:53.763+0000] {processor.py:153} INFO - Started process (PID=112962) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:31:53.765+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:31:53.767+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:53.767+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:31:53.807+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:31:53.972+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:53.971+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:31:54.224+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:54.224+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:31:54.273+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:31:54.277+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:54.274+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 31, 54, 223802, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:31:54.278+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:31:54.277+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:31:54.279+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 31, 54, 223802, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:32:00.024+0000] {processor.py:153} INFO - Started process (PID=112985) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:32:00.025+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:32:00.026+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:00.026+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:32:00.060+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:32:00.189+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:00.189+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:32:00.225+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:00.225+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:32:00.715+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:32:00.720+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:00.716+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 32, 0, 225587, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:32:00.721+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:00.721+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:32:00.723+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 32, 0, 225587, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:32:05.927+0000] {processor.py:153} INFO - Started process (PID=113016) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:32:05.928+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:32:05.929+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:05.929+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:32:05.964+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:32:06.099+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:06.099+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:32:06.147+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:06.147+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:32:06.559+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:32:06.564+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:06.560+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 32, 6, 146781, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:32:06.571+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:06.570+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:32:06.573+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 32, 6, 146781, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:32:12.473+0000] {processor.py:153} INFO - Started process (PID=113047) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:32:12.474+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:32:12.476+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:12.475+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:32:12.506+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:32:12.594+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:12.593+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:32:12.630+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:12.629+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:32:13.059+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:32:13.064+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:13.059+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 32, 12, 629622, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:32:13.065+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:13.064+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:32:13.066+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 32, 12, 629622, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:32:18.124+0000] {processor.py:153} INFO - Started process (PID=113066) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:32:18.126+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:32:18.127+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:18.127+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:32:18.170+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:32:18.329+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:18.329+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:32:18.364+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:18.364+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:32:18.831+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:32:18.836+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:18.831+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 32, 18, 364407, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:32:18.836+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:18.836+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:32:18.838+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 32, 18, 364407, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:32:24.549+0000] {processor.py:153} INFO - Started process (PID=113103) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:32:24.550+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:32:24.551+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:24.551+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:32:24.576+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:32:24.706+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:24.706+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:32:24.737+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:24.737+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:32:25.096+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:32:25.101+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:25.097+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 32, 24, 737306, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:32:25.101+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:25.101+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:32:25.103+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 32, 24, 737306, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:32:30.593+0000] {processor.py:153} INFO - Started process (PID=113128) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:32:30.594+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:32:30.597+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:30.597+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:32:30.638+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:32:30.832+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:30.832+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:32:30.902+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:30.902+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:32:31.046+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:32:31.051+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:31.047+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 32, 30, 901914, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:32:31.052+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:31.052+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:32:31.053+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 32, 30, 901914, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:32:36.085+0000] {processor.py:153} INFO - Started process (PID=113165) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:32:36.086+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:32:36.087+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:36.087+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:32:36.111+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:32:36.199+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:36.199+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:32:36.228+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:36.228+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:32:36.451+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:32:36.456+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:36.452+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 32, 36, 228594, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:32:36.457+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:36.457+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:32:36.459+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 32, 36, 228594, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:32:41.507+0000] {processor.py:153} INFO - Started process (PID=113190) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:32:41.509+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:32:41.510+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:41.510+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:32:41.540+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:32:41.674+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:41.674+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:32:41.732+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:41.732+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:32:42.306+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:32:42.310+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:42.306+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 32, 41, 732438, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:32:42.311+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:42.310+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:32:42.312+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 32, 41, 732438, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:32:47.735+0000] {processor.py:153} INFO - Started process (PID=113223) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:32:47.736+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:32:47.738+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:47.737+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:32:47.766+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:32:47.908+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:47.907+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:32:47.958+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:47.958+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:32:48.210+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:32:48.213+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:48.211+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 32, 47, 958163, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:32:48.213+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:48.213+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:32:48.214+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 32, 47, 958163, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:32:53.297+0000] {processor.py:153} INFO - Started process (PID=113254) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:32:53.298+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:32:53.299+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:53.299+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:32:53.323+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:32:53.400+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:53.400+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:32:53.562+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:53.561+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:32:53.809+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:32:53.817+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:53.810+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 32, 53, 561793, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:32:53.818+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:53.818+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:32:53.821+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 32, 53, 561793, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:32:59.757+0000] {processor.py:153} INFO - Started process (PID=113275) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:32:59.758+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:32:59.759+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:59.759+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:32:59.791+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:32:59.928+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:59.927+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:32:59.989+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:32:59.989+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:33:00.301+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:33:00.305+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:00.301+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 32, 59, 988636, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:33:00.306+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:00.306+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:33:00.307+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 32, 59, 988636, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:33:04.687+0000] {processor.py:153} INFO - Started process (PID=113304) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:33:04.689+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:33:04.689+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:04.689+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:33:04.723+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:33:04.869+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:04.868+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:33:04.916+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:04.916+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:33:05.094+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:33:05.099+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:05.095+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 33, 4, 916044, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:33:05.101+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:05.100+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:33:05.103+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 33, 4, 916044, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:33:10.299+0000] {processor.py:153} INFO - Started process (PID=113328) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:33:10.300+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:33:10.301+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:10.301+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:33:10.333+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:33:10.476+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:10.476+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:33:10.525+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:10.524+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:33:11.012+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:33:11.017+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:11.013+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 33, 10, 524608, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:33:11.019+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:11.018+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:33:11.022+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 33, 10, 524608, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:33:16.755+0000] {processor.py:153} INFO - Started process (PID=113364) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:33:16.758+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:33:16.759+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:16.759+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:33:16.805+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:33:16.965+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:16.964+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:33:17.003+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:17.002+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:33:17.115+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:33:17.118+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:17.115+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 33, 17, 2641, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:33:17.119+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:17.119+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:33:17.120+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 33, 17, 2641, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:33:22.283+0000] {processor.py:153} INFO - Started process (PID=113388) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:33:22.285+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:33:22.286+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:22.286+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:33:22.319+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:33:22.409+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:22.408+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:33:22.449+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:22.449+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:33:22.912+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:33:22.916+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:22.912+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 33, 22, 448824, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:33:22.916+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:22.916+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:33:22.917+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 33, 22, 448824, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:33:27.989+0000] {processor.py:153} INFO - Started process (PID=113410) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:33:27.990+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:33:27.991+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:27.991+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:33:28.070+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:33:28.233+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:28.232+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:33:28.298+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:28.297+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:33:28.571+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:33:28.575+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:28.571+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 33, 28, 297605, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:33:28.577+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:28.576+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:33:28.578+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 33, 28, 297605, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:33:33.857+0000] {processor.py:153} INFO - Started process (PID=113442) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:33:33.859+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:33:33.860+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:33.860+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:33:33.918+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:33:34.073+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:34.073+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:33:34.148+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:34.148+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:33:34.309+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:33:34.312+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:34.309+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 33, 34, 148153, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:33:34.312+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:34.312+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:33:34.313+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 33, 34, 148153, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:33:39.433+0000] {processor.py:153} INFO - Started process (PID=113467) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:33:39.434+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:33:39.435+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:39.435+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:33:39.458+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:33:39.536+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:39.536+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:33:39.567+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:39.567+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:33:40.066+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:33:40.069+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:40.066+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 33, 39, 566839, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:33:40.070+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:40.070+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:33:40.071+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 33, 39, 566839, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:33:45.344+0000] {processor.py:153} INFO - Started process (PID=113500) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:33:45.345+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:33:45.346+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:45.346+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:33:45.381+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:33:45.494+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:45.494+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:33:45.540+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:45.540+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:33:45.900+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:33:45.905+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:45.901+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 33, 45, 540046, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:33:45.906+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:45.906+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:33:45.908+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 33, 45, 540046, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:33:51.817+0000] {processor.py:153} INFO - Started process (PID=113530) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:33:51.821+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:33:51.822+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:51.822+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:33:51.845+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:33:51.921+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:51.921+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:33:52.092+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:52.092+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:33:52.422+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:33:52.432+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:52.422+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 33, 52, 92636, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:33:52.434+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:52.433+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:33:52.438+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 33, 52, 92636, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:33:58.039+0000] {processor.py:153} INFO - Started process (PID=113563) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:33:58.043+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:33:58.044+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:58.044+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:33:58.096+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:33:58.257+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:58.257+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:33:58.496+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:58.495+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:33:58.620+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:33:58.626+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:58.621+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 33, 58, 495695, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:33:58.627+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:33:58.627+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:33:58.629+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 33, 58, 495695, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:34:04.049+0000] {processor.py:153} INFO - Started process (PID=113593) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:34:04.051+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:34:04.053+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:04.053+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:34:04.081+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:34:04.164+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:04.164+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:34:04.192+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:04.192+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:34:04.645+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:34:04.649+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:04.645+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 34, 4, 192404, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:34:04.650+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:04.650+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:34:04.651+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 34, 4, 192404, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:34:09.724+0000] {processor.py:153} INFO - Started process (PID=113618) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:34:09.725+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:34:09.726+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:09.726+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:34:09.748+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:34:09.846+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:09.846+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:34:09.887+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:09.886+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:34:10.036+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:34:10.041+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:10.036+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 34, 9, 886511, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:34:10.042+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:10.041+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:34:10.043+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 34, 9, 886511, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:34:15.135+0000] {processor.py:153} INFO - Started process (PID=113648) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:34:15.136+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:34:15.138+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:15.138+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:34:15.172+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:34:15.389+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:15.389+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:34:15.458+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:15.458+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:34:15.830+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:34:15.835+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:15.830+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 34, 15, 457715, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:34:15.836+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:15.835+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:34:15.840+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 34, 15, 457715, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:34:20.609+0000] {processor.py:153} INFO - Started process (PID=113672) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:34:20.611+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:34:20.612+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:20.612+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:34:20.616+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:20.615+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 29
    check_api >>>
                ^
SyntaxError: invalid syntax
[2024-02-10T18:34:20.620+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:34:20.691+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.095 seconds
[2024-02-10T18:34:21.155+0000] {processor.py:153} INFO - Started process (PID=113677) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:34:21.156+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:34:21.157+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:21.157+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:34:21.160+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:21.159+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 29
    check_api >>
                ^
SyntaxError: invalid syntax
[2024-02-10T18:34:21.160+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:34:21.187+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.036 seconds
[2024-02-10T18:34:23.817+0000] {processor.py:153} INFO - Started process (PID=113683) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:34:23.819+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:34:23.820+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:23.820+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:34:23.864+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:34:24.016+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:24.016+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:34:24.099+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:24.099+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:34:24.143+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:34:24.156+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:24.143+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 34, 24, 98886, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:34:24.157+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:24.156+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:34:24.159+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 34, 24, 98886, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:34:29.944+0000] {processor.py:153} INFO - Started process (PID=113717) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:34:29.946+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:34:29.947+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:29.946+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:34:30.008+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:34:30.212+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:30.212+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:34:30.265+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:30.265+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:34:30.476+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:34:30.482+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:30.477+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 34, 30, 265182, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:34:30.483+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:30.483+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:34:30.485+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 34, 30, 265182, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:34:35.521+0000] {processor.py:153} INFO - Started process (PID=113745) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:34:35.522+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:34:35.522+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:35.522+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:34:35.547+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:34:35.644+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:35.643+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:34:35.684+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:35.684+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:34:36.071+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:34:36.078+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:36.071+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 34, 35, 684007, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:34:36.080+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:36.080+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:34:36.082+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 34, 35, 684007, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:34:41.526+0000] {processor.py:153} INFO - Started process (PID=113770) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:34:41.527+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:34:41.531+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:41.530+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:34:41.592+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:34:41.749+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:41.749+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:34:41.804+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:41.803+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:34:42.363+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:34:42.368+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:42.364+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 34, 41, 803550, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:34:42.370+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:42.369+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:34:42.371+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 34, 41, 803550, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:34:47.540+0000] {processor.py:153} INFO - Started process (PID=113807) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:34:47.542+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:34:47.543+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:47.543+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:34:47.588+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:34:47.719+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:47.718+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:34:47.749+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:47.748+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:34:48.220+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:34:48.223+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:48.221+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 34, 47, 748751, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:34:48.224+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:48.224+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:34:48.225+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 34, 47, 748751, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:34:53.811+0000] {processor.py:153} INFO - Started process (PID=113832) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:34:53.811+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:34:53.812+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:53.812+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:34:53.836+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:34:53.929+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:53.929+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:34:54.098+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:54.098+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:34:54.397+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:34:54.401+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:54.397+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 34, 54, 98332, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:34:54.401+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:54.401+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:34:54.403+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 34, 54, 98332, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:34:55.855+0000] {processor.py:153} INFO - Started process (PID=113839) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:34:55.856+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:34:55.857+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:55.857+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:34:55.889+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:34:55.988+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:55.988+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:34:56.247+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:56.247+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:34:56.578+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:34:56.581+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:56.578+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 34, 56, 246961, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:34:56.582+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:34:56.582+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:34:56.584+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 34, 56, 246961, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:35:01.776+0000] {processor.py:153} INFO - Started process (PID=113874) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:35:01.777+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:35:01.779+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:01.779+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:35:01.832+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:35:02.095+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:02.095+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:35:02.420+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:02.420+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:35:02.581+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:35:02.585+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:02.582+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 35, 2, 420141, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:35:02.586+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:02.586+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:35:02.588+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 35, 2, 420141, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:35:08.472+0000] {processor.py:153} INFO - Started process (PID=113899) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:35:08.475+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:35:08.477+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:08.477+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:35:08.523+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:35:08.693+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:08.693+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:35:08.943+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:08.943+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:35:09.207+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:35:09.212+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:09.208+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 35, 8, 943505, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:35:09.214+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:09.214+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:35:09.215+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 35, 8, 943505, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:35:14.708+0000] {processor.py:153} INFO - Started process (PID=113932) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:35:14.716+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:35:14.723+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:14.722+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:35:14.760+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:35:15.025+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:15.024+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:35:15.125+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:15.125+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:35:15.420+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:35:15.424+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:15.420+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 35, 15, 125031, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:35:15.425+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:15.425+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:35:15.427+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 35, 15, 125031, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:35:17.246+0000] {processor.py:153} INFO - Started process (PID=113943) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:35:17.247+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:35:17.248+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:17.248+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:35:17.316+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:35:17.497+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:17.497+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:35:17.566+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:17.566+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:35:18.085+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:35:18.095+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:18.090+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 35, 17, 566269, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:35:18.095+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:18.095+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:35:18.097+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 35, 17, 566269, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:35:23.159+0000] {processor.py:153} INFO - Started process (PID=113971) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:35:23.160+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:35:23.161+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:23.161+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:35:23.185+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:35:23.312+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:23.312+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:35:23.359+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:23.359+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:35:23.720+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:35:23.729+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:23.720+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 35, 23, 359236, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:35:23.730+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:23.729+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:35:23.731+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 35, 23, 359236, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:35:27.224+0000] {processor.py:153} INFO - Started process (PID=113990) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:35:27.225+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:35:27.227+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:27.226+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:35:27.270+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:35:27.495+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:27.494+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:35:27.642+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:27.642+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:35:27.695+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:35:27.703+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:27.696+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 35, 27, 641719, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:35:27.704+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:27.704+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:35:27.706+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 35, 27, 641719, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:35:28.059+0000] {processor.py:153} INFO - Started process (PID=113994) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:35:28.060+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:35:28.061+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:28.061+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:35:28.098+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:35:28.268+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:28.268+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:35:28.333+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:28.333+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:35:28.394+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:35:28.399+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:28.395+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 35, 28, 332724, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:35:28.400+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:28.399+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:35:28.401+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 35, 28, 332724, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:35:28.590+0000] {processor.py:153} INFO - Started process (PID=113998) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:35:28.591+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:35:28.593+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:28.592+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:35:28.627+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:35:28.808+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:28.808+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:35:28.863+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:28.862+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:35:29.011+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:35:29.017+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:29.011+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 35, 28, 862665, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:35:29.018+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:29.018+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:35:29.020+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 35, 28, 862665, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:35:29.320+0000] {processor.py:153} INFO - Started process (PID=114000) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:35:29.321+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:35:29.322+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:29.322+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:35:29.361+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:35:29.516+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:29.516+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:35:29.546+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:29.546+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:35:30.039+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:35:30.042+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:30.039+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 35, 29, 546591, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:35:30.043+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:30.043+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:35:30.044+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 35, 29, 546591, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:35:35.270+0000] {processor.py:153} INFO - Started process (PID=114037) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:35:35.272+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:35:35.273+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:35.273+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:35:35.295+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:35:35.433+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:35.433+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:35:35.492+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:35.491+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:35:35.856+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:35:35.864+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:35.857+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 35, 35, 491668, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:35:35.864+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:35.864+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:35:35.866+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 35, 35, 491668, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:35:41.340+0000] {processor.py:153} INFO - Started process (PID=114063) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:35:41.342+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:35:41.344+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:41.344+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:35:41.401+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:35:41.545+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:41.545+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:35:41.601+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:41.600+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:35:41.910+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:35:41.916+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:41.911+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 35, 41, 600543, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:35:41.916+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:41.916+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:35:41.918+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 35, 41, 600543, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:35:47.543+0000] {processor.py:153} INFO - Started process (PID=114094) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:35:47.548+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:35:47.550+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:47.549+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:35:47.586+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:35:47.743+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:47.743+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:35:47.820+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:47.820+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:35:48.467+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:35:48.472+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:48.467+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 35, 47, 820302, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:35:48.472+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:48.472+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:35:48.473+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 35, 47, 820302, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:35:53.628+0000] {processor.py:153} INFO - Started process (PID=114122) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:35:53.629+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:35:53.630+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:53.630+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:35:53.654+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:35:53.736+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:53.736+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:35:53.906+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:53.906+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:35:53.967+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:35:53.971+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:53.967+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 35, 53, 905965, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:35:53.971+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:53.971+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:35:53.973+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 35, 53, 905965, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:35:59.179+0000] {processor.py:153} INFO - Started process (PID=114147) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:35:59.180+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:35:59.182+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:59.181+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:35:59.215+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:35:59.354+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:59.354+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:35:59.650+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:35:59.650+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:36:00.144+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:36:00.148+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:00.145+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 35, 59, 650408, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:36:00.149+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:00.148+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:36:00.150+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 35, 59, 650408, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:36:01.592+0000] {processor.py:153} INFO - Started process (PID=114162) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:36:01.594+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:36:01.595+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:01.595+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:36:01.629+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:36:01.764+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:01.763+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:36:01.990+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:01.989+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:36:02.169+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:36:02.173+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:02.169+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 36, 1, 989720, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:36:02.174+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:02.174+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:36:02.176+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 36, 1, 989720, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:36:07.260+0000] {processor.py:153} INFO - Started process (PID=114192) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:36:07.261+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:36:07.262+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:07.262+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:36:07.301+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:36:07.448+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:07.447+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:36:07.633+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:07.633+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:36:08.072+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:36:08.075+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:08.072+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 36, 7, 633057, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:36:08.076+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:08.076+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:36:08.077+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 36, 7, 633057, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:36:13.376+0000] {processor.py:153} INFO - Started process (PID=114217) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:36:13.377+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:36:13.379+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:13.378+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:36:13.423+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:36:13.607+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:13.606+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:36:13.647+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:13.647+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:36:14.111+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:36:14.115+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:14.111+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 36, 13, 647161, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:36:14.116+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:14.116+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:36:14.117+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 36, 13, 647161, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:36:19.221+0000] {processor.py:153} INFO - Started process (PID=114251) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:36:19.222+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:36:19.223+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:19.223+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:36:19.285+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:36:19.558+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:19.557+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:36:19.635+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:19.635+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:36:20.024+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:36:20.028+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:20.024+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 36, 19, 634914, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:36:20.029+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:20.029+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:36:20.031+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 36, 19, 634914, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:36:25.173+0000] {processor.py:153} INFO - Started process (PID=114277) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:36:25.174+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:36:25.175+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:25.175+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:36:25.209+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:36:25.352+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:25.352+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:36:25.415+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:25.414+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:36:25.654+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:36:25.662+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:25.655+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 36, 25, 414664, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:36:25.663+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:25.662+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:36:25.664+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 36, 25, 414664, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:36:30.899+0000] {processor.py:153} INFO - Started process (PID=114302) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:36:30.900+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:36:30.901+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:30.900+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:36:30.924+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:36:31.023+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:31.023+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:36:31.061+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:31.061+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:36:31.403+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:36:31.406+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:31.403+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 36, 31, 61031, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:36:31.406+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:31.406+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:36:31.407+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 36, 31, 61031, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:36:35.298+0000] {processor.py:153} INFO - Started process (PID=114328) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:36:35.305+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:36:35.306+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:35.306+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:36:35.406+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:36:35.819+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:35.819+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:36:36.050+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:36.050+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:36:36.416+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:36:36.436+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:36.422+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 36, 36, 49815, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:36:36.441+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:36.441+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:36:36.463+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 36, 36, 49815, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:36:42.822+0000] {processor.py:153} INFO - Started process (PID=114355) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:36:42.823+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:36:42.823+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:42.823+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:36:42.847+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:36:42.951+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:42.951+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:36:43.003+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:43.002+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:36:43.369+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:36:43.373+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:43.369+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 36, 43, 2631, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:36:43.374+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:43.374+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:36:43.376+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 36, 43, 2631, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:36:49.320+0000] {processor.py:153} INFO - Started process (PID=114376) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:36:49.321+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:36:49.323+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:49.323+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:36:49.371+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:36:49.480+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:49.479+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:36:49.511+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:49.511+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:36:49.706+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:36:49.711+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:49.706+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 36, 49, 511480, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:36:49.711+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:49.711+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:36:49.934+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 36, 49, 511480, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:36:51.104+0000] {processor.py:153} INFO - Started process (PID=114386) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:36:51.105+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:36:51.106+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:51.106+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:36:51.142+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:36:51.288+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:51.288+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:36:51.346+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:51.345+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:36:51.655+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:36:51.667+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:51.655+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 36, 51, 345470, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:36:51.671+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:51.671+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:36:51.674+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 36, 51, 345470, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:36:56.228+0000] {processor.py:153} INFO - Started process (PID=114409) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:36:56.230+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:36:56.231+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:56.231+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:36:56.235+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:56.234+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 29
    check_api >>
               ^
SyntaxError: invalid syntax
[2024-02-10T18:36:56.236+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:36:56.273+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.051 seconds
[2024-02-10T18:36:58.876+0000] {processor.py:153} INFO - Started process (PID=114424) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:36:58.877+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:36:58.878+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:58.878+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:36:58.914+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:36:59.064+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:59.064+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:36:59.299+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:59.299+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:36:59.625+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:36:59.630+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:59.625+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 36, 59, 297931, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:36:59.631+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:36:59.631+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:36:59.633+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 36, 59, 297931, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:37:01.341+0000] {processor.py:153} INFO - Started process (PID=114431) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:37:01.342+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:37:01.344+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:01.344+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:37:01.391+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:37:01.527+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:01.526+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:37:01.697+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:01.697+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:37:01.793+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:37:01.798+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:01.794+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 37, 1, 697531, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:37:01.798+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:01.798+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:37:01.800+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 37, 1, 697531, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:37:06.871+0000] {processor.py:153} INFO - Started process (PID=114456) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:37:06.875+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:37:06.876+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:06.876+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:37:06.917+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:37:07.034+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:07.033+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:37:07.255+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:07.255+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:37:07.362+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:37:07.365+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:07.363+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 37, 7, 255450, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:37:07.366+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:07.366+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:37:07.367+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 37, 7, 255450, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:37:13.182+0000] {processor.py:153} INFO - Started process (PID=114491) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:37:13.183+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:37:13.185+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:13.184+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:37:13.214+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:37:13.362+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:13.362+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:37:13.594+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:13.594+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:37:14.014+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:37:14.016+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:14.014+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 37, 13, 594277, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:37:14.017+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:14.017+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:37:14.018+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 37, 13, 594277, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:37:19.211+0000] {processor.py:153} INFO - Started process (PID=114518) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:37:19.212+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:37:19.213+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:19.213+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:37:19.247+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:37:19.555+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:19.555+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:37:19.599+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:19.598+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:37:19.954+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:37:19.957+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:19.955+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 37, 19, 598734, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:37:19.958+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:19.958+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:37:19.959+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 37, 19, 598734, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:37:25.388+0000] {processor.py:153} INFO - Started process (PID=114553) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:37:25.401+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:37:25.404+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:25.404+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:37:25.463+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:37:25.735+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:25.735+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:37:25.858+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:25.858+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:37:26.207+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:37:26.211+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:26.207+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 37, 25, 857748, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:37:26.212+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:26.211+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:37:26.225+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 37, 25, 857748, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:37:32.017+0000] {processor.py:153} INFO - Started process (PID=114578) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:37:32.018+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:37:32.018+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:32.018+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:37:32.044+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:37:32.144+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:32.144+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:37:32.198+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:32.197+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:37:32.710+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:37:32.716+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:32.710+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 37, 32, 197500, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:37:32.717+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:32.716+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:37:32.718+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 37, 32, 197500, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:37:38.295+0000] {processor.py:153} INFO - Started process (PID=114615) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:37:38.296+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:37:38.298+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:38.297+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:37:38.334+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:37:38.470+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:38.470+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:37:38.532+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:38.532+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:37:38.786+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:37:38.799+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:38.786+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 37, 38, 531579, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:37:38.799+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:38.799+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:37:38.806+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 37, 38, 531579, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:37:43.911+0000] {processor.py:153} INFO - Started process (PID=114640) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:37:43.913+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:37:43.914+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:43.914+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:37:43.946+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:37:44.025+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:44.025+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:37:44.053+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:44.053+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:37:44.388+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:37:44.397+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:44.389+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 37, 44, 53106, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:37:44.398+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:44.398+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:37:44.401+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 37, 44, 53106, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:37:49.502+0000] {processor.py:153} INFO - Started process (PID=114665) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:37:49.504+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:37:49.506+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:49.506+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:37:49.538+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:37:49.615+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:49.615+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:37:49.643+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:49.643+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:37:50.184+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:37:50.187+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:50.185+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 37, 49, 643493, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:37:50.187+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:50.187+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:37:50.188+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 37, 49, 643493, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:37:55.273+0000] {processor.py:153} INFO - Started process (PID=114702) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:37:55.274+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:37:55.275+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:55.275+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:37:55.297+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:37:55.417+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:55.417+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:37:55.465+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:55.465+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:37:56.082+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:37:56.086+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:56.082+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 37, 55, 465137, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:37:56.087+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:37:56.086+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:37:56.088+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 37, 55, 465137, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:38:01.674+0000] {processor.py:153} INFO - Started process (PID=114727) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:38:01.674+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:38:01.675+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:01.675+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:38:01.702+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:38:01.841+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:01.841+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:38:02.040+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:02.040+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:38:02.526+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:38:02.528+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:02.526+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 38, 2, 39908, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:38:02.529+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:02.528+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:38:02.530+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 38, 2, 39908, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:38:07.645+0000] {processor.py:153} INFO - Started process (PID=114765) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:38:07.646+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:38:07.648+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:07.647+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:38:07.681+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:38:07.826+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:07.826+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:38:08.125+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:08.125+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:38:08.553+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:38:08.557+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:08.554+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 38, 8, 125193, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:38:08.558+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:08.558+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:38:08.559+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 38, 8, 125193, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:38:14.532+0000] {processor.py:153} INFO - Started process (PID=114793) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:38:14.533+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:38:14.534+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:14.534+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:38:14.557+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:38:14.662+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:14.661+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:38:14.861+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:14.861+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:38:15.182+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:38:15.186+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:15.183+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 38, 14, 861271, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:38:15.187+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:15.187+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:38:15.188+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 38, 14, 861271, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:38:20.501+0000] {processor.py:153} INFO - Started process (PID=114832) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:38:20.502+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:38:20.503+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:20.503+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:38:20.535+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:38:20.840+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:20.840+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:38:20.888+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:20.887+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:38:21.395+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:38:21.399+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:21.396+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 38, 20, 887789, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:38:21.400+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:21.400+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:38:21.402+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 38, 20, 887789, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:38:26.565+0000] {processor.py:153} INFO - Started process (PID=114857) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:38:26.569+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:38:26.570+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:26.570+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:38:26.603+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:38:26.737+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:26.737+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:38:26.766+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:26.766+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:38:27.114+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:38:27.121+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:27.115+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 38, 26, 766416, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:38:27.122+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:27.122+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:38:27.124+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 38, 26, 766416, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:38:32.174+0000] {processor.py:153} INFO - Started process (PID=114880) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:38:32.176+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:38:32.177+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:32.177+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:38:32.206+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:38:32.298+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:32.297+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:38:32.327+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:32.327+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:38:32.823+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:38:32.827+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:32.824+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 38, 32, 326968, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:38:32.828+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:32.828+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:38:32.829+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 38, 32, 326968, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:38:38.578+0000] {processor.py:153} INFO - Started process (PID=114917) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:38:38.579+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:38:38.580+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:38.580+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:38:38.603+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:38:38.696+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:38.696+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:38:38.728+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:38.728+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:38:38.929+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:38:38.931+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:38.929+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 38, 38, 728438, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:38:38.932+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:38.932+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:38:38.933+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 38, 38, 728438, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:38:44.303+0000] {processor.py:153} INFO - Started process (PID=114942) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:38:44.304+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:38:44.305+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:44.305+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:38:44.329+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:38:44.421+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:44.421+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:38:44.460+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:44.460+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:38:44.865+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:38:44.868+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:44.865+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 38, 44, 459907, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:38:44.868+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:44.868+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:38:44.869+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 38, 44, 459907, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:38:50.015+0000] {processor.py:153} INFO - Started process (PID=114980) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:38:50.016+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:38:50.017+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:50.017+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:38:50.051+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:38:50.233+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:50.232+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:38:50.314+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:50.313+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:38:50.840+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:38:50.852+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:50.840+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 38, 50, 313542, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:38:50.853+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:50.853+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:38:50.854+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 38, 50, 313542, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:38:56.494+0000] {processor.py:153} INFO - Started process (PID=115006) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:38:56.495+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:38:56.496+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:56.496+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:38:56.521+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:38:56.612+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:56.612+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:38:56.791+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:56.791+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:38:57.157+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:38:57.161+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:57.157+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 38, 56, 791483, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:38:57.162+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:38:57.161+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:38:57.163+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 38, 56, 791483, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:39:02.274+0000] {processor.py:153} INFO - Started process (PID=115042) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:39:02.275+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:39:02.276+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:02.276+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:39:02.307+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:39:02.465+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:02.465+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:39:02.721+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:02.721+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:39:03.188+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:39:03.203+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:03.197+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 39, 2, 721168, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:39:03.204+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:03.204+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:39:03.206+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 39, 2, 721168, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:39:08.520+0000] {processor.py:153} INFO - Started process (PID=115068) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:39:08.521+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:39:08.522+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:08.522+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:39:08.546+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:39:08.689+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:08.689+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:39:08.901+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:08.901+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:39:09.138+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:39:09.141+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:09.138+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 39, 8, 901479, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:39:09.141+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:09.141+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:39:09.142+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 39, 8, 901479, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:39:14.524+0000] {processor.py:153} INFO - Started process (PID=115093) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:39:14.528+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:39:14.533+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:14.532+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:39:14.594+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:39:14.867+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:14.867+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:39:14.897+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:14.897+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:39:15.006+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:39:15.009+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:15.006+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 39, 14, 896895, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:39:15.010+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:15.010+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:39:15.011+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 39, 14, 896895, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:39:20.195+0000] {processor.py:153} INFO - Started process (PID=115128) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:39:20.203+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:39:20.205+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:20.204+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:39:20.265+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:39:20.466+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:20.465+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:39:20.553+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:20.553+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:39:20.610+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:39:20.629+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:20.623+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 39, 20, 553261, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:39:20.630+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:20.630+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:39:20.631+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 39, 20, 553261, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:39:25.783+0000] {processor.py:153} INFO - Started process (PID=115153) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:39:25.784+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:39:25.785+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:25.785+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:39:25.807+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:39:25.883+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:25.883+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:39:25.910+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:25.910+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:39:26.132+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:39:26.142+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:26.133+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 39, 25, 910620, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:39:26.143+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:26.143+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:39:26.145+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 39, 25, 910620, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:39:31.365+0000] {processor.py:153} INFO - Started process (PID=115178) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:39:31.366+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:39:31.367+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:31.367+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:39:31.390+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:39:31.474+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:31.474+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:39:31.510+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:31.510+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:39:31.922+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:39:31.925+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:31.922+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 39, 31, 509978, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:39:31.925+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:31.925+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:39:31.926+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 39, 31, 509978, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:39:38.642+0000] {processor.py:153} INFO - Started process (PID=115215) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:39:38.643+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:39:38.644+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:38.644+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:39:38.669+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:39:38.766+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:38.766+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:39:38.812+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:38.812+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:39:39.326+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:39:39.329+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:39.326+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 39, 38, 811655, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:39:39.329+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:39.329+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:39:39.330+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 39, 38, 811655, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:39:44.556+0000] {processor.py:153} INFO - Started process (PID=115243) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:39:44.557+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:39:44.558+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:44.558+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:39:44.592+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:39:44.675+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:44.675+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:39:44.714+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:44.714+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:39:45.253+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:39:45.260+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:45.254+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 39, 44, 713749, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:39:45.262+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:45.261+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:39:45.264+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 39, 44, 713749, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:39:50.744+0000] {processor.py:153} INFO - Started process (PID=115275) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:39:50.754+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:39:50.769+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:50.769+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:39:50.803+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:39:50.941+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:50.941+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:39:50.998+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:50.997+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:39:51.757+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:39:51.761+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:51.758+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 39, 50, 997618, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:39:51.762+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:51.762+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:39:51.764+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 39, 50, 997618, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:39:56.874+0000] {processor.py:153} INFO - Started process (PID=115303) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:39:56.875+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:39:56.875+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:56.875+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:39:56.898+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:39:56.981+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:56.981+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:39:57.155+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:57.154+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:39:57.649+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:39:57.656+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:57.650+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 39, 57, 154584, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:39:57.658+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:39:57.657+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:39:57.660+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 39, 57, 154584, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:40:03.478+0000] {processor.py:153} INFO - Started process (PID=115340) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:40:03.479+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:40:03.481+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:03.480+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:40:03.540+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:40:03.748+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:03.748+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:40:03.984+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:03.983+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:40:04.306+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:40:04.310+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:04.306+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 40, 3, 983738, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:40:04.310+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:04.310+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:40:04.312+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 40, 3, 983738, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:40:09.462+0000] {processor.py:153} INFO - Started process (PID=115367) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:40:09.463+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:40:09.463+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:09.463+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:40:09.497+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:40:09.613+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:09.613+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:40:09.817+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:09.816+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:40:10.201+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:40:10.204+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:10.202+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 40, 9, 816825, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:40:10.204+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:10.204+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:40:10.205+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 40, 9, 816825, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:40:15.454+0000] {processor.py:153} INFO - Started process (PID=115391) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:40:15.455+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:40:15.456+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:15.456+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:40:15.479+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:40:15.710+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:15.710+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:40:15.760+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:15.759+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:40:15.808+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:40:15.812+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:15.808+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 40, 15, 759551, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:40:15.813+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:15.813+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:40:15.814+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 40, 15, 759551, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:40:22.022+0000] {processor.py:153} INFO - Started process (PID=115427) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:40:22.023+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:40:22.025+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:22.024+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:40:22.059+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:40:22.374+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:22.374+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:40:22.421+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:22.421+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:40:22.651+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:40:22.657+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:22.652+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 40, 22, 420888, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:40:22.658+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:22.657+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:40:22.659+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 40, 22, 420888, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:40:28.362+0000] {processor.py:153} INFO - Started process (PID=115452) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:40:28.363+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:40:28.365+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:28.364+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:40:28.389+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:40:28.478+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:28.477+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:40:28.512+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:28.512+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:40:28.620+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:40:28.624+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:28.621+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 40, 28, 512382, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:40:28.625+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:28.624+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:40:28.626+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 40, 28, 512382, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:40:34.108+0000] {processor.py:153} INFO - Started process (PID=115486) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:40:34.112+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:40:34.113+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:34.113+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:40:34.148+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:40:34.445+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:34.445+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:40:34.556+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:34.555+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:40:34.624+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:40:34.629+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:34.624+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 40, 34, 555516, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:40:34.630+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:34.630+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:40:34.642+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 40, 34, 555516, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:40:39.848+0000] {processor.py:153} INFO - Started process (PID=115515) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:40:39.848+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:40:39.850+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:39.849+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:40:39.884+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:40:39.986+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:39.986+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:40:40.015+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:40.015+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:40:40.481+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:40:40.485+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:40.482+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 40, 40, 15004, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:40:40.486+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:40.485+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:40:40.487+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 40, 40, 15004, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:40:45.791+0000] {processor.py:153} INFO - Started process (PID=115541) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:40:45.792+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:40:45.793+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:45.792+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:40:45.824+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:40:45.950+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:45.949+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:40:45.998+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:45.998+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:40:46.509+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:40:46.511+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:46.509+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 40, 45, 998227, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:40:46.512+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:46.512+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:40:46.513+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 40, 45, 998227, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:40:52.003+0000] {processor.py:153} INFO - Started process (PID=115576) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:40:52.005+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:40:52.007+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:52.006+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:40:52.046+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:40:52.206+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:52.206+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:40:52.259+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:52.259+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:40:52.760+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:40:52.764+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:52.761+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 40, 52, 259234, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:40:52.765+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:52.765+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:40:52.766+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 40, 52, 259234, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:40:57.959+0000] {processor.py:153} INFO - Started process (PID=115601) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:40:57.960+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:40:57.961+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:57.961+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:40:57.985+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:40:58.091+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:58.090+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:40:58.293+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:58.292+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:40:58.608+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:40:58.611+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:58.609+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 40, 58, 292796, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:40:58.611+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:40:58.611+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:40:58.612+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 40, 58, 292796, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:41:04.083+0000] {processor.py:153} INFO - Started process (PID=115637) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:41:04.084+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:41:04.085+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:04.085+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:41:04.118+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:41:04.261+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:04.261+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:41:04.568+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:04.568+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:41:04.752+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:41:04.757+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:04.753+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 41, 4, 568461, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:41:04.758+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:04.758+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:41:04.759+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 41, 4, 568461, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:41:10.250+0000] {processor.py:153} INFO - Started process (PID=115665) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:41:10.251+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:41:10.252+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:10.252+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:41:10.275+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:41:10.365+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:10.364+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:41:10.530+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:10.530+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:41:10.836+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:41:10.841+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:10.837+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 41, 10, 530533, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:41:10.842+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:10.841+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:41:10.843+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 41, 10, 530533, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:41:16.387+0000] {processor.py:153} INFO - Started process (PID=115692) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:41:16.388+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:41:16.389+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:16.389+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:41:16.427+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:41:16.667+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:16.667+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:41:16.697+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:16.697+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:41:16.968+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:41:16.971+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:16.969+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 41, 16, 697316, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:41:16.972+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:16.972+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:41:16.973+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 41, 16, 697316, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:41:22.317+0000] {processor.py:153} INFO - Started process (PID=115730) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:41:22.318+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:41:22.319+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:22.319+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:41:22.352+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:41:22.610+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:22.610+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:41:22.648+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:22.648+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:41:22.782+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:41:22.784+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:22.782+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 41, 22, 648501, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:41:22.785+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:22.785+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:41:22.786+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 41, 22, 648501, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:41:28.062+0000] {processor.py:153} INFO - Started process (PID=115752) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:41:28.063+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:41:28.065+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:28.064+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:41:28.087+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:41:28.170+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:28.170+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:41:28.216+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:28.215+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:41:28.483+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:41:28.487+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:28.483+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 41, 28, 215574, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:41:28.488+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:28.488+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:41:28.489+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 41, 28, 215574, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:41:33.825+0000] {processor.py:153} INFO - Started process (PID=115786) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:41:33.826+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:41:33.827+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:33.827+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:41:33.859+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:41:34.006+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:34.006+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:41:34.060+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:34.060+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:41:34.151+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:41:34.156+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:34.152+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 41, 34, 60242, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:41:34.157+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:34.157+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:41:34.159+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 41, 34, 60242, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:41:39.726+0000] {processor.py:153} INFO - Started process (PID=115814) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:41:39.730+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:41:39.731+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:39.731+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:41:39.761+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:41:39.845+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:39.845+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:41:39.878+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:39.878+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:41:40.356+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:41:40.360+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:40.356+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 41, 39, 878343, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:41:40.361+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:40.361+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:41:40.363+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 41, 39, 878343, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:41:45.545+0000] {processor.py:153} INFO - Started process (PID=115839) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:41:45.547+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:41:45.549+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:45.548+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:41:45.584+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:41:45.700+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:45.700+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:41:45.733+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:45.733+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:41:46.334+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:41:46.337+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:46.334+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 41, 45, 733371, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:41:46.338+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:46.338+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:41:46.339+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 41, 45, 733371, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:41:51.990+0000] {processor.py:153} INFO - Started process (PID=115878) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:41:51.994+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:41:51.995+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:51.995+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:41:52.020+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:41:52.153+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:52.153+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:41:52.217+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:52.216+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:41:52.730+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:41:52.734+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:52.730+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 41, 52, 216646, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:41:52.735+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:52.734+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:41:52.736+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 41, 52, 216646, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:41:58.625+0000] {processor.py:153} INFO - Started process (PID=115903) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:41:58.627+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:41:58.628+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:58.628+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:41:58.655+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:41:58.744+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:58.743+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:41:58.921+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:58.920+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:41:59.372+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:41:59.376+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:59.372+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 41, 58, 920626, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:41:59.377+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:41:59.376+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:41:59.378+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 41, 58, 920626, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:42:05.122+0000] {processor.py:153} INFO - Started process (PID=115942) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:42:05.124+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:42:05.125+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:05.125+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:42:05.157+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:42:05.296+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:05.296+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:42:05.520+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:05.520+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:42:05.733+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:42:05.737+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:05.733+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 42, 5, 520405, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:42:05.738+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:05.737+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:42:05.739+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 42, 5, 520405, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:42:11.146+0000] {processor.py:153} INFO - Started process (PID=115965) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:42:11.147+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:42:11.148+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:11.148+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:42:11.173+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:42:11.268+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:11.267+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:42:11.448+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:11.448+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:42:11.657+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:42:11.660+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:11.657+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 42, 11, 448066, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:42:11.661+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:11.660+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:42:11.662+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 42, 11, 448066, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:42:17.012+0000] {processor.py:153} INFO - Started process (PID=115990) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:42:17.013+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:42:17.014+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:17.014+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:42:17.056+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:42:17.361+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:17.361+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:42:17.435+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:17.435+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:42:17.688+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:42:17.692+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:17.688+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 42, 17, 434858, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:42:17.693+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:17.693+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:42:17.694+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 42, 17, 434858, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:42:23.618+0000] {processor.py:153} INFO - Started process (PID=116024) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:42:23.624+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:42:23.626+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:23.625+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:42:23.698+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:42:24.211+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:24.211+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:42:24.281+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:24.280+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:42:24.741+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:42:24.745+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:24.741+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 42, 24, 280639, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:42:24.746+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:24.746+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:42:24.748+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 42, 24, 280639, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:42:30.526+0000] {processor.py:153} INFO - Started process (PID=116049) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:42:30.532+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:42:30.534+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:30.534+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:42:30.588+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:42:30.736+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:30.736+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:42:30.786+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:30.786+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:42:31.208+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:42:31.213+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:31.209+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 42, 30, 786457, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:42:31.213+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:31.213+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:42:31.214+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 42, 30, 786457, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:42:36.346+0000] {processor.py:153} INFO - Started process (PID=116083) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:42:36.348+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:42:36.349+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:36.349+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:42:36.412+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:42:36.577+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:36.577+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:42:36.630+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:36.630+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:42:37.135+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:42:37.142+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:37.136+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 42, 36, 630184, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:42:37.143+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:37.143+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:42:37.145+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 42, 36, 630184, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:42:42.947+0000] {processor.py:153} INFO - Started process (PID=116111) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:42:42.952+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:42:42.954+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:42.953+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:42:42.984+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:42:43.078+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:43.077+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:42:43.109+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:43.109+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:42:43.476+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:42:43.478+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:43.476+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 42, 43, 109378, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:42:43.479+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:43.479+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:42:43.480+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 42, 43, 109378, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:42:48.924+0000] {processor.py:153} INFO - Started process (PID=116136) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:42:48.927+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:42:48.928+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:48.928+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:42:48.953+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:42:49.094+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:49.094+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:42:49.147+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:49.147+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:42:49.550+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:42:49.554+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:49.550+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 42, 49, 147252, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:42:49.555+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:49.555+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:42:49.556+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 42, 49, 147252, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:42:54.752+0000] {processor.py:153} INFO - Started process (PID=116174) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:42:54.754+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:42:54.756+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:54.755+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:42:54.791+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:42:54.888+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:54.888+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:42:54.918+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:54.918+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:42:55.186+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:42:55.189+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:55.186+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 42, 54, 917922, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:42:55.190+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:42:55.190+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:42:55.191+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 42, 54, 917922, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:43:00.617+0000] {processor.py:153} INFO - Started process (PID=116199) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:43:00.619+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:43:00.621+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:00.620+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:43:00.656+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:43:00.748+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:00.747+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:43:00.969+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:00.969+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:43:01.013+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:43:01.017+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:01.014+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 43, 0, 968774, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:43:01.018+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:01.018+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:43:01.019+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 43, 0, 968774, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:43:06.853+0000] {processor.py:153} INFO - Started process (PID=116237) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:43:06.855+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:43:06.856+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:06.856+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:43:06.891+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:43:07.031+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:07.031+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:43:07.297+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:07.297+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:43:07.434+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:43:07.438+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:07.434+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 43, 7, 296846, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:43:07.439+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:07.439+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:43:07.440+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 43, 7, 296846, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:43:12.676+0000] {processor.py:153} INFO - Started process (PID=116262) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:43:12.677+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:43:12.678+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:12.678+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:43:12.733+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:43:12.875+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:12.875+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:43:13.146+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:13.146+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:43:13.246+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:43:13.254+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:13.248+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 43, 13, 146339, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:43:13.255+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:13.255+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:43:13.257+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 43, 13, 146339, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:43:18.810+0000] {processor.py:153} INFO - Started process (PID=116296) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:43:18.811+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:43:18.813+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:18.812+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:43:18.846+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:43:19.172+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:19.172+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:43:19.222+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:19.221+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:43:19.347+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:43:19.352+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:19.347+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 43, 19, 221654, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:43:19.356+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:19.356+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:43:19.358+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 43, 19, 221654, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:43:24.672+0000] {processor.py:153} INFO - Started process (PID=116324) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:43:24.676+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:43:24.676+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:24.676+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:43:24.706+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:43:24.933+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:24.932+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:43:24.969+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:24.968+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:43:25.434+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:43:25.436+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:25.434+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 43, 24, 968679, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:43:25.437+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:25.437+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:43:25.438+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 43, 24, 968679, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:43:30.608+0000] {processor.py:153} INFO - Started process (PID=116349) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:43:30.610+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:43:30.612+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:30.611+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:43:30.644+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:43:30.796+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:30.795+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:43:30.833+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:30.833+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:43:31.073+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:43:31.077+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:31.073+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 43, 30, 833100, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:43:31.078+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:31.078+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:43:31.079+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 43, 30, 833100, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:43:36.496+0000] {processor.py:153} INFO - Started process (PID=116386) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:43:36.497+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:43:36.498+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:36.498+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:43:36.522+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:43:36.603+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:36.603+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:43:36.640+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:36.639+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:43:36.993+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:43:37.002+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:36.993+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 43, 36, 639745, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:43:37.003+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:37.003+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:43:37.006+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 43, 36, 639745, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:43:42.364+0000] {processor.py:153} INFO - Started process (PID=116411) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:43:42.366+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:43:42.367+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:42.367+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:43:42.401+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:43:42.521+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:42.521+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:43:42.563+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:42.562+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:43:42.682+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:43:42.685+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:42.682+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 43, 42, 562744, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:43:42.686+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:42.685+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:43:42.687+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 43, 42, 562744, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:43:47.834+0000] {processor.py:153} INFO - Started process (PID=116445) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:43:47.835+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:43:47.836+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:47.836+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:43:47.840+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:47.838+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 17
    /*
    ^
SyntaxError: invalid syntax
[2024-02-10T18:43:47.840+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:43:47.873+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.044 seconds
[2024-02-10T18:43:49.524+0000] {processor.py:153} INFO - Started process (PID=116457) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:43:49.525+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:43:49.526+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:49.525+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:43:49.529+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:49.527+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 17
    /*
    ^
SyntaxError: invalid syntax
[2024-02-10T18:43:49.530+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:43:49.554+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.036 seconds
[2024-02-10T18:43:52.462+0000] {processor.py:153} INFO - Started process (PID=116465) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:43:52.466+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:43:52.470+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:52.469+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:43:52.484+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:52.479+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 17
    /*
    ^
SyntaxError: invalid syntax
[2024-02-10T18:43:52.486+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:43:52.537+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.096 seconds
[2024-02-10T18:43:54.410+0000] {processor.py:153} INFO - Started process (PID=116480) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:43:54.411+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:43:54.412+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:54.412+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:43:54.418+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:54.414+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 17
    /*
    ^
SyntaxError: invalid syntax
[2024-02-10T18:43:54.419+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:43:54.449+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.046 seconds
[2024-02-10T18:43:56.965+0000] {processor.py:153} INFO - Started process (PID=116491) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:43:56.966+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:43:56.967+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:56.967+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:43:56.971+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:43:56.970+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 17
    ///
     ^
SyntaxError: invalid syntax
[2024-02-10T18:43:56.972+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:43:57.009+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.051 seconds
[2024-02-10T18:44:01.140+0000] {processor.py:153} INFO - Started process (PID=116518) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:44:01.140+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:44:01.141+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:01.141+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:44:01.144+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:01.143+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 31
    ///
     ^
SyntaxError: invalid syntax
[2024-02-10T18:44:01.144+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:44:01.177+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.042 seconds
[2024-02-10T18:44:02.903+0000] {processor.py:153} INFO - Started process (PID=116527) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:44:02.904+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:44:02.905+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:02.905+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:44:02.909+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:02.908+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 31
    ///
     ^
SyntaxError: invalid syntax
[2024-02-10T18:44:02.910+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:44:02.939+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.041 seconds
[2024-02-10T18:44:06.262+0000] {processor.py:153} INFO - Started process (PID=116542) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:44:06.263+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:44:06.265+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:06.264+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:44:06.269+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:06.267+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 36
    ///
     ^
SyntaxError: invalid syntax
[2024-02-10T18:44:06.270+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:44:06.317+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.060 seconds
[2024-02-10T18:44:10.742+0000] {processor.py:153} INFO - Started process (PID=116560) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:44:10.743+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:44:10.745+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:10.744+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:44:10.781+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:44:10.909+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:10.909+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:44:11.176+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:11.176+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:44:11.543+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:44:11.556+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:11.544+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 44, 11, 175864, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:44:11.562+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:11.561+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:44:11.563+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 44, 11, 175864, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:44:15.654+0000] {processor.py:153} INFO - Started process (PID=116580) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:44:15.658+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:44:15.659+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:15.659+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:44:15.700+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:44:15.910+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:15.910+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:44:16.349+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:16.349+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:44:16.748+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:44:16.753+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:16.749+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 44, 16, 349405, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:44:16.754+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:16.753+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:44:16.755+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 44, 16, 349405, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:44:19.319+0000] {processor.py:153} INFO - Started process (PID=116602) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:44:19.320+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:44:19.321+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:19.321+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:44:19.353+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:44:19.460+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:19.460+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:44:19.710+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:19.709+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:44:19.863+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:44:19.868+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:19.864+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 44, 19, 709745, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:44:19.869+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:19.869+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:44:19.870+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 44, 19, 709745, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:44:22.592+0000] {processor.py:153} INFO - Started process (PID=116618) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:44:22.592+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:44:22.593+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:22.593+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:44:22.620+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:44:22.886+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:22.886+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:44:22.935+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:22.935+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:44:22.967+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:44:22.972+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:22.967+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 44, 22, 934961, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:44:22.973+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:22.973+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:44:22.975+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 44, 22, 934961, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:44:30.656+0000] {processor.py:153} INFO - Started process (PID=116643) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:44:30.658+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:44:30.660+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:30.659+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:44:30.708+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:44:30.989+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:30.989+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:44:31.019+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:31.019+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:44:31.232+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:44:31.237+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:31.232+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 44, 31, 18721, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:44:31.238+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:31.238+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:44:31.239+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 44, 31, 18721, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:44:37.165+0000] {processor.py:153} INFO - Started process (PID=116680) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:44:37.171+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:44:37.173+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:37.173+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:44:37.258+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:44:37.375+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:37.375+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:44:37.415+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:37.414+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:44:37.644+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:44:37.647+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:37.644+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 44, 37, 414534, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:44:37.648+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:37.647+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:44:37.648+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 44, 37, 414534, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:44:42.699+0000] {processor.py:153} INFO - Started process (PID=116705) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:44:42.701+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:44:42.703+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:42.703+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:44:42.739+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:44:43.021+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:43.020+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:44:43.103+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:43.103+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:44:43.188+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:44:43.198+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:43.188+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 44, 43, 103382, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:44:43.199+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:43.199+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:44:43.203+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 44, 43, 103382, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:44:49.154+0000] {processor.py:153} INFO - Started process (PID=116739) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:44:49.160+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:44:49.161+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:49.161+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:44:49.198+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:44:49.385+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:49.385+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:44:49.443+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:49.443+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:44:49.655+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:44:49.660+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:49.655+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 44, 49, 442829, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:44:49.660+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:49.660+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:44:49.662+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 44, 49, 442829, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:44:54.734+0000] {processor.py:153} INFO - Started process (PID=116763) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:44:54.738+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:44:54.739+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:54.738+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:44:54.779+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:44:54.973+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:54.973+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:44:55.045+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:55.045+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:44:55.433+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:44:55.437+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:55.433+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 44, 55, 45300, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:44:55.438+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:44:55.438+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:44:55.700+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 44, 55, 45300, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:45:01.059+0000] {processor.py:153} INFO - Started process (PID=116791) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:45:01.069+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:45:01.070+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:01.070+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:45:01.108+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:45:01.277+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:01.277+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:45:01.348+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:01.347+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:45:01.662+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:45:01.665+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:01.662+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 45, 1, 347624, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:45:01.666+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:01.666+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:45:01.668+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 45, 1, 347624, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:45:06.874+0000] {processor.py:153} INFO - Started process (PID=116829) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:45:06.876+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:45:06.877+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:06.876+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:45:06.910+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:45:07.013+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:07.013+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:45:07.250+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:07.250+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:45:07.563+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:45:07.567+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:07.563+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 45, 7, 249899, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:45:07.568+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:07.567+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:45:07.569+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 45, 7, 249899, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:45:13.768+0000] {processor.py:153} INFO - Started process (PID=116855) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:45:13.770+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:45:13.771+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:13.771+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:45:13.826+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:45:14.323+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:14.323+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:45:14.852+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:14.851+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:45:15.241+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:45:15.246+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:15.241+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 45, 14, 851768, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:45:15.247+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:15.247+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:45:15.248+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 45, 14, 851768, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:45:20.808+0000] {processor.py:153} INFO - Started process (PID=116893) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:45:20.813+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:45:20.814+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:20.814+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:45:20.850+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:45:21.056+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:21.055+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:45:21.275+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:21.274+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:45:21.620+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:45:21.624+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:21.620+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 45, 21, 274491, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:45:21.624+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:21.624+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:45:21.626+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 45, 21, 274491, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:45:27.279+0000] {processor.py:153} INFO - Started process (PID=116918) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:45:27.280+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:45:27.281+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:27.281+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:45:27.305+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:45:27.540+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:27.539+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:45:27.587+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:27.586+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:45:27.619+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:45:27.623+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:27.619+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 45, 27, 586744, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:45:27.623+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:27.623+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:45:27.625+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 45, 27, 586744, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:45:32.689+0000] {processor.py:153} INFO - Started process (PID=116950) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:45:32.691+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:45:32.692+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:32.692+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:45:32.724+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:45:32.984+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:32.983+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:45:33.012+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:33.012+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:45:33.408+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:45:33.411+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:33.409+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 45, 33, 11990, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:45:33.412+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:33.412+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:45:33.413+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 45, 33, 11990, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:45:38.920+0000] {processor.py:153} INFO - Started process (PID=116980) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:45:38.921+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:45:38.922+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:38.922+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:45:38.951+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:45:39.036+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:39.036+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:45:39.064+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:39.064+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:45:39.222+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:45:39.226+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:39.222+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 45, 39, 64298, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:45:39.226+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:39.226+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:45:39.228+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 45, 39, 64298, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:45:44.454+0000] {processor.py:153} INFO - Started process (PID=117004) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:45:44.455+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:45:44.457+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:44.456+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:45:44.487+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:45:44.597+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:44.597+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:45:44.639+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:44.639+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:45:44.948+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:45:44.951+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:44.949+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 45, 44, 639072, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:45:44.952+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:44.952+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:45:44.953+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 45, 44, 639072, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:45:50.558+0000] {processor.py:153} INFO - Started process (PID=117042) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:45:50.559+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:45:50.560+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:50.560+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:45:50.581+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:45:50.671+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:50.671+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:45:50.699+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:50.699+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:45:50.811+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:45:50.817+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:50.811+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 45, 50, 698994, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:45:50.819+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:50.818+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:45:50.821+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 45, 50, 698994, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:45:56.948+0000] {processor.py:153} INFO - Started process (PID=117067) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:45:56.950+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:45:56.953+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:56.952+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:45:57.041+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:45:57.217+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:57.217+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:45:57.274+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:57.274+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:45:57.563+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:45:57.567+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:57.563+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 45, 57, 273778, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:45:57.568+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:45:57.567+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:45:57.569+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 45, 57, 273778, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:46:02.795+0000] {processor.py:153} INFO - Started process (PID=117104) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:46:02.796+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:46:02.797+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:02.797+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:46:02.822+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:46:02.934+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:02.934+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:46:03.144+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:03.144+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:46:03.362+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:46:03.369+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:03.363+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 46, 3, 143972, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:46:03.370+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:03.370+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:46:03.371+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 46, 3, 143972, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:46:08.484+0000] {processor.py:153} INFO - Started process (PID=117130) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:46:08.487+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:46:08.488+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:08.488+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:46:08.522+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:46:08.600+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:08.600+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:46:08.816+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:08.816+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:46:09.019+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:46:09.026+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:09.020+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 46, 8, 816306, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:46:09.027+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:09.027+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:46:09.029+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 46, 8, 816306, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:46:14.233+0000] {processor.py:153} INFO - Started process (PID=117159) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:46:14.237+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:46:14.238+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:14.238+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:46:14.259+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:46:14.374+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:14.373+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:46:14.608+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:14.608+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:46:14.899+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:46:14.901+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:14.899+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 46, 14, 608144, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:46:14.902+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:14.902+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:46:14.903+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 46, 14, 608144, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:46:20.042+0000] {processor.py:153} INFO - Started process (PID=117189) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:46:20.043+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:46:20.043+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:20.043+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:46:20.068+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:46:20.168+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:20.168+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:46:20.372+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:20.372+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:46:20.837+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:46:20.845+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:20.838+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 46, 20, 372651, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:46:20.846+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:20.846+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:46:20.848+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 46, 20, 372651, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:46:27.316+0000] {processor.py:153} INFO - Started process (PID=117211) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:46:27.317+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:46:27.318+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:27.318+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:46:27.350+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:46:27.791+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:27.790+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:46:27.848+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:27.847+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:46:28.287+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:46:28.301+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:28.287+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 46, 27, 847504, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:46:28.302+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:28.302+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:46:28.304+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 46, 27, 847504, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:46:33.448+0000] {processor.py:153} INFO - Started process (PID=117245) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:46:33.450+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:46:33.451+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:33.451+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:46:33.506+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:46:33.843+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:33.843+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:46:33.895+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:33.894+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:46:34.176+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:46:34.180+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:34.177+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 46, 33, 894705, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:46:34.180+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:34.180+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:46:34.181+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 46, 33, 894705, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:46:39.711+0000] {processor.py:153} INFO - Started process (PID=117270) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:46:39.712+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:46:39.713+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:39.712+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:46:39.756+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:46:39.904+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:39.904+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:46:39.963+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:39.963+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:46:40.123+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:46:40.136+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:40.123+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 46, 39, 963182, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:46:40.137+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:40.136+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:46:40.138+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 46, 39, 963182, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:46:45.231+0000] {processor.py:153} INFO - Started process (PID=117303) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:46:45.232+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:46:45.233+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:45.233+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:46:45.263+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:46:45.367+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:45.367+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:46:45.398+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:45.398+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:46:45.417+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:46:45.420+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:45.418+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 46, 45, 398381, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:46:45.421+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:45.421+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:46:45.422+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 46, 45, 398381, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:46:50.578+0000] {processor.py:153} INFO - Started process (PID=117330) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:46:50.579+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:46:50.582+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:50.581+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:46:50.616+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:46:50.760+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:50.760+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:46:50.832+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:50.832+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:46:50.876+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:46:50.879+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:50.876+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 46, 50, 831786, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:46:50.880+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:50.880+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:46:50.881+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 46, 50, 831786, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:46:56.000+0000] {processor.py:153} INFO - Started process (PID=117349) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:46:56.002+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:46:56.003+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:56.003+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:46:56.037+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:46:56.162+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:56.161+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:46:56.241+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:56.240+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:46:56.779+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:46:56.783+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:56.779+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 46, 56, 240694, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:46:56.784+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:46:56.783+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:46:56.785+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 46, 56, 240694, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:48:06.216+0000] {processor.py:153} INFO - Started process (PID=117683) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:48:06.220+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:48:06.224+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:06.223+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:48:06.304+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:48:06.478+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:06.477+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:48:06.828+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:06.828+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:48:07.349+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:48:07.376+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:07.349+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 48, 6, 824781, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:48:07.377+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:07.377+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:48:07.379+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 48, 6, 824781, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:48:12.724+0000] {processor.py:153} INFO - Started process (PID=117702) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:48:12.725+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:48:12.726+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:12.726+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:48:12.758+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:48:12.914+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:12.914+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:48:13.328+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:13.328+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:48:13.752+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:48:13.758+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:13.752+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 48, 13, 328469, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:48:13.759+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:13.759+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:48:13.761+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 48, 13, 328469, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:48:19.331+0000] {processor.py:153} INFO - Started process (PID=117739) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:48:19.332+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:48:19.333+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:19.333+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:48:19.358+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:48:19.443+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:19.442+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:48:19.627+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:19.627+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:48:19.645+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:48:19.648+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:19.645+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 48, 19, 627564, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:48:19.648+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:19.648+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:48:19.650+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 48, 19, 627564, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:48:25.246+0000] {processor.py:153} INFO - Started process (PID=117764) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:48:25.250+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:48:25.254+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:25.254+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:48:25.294+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:48:25.450+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:25.450+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:48:25.700+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:25.699+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:48:26.207+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:48:26.213+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:26.208+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 48, 25, 699636, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:48:26.214+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:26.214+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:48:26.219+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 48, 25, 699636, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:48:31.941+0000] {processor.py:153} INFO - Started process (PID=117798) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:48:31.942+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:48:31.944+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:31.943+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:48:31.978+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:48:32.409+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:32.409+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:48:32.469+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:32.469+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:48:32.555+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:48:32.559+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:32.555+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 48, 32, 469480, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:48:32.560+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:32.560+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:48:32.561+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 48, 32, 469480, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:48:38.263+0000] {processor.py:153} INFO - Started process (PID=117826) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:48:38.264+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:48:38.266+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:38.265+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:48:38.302+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:48:38.725+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:38.725+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:48:38.778+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:38.778+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:48:39.183+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:48:39.188+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:39.184+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 48, 38, 777942, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:48:39.189+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:39.189+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:48:39.193+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 48, 38, 777942, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:48:44.327+0000] {processor.py:153} INFO - Started process (PID=117851) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:48:44.329+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:48:44.330+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:44.330+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:48:44.364+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:48:44.728+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:44.728+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:48:44.778+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:44.778+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:48:45.002+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:48:45.007+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:45.003+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 48, 44, 778290, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:48:45.008+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:45.007+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:48:45.009+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 48, 44, 778290, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:48:50.996+0000] {processor.py:153} INFO - Started process (PID=117888) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:48:50.997+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:48:50.998+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:50.998+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:48:51.042+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:48:51.199+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:51.199+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:48:51.247+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:51.247+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:48:51.399+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:48:51.402+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:51.399+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 48, 51, 247125, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:48:51.402+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:51.402+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:48:51.403+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 48, 51, 247125, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:48:56.952+0000] {processor.py:153} INFO - Started process (PID=117913) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:48:56.963+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:48:56.965+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:56.964+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:48:57.072+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:48:57.252+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:57.251+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:48:57.315+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:57.315+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:48:57.732+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:48:57.743+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:57.737+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 48, 57, 314920, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:48:57.744+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:48:57.744+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:48:57.747+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 48, 57, 314920, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:49:02.872+0000] {processor.py:153} INFO - Started process (PID=117949) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:49:02.873+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:49:02.874+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:02.874+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:49:02.920+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:49:03.052+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:03.052+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:49:03.091+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:03.091+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:49:03.420+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:49:03.423+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:03.420+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 49, 3, 91509, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:49:03.423+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:03.423+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:49:03.425+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 49, 3, 91509, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:49:08.770+0000] {processor.py:153} INFO - Started process (PID=117974) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:49:08.771+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:49:08.772+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:08.772+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:49:08.802+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:49:08.893+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:08.892+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:49:09.098+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:09.097+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:49:09.261+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:49:09.266+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:09.262+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 49, 9, 97619, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:49:09.267+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:09.267+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:49:09.269+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 49, 9, 97619, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:49:14.433+0000] {processor.py:153} INFO - Started process (PID=118000) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:49:14.434+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:49:14.436+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:14.435+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:49:14.465+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:49:14.564+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:14.564+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:49:14.761+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:14.761+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:49:15.260+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:49:15.265+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:15.261+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 49, 14, 760964, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:49:15.266+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:15.265+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:49:15.267+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 49, 14, 760964, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:49:20.541+0000] {processor.py:153} INFO - Started process (PID=118036) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:49:20.542+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:49:20.544+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:20.543+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:49:20.585+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:49:20.750+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:20.750+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:49:21.092+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:21.092+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:49:21.326+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:49:21.329+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:21.327+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 49, 21, 92468, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:49:21.330+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:21.330+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:49:21.331+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 49, 21, 92468, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:49:27.075+0000] {processor.py:153} INFO - Started process (PID=118061) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:49:27.078+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:49:27.079+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:27.079+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:49:27.118+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:49:27.255+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:27.255+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:49:27.518+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:27.517+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:49:27.730+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:49:27.732+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:27.730+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 49, 27, 517819, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:49:27.733+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:27.733+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:49:27.734+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 49, 27, 517819, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:49:32.889+0000] {processor.py:153} INFO - Started process (PID=118095) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:49:32.890+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:49:32.892+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:32.892+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:49:32.929+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:49:33.242+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:33.242+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:49:33.294+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:33.294+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:49:33.487+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:49:33.491+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:33.487+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 49, 33, 294190, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:49:33.492+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:33.492+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:49:33.494+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 49, 33, 294190, tzinfo=Timezone('UTC')), 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:49:40.066+0000] {processor.py:153} INFO - Started process (PID=118120) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:49:40.067+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:49:40.068+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:40.068+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:49:40.109+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:49:40.499+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:40.498+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:49:40.583+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:40.582+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:49:40.642+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:49:40.645+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:40.642+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 49, 40, 582455, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:49:40.646+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:40.646+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:49:40.647+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 49, 40, 582455, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:49:46.487+0000] {processor.py:153} INFO - Started process (PID=118148) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:49:46.488+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:49:46.489+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:46.489+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:49:46.591+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:49:47.256+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:47.256+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:49:47.353+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:47.353+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:49:47.400+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:49:47.418+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:47.400+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 49, 47, 352939, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:49:47.419+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:47.419+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:49:47.421+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 49, 47, 352939, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:49:52.705+0000] {processor.py:153} INFO - Started process (PID=118172) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:49:52.706+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:49:52.708+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:52.707+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:49:52.741+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:49:52.996+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:52.996+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:49:53.026+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:53.026+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:49:53.484+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:49:53.489+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:53.485+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 49, 53, 26041, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:49:53.490+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:53.489+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:49:53.491+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 49, 53, 26041, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:49:58.567+0000] {processor.py:153} INFO - Started process (PID=118197) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:49:58.568+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:49:58.570+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:58.569+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:49:58.606+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:49:58.978+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:58.977+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:49:59.053+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:59.052+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:49:59.171+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:49:59.175+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:59.172+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 49, 59, 52553, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:49:59.176+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:49:59.176+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:49:59.187+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 49, 59, 52553, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:50:04.983+0000] {processor.py:153} INFO - Started process (PID=118233) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:50:04.992+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:50:04.994+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:04.993+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:50:05.049+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:50:05.224+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:05.224+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:50:05.373+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:05.373+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:50:06.016+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:50:06.031+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:06.016+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 50, 5, 372897, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:50:06.033+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:06.033+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:50:06.036+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 50, 5, 372897, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:50:11.557+0000] {processor.py:153} INFO - Started process (PID=118258) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:50:11.558+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:50:11.559+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:11.559+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:50:11.585+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:50:11.682+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:11.682+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:50:11.970+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:11.970+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:50:12.164+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:50:12.167+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:12.164+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 50, 11, 970145, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:50:12.167+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:12.167+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:50:12.168+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 50, 11, 970145, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:50:15.266+0000] {processor.py:153} INFO - Started process (PID=118278) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:50:15.267+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:50:15.268+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:15.268+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:50:15.303+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:50:15.432+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:15.432+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:50:15.669+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:15.669+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:50:15.981+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:50:15.985+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:15.981+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 50, 15, 669321, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:50:15.986+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:15.986+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:50:15.987+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 50, 15, 669321, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:50:18.194+0000] {processor.py:153} INFO - Started process (PID=118287) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:50:18.195+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:50:18.196+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:18.196+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:50:18.221+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:50:18.362+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:18.361+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:50:18.637+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:18.637+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:50:19.115+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:50:19.122+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:19.115+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 50, 18, 637484, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:50:19.124+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:19.123+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:50:19.125+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 50, 18, 637484, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:50:19.885+0000] {processor.py:153} INFO - Started process (PID=118303) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:50:19.898+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:50:19.900+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:19.899+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:50:20.024+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:50:20.514+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:20.513+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:50:21.186+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:21.185+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:50:21.766+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:50:21.774+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:21.767+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 50, 21, 185614, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:50:21.775+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:21.775+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:50:21.777+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 50, 21, 185614, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:50:25.150+0000] {processor.py:153} INFO - Started process (PID=118321) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:50:25.151+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:50:25.153+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:25.153+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:50:25.216+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:50:25.399+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:25.399+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:50:25.824+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:25.824+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:50:26.023+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:50:26.028+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:26.024+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 50, 25, 823891, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:50:26.029+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:26.029+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:50:26.031+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 50, 25, 823891, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:50:31.340+0000] {processor.py:153} INFO - Started process (PID=118346) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:50:31.341+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:50:31.342+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:31.342+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:50:31.383+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:50:31.746+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:31.746+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:50:31.793+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:31.792+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:50:32.300+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:50:32.303+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:32.300+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 50, 31, 792735, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:50:32.304+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:32.303+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:50:32.304+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 50, 31, 792735, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:50:38.172+0000] {processor.py:153} INFO - Started process (PID=118381) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:50:38.173+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:50:38.174+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:38.174+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:50:38.210+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:50:38.585+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:38.584+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:50:38.682+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:38.682+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:50:38.937+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:50:38.941+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:38.937+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 50, 38, 682352, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:50:38.942+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:38.941+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:50:38.953+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 50, 38, 682352, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:50:42.505+0000] {processor.py:153} INFO - Started process (PID=118398) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:50:42.506+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:50:42.507+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:42.507+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:50:42.539+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:50:42.923+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:42.923+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:50:42.995+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:42.994+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:50:43.075+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:50:43.082+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:43.076+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 50, 42, 994616, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:50:43.083+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:43.083+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:50:43.085+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 50, 42, 994616, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:50:44.408+0000] {processor.py:153} INFO - Started process (PID=118405) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:50:44.409+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:50:44.411+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:44.410+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:50:44.465+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:50:44.879+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:44.879+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:50:44.964+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:44.963+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:50:45.284+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:50:45.307+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:45.285+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 50, 44, 963568, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:50:45.308+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:45.308+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:50:45.309+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 50, 44, 963568, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:50:51.009+0000] {processor.py:153} INFO - Started process (PID=118430) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:50:51.010+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:50:51.010+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:51.010+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:50:51.039+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:50:51.288+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:51.288+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:50:51.336+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:51.336+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:50:51.635+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:50:51.639+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:51.635+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 50, 51, 336498, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:50:51.640+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:51.640+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:50:51.642+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 50, 51, 336498, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:50:57.026+0000] {processor.py:153} INFO - Started process (PID=118459) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:50:57.027+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:50:57.028+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:57.028+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:50:57.051+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:50:57.321+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:57.321+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:50:57.361+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:57.361+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:50:57.701+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:50:57.704+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:57.702+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 50, 57, 361089, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:50:57.705+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:50:57.704+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:50:57.706+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 50, 57, 361089, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:51:03.235+0000] {processor.py:153} INFO - Started process (PID=118484) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:51:03.236+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:51:03.237+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:03.237+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:51:03.265+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:51:03.534+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:03.534+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:51:03.562+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:03.562+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:51:03.611+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:51:03.614+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:03.611+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 51, 3, 562399, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:51:03.614+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:03.614+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:51:03.615+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 51, 3, 562399, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:51:09.168+0000] {processor.py:153} INFO - Started process (PID=118518) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:51:09.169+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:51:09.171+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:09.170+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:51:09.221+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:51:09.428+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:09.428+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:51:09.566+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:09.565+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:51:10.228+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:51:10.231+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:10.228+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 51, 9, 565368, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:51:10.232+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:10.232+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:51:10.234+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 51, 9, 565368, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:51:16.135+0000] {processor.py:153} INFO - Started process (PID=118546) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:51:16.136+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:51:16.138+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:16.137+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:51:16.163+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:51:16.250+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:16.249+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:51:16.536+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:16.536+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:51:16.959+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:51:16.963+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:16.960+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 51, 16, 535938, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:51:16.964+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:16.964+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:51:16.966+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 51, 16, 535938, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:51:22.344+0000] {processor.py:153} INFO - Started process (PID=118571) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:51:22.345+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:51:22.347+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:22.346+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:51:22.385+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:51:22.549+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:22.548+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:51:23.055+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:23.055+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:51:23.130+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:51:23.139+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:23.130+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 51, 23, 54815, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:51:23.141+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:23.141+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:51:23.143+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 51, 23, 54815, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:51:28.230+0000] {processor.py:153} INFO - Started process (PID=118608) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:51:28.232+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:51:28.233+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:28.233+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:51:28.270+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:51:28.400+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:28.399+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:51:28.676+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:28.675+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:51:29.145+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:51:29.157+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:29.149+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 51, 28, 675603, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:51:29.158+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:29.158+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:51:29.162+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 51, 28, 675603, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:51:34.911+0000] {processor.py:153} INFO - Started process (PID=118633) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:51:34.914+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:51:34.918+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:34.918+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:51:34.964+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:51:35.237+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:35.236+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:51:35.268+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:35.268+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:51:35.326+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:51:35.329+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:35.326+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 51, 35, 268361, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:51:35.329+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:35.329+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:51:35.331+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 51, 35, 268361, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:51:41.172+0000] {processor.py:153} INFO - Started process (PID=118667) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:51:41.174+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:51:41.175+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:41.175+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:51:41.210+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:51:41.593+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:41.593+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:51:41.644+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:41.644+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:51:42.030+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:51:42.046+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:42.031+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 51, 41, 643834, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:51:42.047+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:42.047+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:51:42.050+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 51, 41, 643834, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:51:47.751+0000] {processor.py:153} INFO - Started process (PID=118691) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:51:47.752+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:51:47.754+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:47.753+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:51:47.800+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:51:48.130+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:48.130+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:51:48.161+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:48.161+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:51:48.671+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:51:48.679+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:48.671+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 51, 48, 160910, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:51:48.680+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:48.680+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:51:48.682+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 51, 48, 160910, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:51:53.929+0000] {processor.py:153} INFO - Started process (PID=118716) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:51:53.931+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:51:53.932+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:53.931+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:51:53.963+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:51:54.318+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:54.318+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:51:54.407+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:54.407+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:51:54.912+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:51:54.915+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:54.913+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 51, 54, 407064, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:51:54.915+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:51:54.915+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:51:54.916+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 51, 54, 407064, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:52:00.491+0000] {processor.py:153} INFO - Started process (PID=118755) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:52:00.493+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:52:00.494+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:00.494+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:52:00.528+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:52:00.860+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:00.859+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:52:00.917+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:00.917+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:52:01.170+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:52:01.174+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:01.170+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 52, 0, 916755, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:52:01.175+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:01.175+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:52:01.176+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 52, 0, 916755, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:52:06.942+0000] {processor.py:153} INFO - Started process (PID=118780) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:52:06.943+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:52:06.944+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:06.943+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:52:06.968+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:52:07.470+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:07.470+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:52:07.537+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:07.536+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:52:07.940+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:52:07.946+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:07.941+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 52, 7, 536763, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:52:07.953+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:07.952+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:52:07.956+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 52, 7, 536763, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:52:13.021+0000] {processor.py:153} INFO - Started process (PID=118819) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:52:13.027+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:52:13.029+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:13.029+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:52:13.066+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:52:13.191+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:13.191+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:52:13.398+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:13.397+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:52:13.728+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:52:13.732+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:13.728+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 52, 13, 397743, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:52:13.733+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:13.732+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:52:13.735+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 52, 13, 397743, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:52:18.926+0000] {processor.py:153} INFO - Started process (PID=118844) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:52:18.927+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:52:18.929+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:18.928+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:52:18.952+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:52:19.053+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:19.053+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:52:19.244+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:19.244+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:52:19.355+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:52:19.357+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:19.355+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 52, 19, 244454, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:52:19.358+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:19.358+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:52:19.359+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 52, 19, 244454, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:52:24.639+0000] {processor.py:153} INFO - Started process (PID=118867) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:52:24.640+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:52:24.641+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:24.641+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:52:24.666+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:52:24.759+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:24.759+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:52:24.947+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:24.947+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:52:25.191+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:52:25.201+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:25.192+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 52, 24, 947640, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:52:25.203+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:25.203+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:52:25.207+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 52, 24, 947640, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:52:30.554+0000] {processor.py:153} INFO - Started process (PID=118900) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:52:30.555+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:52:30.557+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:30.556+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:52:30.658+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:52:31.048+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:31.047+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:52:31.459+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:31.459+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:52:31.511+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:52:31.517+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:31.511+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 52, 31, 459188, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:52:31.519+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:31.519+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:52:31.528+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 52, 31, 459188, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:52:37.784+0000] {processor.py:153} INFO - Started process (PID=118925) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:52:37.785+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:52:37.787+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:37.786+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:52:37.813+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:52:38.127+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:38.127+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:52:38.158+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:38.158+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:52:38.376+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:52:38.381+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:38.377+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 52, 38, 158265, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:52:38.381+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:38.381+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:52:38.383+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 52, 38, 158265, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:52:43.911+0000] {processor.py:153} INFO - Started process (PID=118958) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:52:43.925+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:52:43.926+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:43.926+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:52:44.105+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:52:44.714+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:44.713+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:52:44.807+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:44.806+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:52:45.052+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:52:45.057+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:45.053+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 52, 44, 806676, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:52:45.058+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:45.057+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:52:45.059+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 52, 44, 806676, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:52:52.022+0000] {processor.py:153} INFO - Started process (PID=118983) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:52:52.025+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:52:52.026+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:52.026+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:52:52.062+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:52:52.430+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:52.429+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:52:52.482+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:52.481+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:52:52.530+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:52:52.535+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:52.531+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 52, 52, 481647, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:52:52.536+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:52.535+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:52:52.541+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 52, 52, 481647, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:52:57.595+0000] {processor.py:153} INFO - Started process (PID=119017) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:52:57.596+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:52:57.597+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:57.597+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:52:57.619+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:52:57.833+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:57.833+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:52:57.860+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:57.860+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:52:58.100+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:52:58.103+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:58.101+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 52, 57, 860186, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:52:58.104+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:52:58.103+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:52:58.104+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 52, 57, 860186, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:53:03.211+0000] {processor.py:153} INFO - Started process (PID=119041) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:53:03.215+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:53:03.217+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:03.216+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:53:03.267+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:53:03.657+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:03.656+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:53:03.788+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:03.787+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:53:04.221+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:53:04.225+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:04.221+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 53, 3, 787736, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:53:04.226+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:04.226+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:53:04.228+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 53, 3, 787736, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:53:09.513+0000] {processor.py:153} INFO - Started process (PID=119067) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:53:09.514+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:53:09.515+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:09.515+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:53:09.536+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:53:09.742+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:09.741+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:53:09.768+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:09.767+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:53:09.820+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:53:09.823+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:09.820+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 53, 9, 767703, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:53:09.823+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:09.823+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:53:09.824+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 53, 9, 767703, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:53:15.455+0000] {processor.py:153} INFO - Started process (PID=119102) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:53:15.456+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:53:15.457+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:15.456+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:53:15.491+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:53:15.762+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:15.761+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:53:15.807+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:15.806+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:53:16.200+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:53:16.203+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:16.200+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 53, 15, 806672, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:53:16.203+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:16.203+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:53:16.204+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 53, 15, 806672, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:53:21.711+0000] {processor.py:153} INFO - Started process (PID=119129) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:53:21.712+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:53:21.713+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:21.712+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:53:21.744+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:53:21.834+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:21.834+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:53:22.036+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:22.035+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:53:22.508+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:53:22.514+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:22.508+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 53, 22, 35751, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:53:22.516+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:22.515+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:53:22.518+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 53, 22, 35751, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:53:27.701+0000] {processor.py:153} INFO - Started process (PID=119166) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:53:27.702+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:53:27.703+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:27.703+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:53:27.729+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:53:27.858+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:27.858+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:53:28.065+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:28.065+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:53:28.524+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:53:28.528+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:28.525+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 53, 28, 64962, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:53:28.528+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:28.528+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:53:28.530+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 53, 28, 64962, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:53:33.706+0000] {processor.py:153} INFO - Started process (PID=119184) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:53:33.707+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:53:33.709+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:33.708+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:53:33.756+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:53:33.940+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:33.940+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:53:34.331+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:34.330+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:53:34.388+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:53:34.392+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:34.388+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 53, 34, 330415, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:53:34.393+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:34.393+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:53:34.395+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 53, 34, 330415, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:53:40.653+0000] {processor.py:153} INFO - Started process (PID=119217) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:53:40.654+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:53:40.655+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:40.655+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:53:40.693+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:53:40.818+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:40.818+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:53:41.062+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:41.061+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:53:41.128+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:53:41.136+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:41.129+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 53, 41, 61528, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:53:41.137+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:41.137+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:53:41.138+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 53, 41, 61528, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:53:46.328+0000] {processor.py:153} INFO - Started process (PID=119243) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:53:46.330+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:53:46.331+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:46.331+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:53:46.366+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:53:46.690+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:46.690+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:53:46.738+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:46.738+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:53:47.186+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:53:47.190+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:47.187+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 53, 46, 737955, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:53:47.191+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:47.191+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:53:47.193+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 53, 46, 737955, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:53:52.248+0000] {processor.py:153} INFO - Started process (PID=119268) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:53:52.249+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:53:52.250+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:52.250+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:53:52.271+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:53:52.479+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:52.478+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:53:52.513+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:52.513+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:53:52.563+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py:639 RemovedInAirflow3Warning: DAG.full_filepath is deprecated in favour of fileloc
[2024-02-10T18:53:52.566+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:52.563+0000] {dagbag.py:639} ERROR - Failed to write serialized DAG: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 631, in _serialize_dag_capturing_errors
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 149, in write_dag
    (timezone.utcnow() - timedelta(seconds=min_update_interval)) < cls.last_updated,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2893, in scalar
    ret = self.one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2870, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 53, 52, 512933, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:53:52.566+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:52.566+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:53:52.567+0000] {processor.py:178} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 174, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 158, in _handle_dag_file_processing
    callback_requests=callback_requests,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 768, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 645, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.7/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 660, in sync_to_db
    self.dags.values(), processor_subdir=processor_subdir, session=session
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2701, in bulk_write_to_db
    orm_dags: list[DagModel] = with_row_locks(query, of=DagModel, session=session).all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 2919, in _iter
    execution_options={"_sa_orm_load_options": self.load_options},
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1553, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 608, in _assert_active
    code="7s2a",
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.ProgrammingError) can't adapt type 'BaseOperatorMeta'
[SQL: UPDATE dag SET is_active=%(is_active)s, last_parsed_time=%(last_parsed_time)s, owners=%(owners)s, description=%(description)s, schedule_interval=%(schedule_interval)s, timetable_description=%(timetable_description)s, has_import_errors=%(has_import_errors)s WHERE dag.dag_id = %(dag_dag_id)s]
[parameters: {'is_active': True, 'last_parsed_time': datetime.datetime(2024, 2, 10, 18, 53, 52, 512933, tzinfo=Timezone('UTC')), 'owners': 'airflow', 'description': <class 'airflow.providers.http.sensors.http.HttpSensor'>, 'schedule_interval': 'null', 'timetable_description': 'Never, external triggers only', 'has_import_errors': False, 'dag_dag_id': 'httpsensor'}]
(Background on this error at: https://sqlalche.me/e/14/f405) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-02-10T18:53:57.832+0000] {processor.py:153} INFO - Started process (PID=119301) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:53:57.836+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:53:57.838+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:57.837+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:53:57.842+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:53:57.840+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/sensors.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/sensors.py", line 8
    description="HttpSensor,
                           ^
SyntaxError: EOL while scanning string literal
[2024-02-10T18:53:57.842+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:53:57.925+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.098 seconds
[2024-02-10T18:54:01.316+0000] {processor.py:153} INFO - Started process (PID=119313) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:54:01.321+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:54:01.326+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:54:01.326+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:54:01.467+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:54:02.482+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:54:02.481+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:54:02.554+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:54:02.553+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:54:02.605+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 1.302 seconds
[2024-02-10T18:54:09.846+0000] {processor.py:153} INFO - Started process (PID=119347) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:54:09.848+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:54:09.850+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:54:09.849+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:54:09.926+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:54:09.972+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:54:09.971+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:54:10.259+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:54:10.259+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:54:10.311+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.472 seconds
[2024-02-10T18:54:15.525+0000] {processor.py:153} INFO - Started process (PID=119374) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:54:15.526+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:54:15.527+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:54:15.527+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:54:15.553+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:54:15.569+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:54:15.569+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:54:15.794+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:54:15.794+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:54:15.822+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.302 seconds
[2024-02-10T18:54:21.755+0000] {processor.py:153} INFO - Started process (PID=119396) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:54:21.756+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:54:21.757+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:54:21.757+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:54:21.792+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:54:22.157+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:54:22.156+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:54:22.244+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:54:22.243+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:54:22.297+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.556 seconds
[2024-02-10T18:54:27.663+0000] {processor.py:153} INFO - Started process (PID=119434) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:54:27.664+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:54:27.665+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:54:27.665+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:54:27.850+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:54:27.867+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:54:27.867+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:54:27.914+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:54:27.914+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:54:27.943+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.285 seconds
[2024-02-10T18:54:33.626+0000] {processor.py:153} INFO - Started process (PID=119459) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:54:33.627+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:54:33.628+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:54:33.628+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:54:33.691+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:54:33.746+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:54:33.746+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:54:33.843+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:54:33.843+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:54:33.962+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.342 seconds
[2024-02-10T18:54:39.121+0000] {processor.py:153} INFO - Started process (PID=119482) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:54:39.122+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:54:39.124+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:54:39.123+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:54:39.150+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:54:39.182+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:54:39.181+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:54:39.218+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:54:39.218+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:54:39.246+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.130 seconds
[2024-02-10T18:54:44.428+0000] {processor.py:153} INFO - Started process (PID=119514) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:54:44.429+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:54:44.430+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:54:44.429+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:54:44.455+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:54:44.491+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:54:44.491+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:54:44.539+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:54:44.539+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:54:44.722+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.298 seconds
[2024-02-10T18:54:49.858+0000] {processor.py:153} INFO - Started process (PID=119534) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:54:49.859+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:54:49.861+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:54:49.860+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:54:49.915+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:54:49.976+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:54:49.976+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:54:50.254+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:54:50.254+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:54:50.295+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.449 seconds
[2024-02-10T18:54:55.903+0000] {processor.py:153} INFO - Started process (PID=119571) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:54:55.908+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:54:55.909+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:54:55.909+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:54:55.961+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:54:55.992+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:54:55.992+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:54:56.177+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:54:56.177+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:54:56.204+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.317 seconds
[2024-02-10T18:55:01.842+0000] {processor.py:153} INFO - Started process (PID=119596) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:55:01.843+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:55:01.845+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:01.845+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:55:01.870+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:55:01.898+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:01.898+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:55:02.081+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:02.081+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:55:02.117+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.285 seconds
[2024-02-10T18:55:07.202+0000] {processor.py:153} INFO - Started process (PID=119621) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:55:07.203+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:55:07.204+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:07.204+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:55:07.226+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:55:07.256+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:07.256+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:55:07.431+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:07.430+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:55:07.454+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.256 seconds
[2024-02-10T18:55:12.744+0000] {processor.py:153} INFO - Started process (PID=119658) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:55:12.745+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:55:12.747+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:12.746+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:55:12.784+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:55:12.967+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:12.967+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:55:13.003+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:13.003+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:55:13.047+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.310 seconds
[2024-02-10T18:55:18.788+0000] {processor.py:153} INFO - Started process (PID=119683) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:55:18.790+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:55:18.791+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:18.790+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:55:18.877+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:55:19.148+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:19.148+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:55:19.196+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:19.196+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:55:19.240+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.473 seconds
[2024-02-10T18:55:24.328+0000] {processor.py:153} INFO - Started process (PID=119714) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:55:24.328+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:55:24.329+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:24.329+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:55:24.506+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:55:24.536+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:24.536+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:55:24.568+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:24.568+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:55:24.593+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.271 seconds
[2024-02-10T18:55:29.758+0000] {processor.py:153} INFO - Started process (PID=119742) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:55:29.759+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:55:29.760+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:29.760+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:55:29.935+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:55:29.961+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:29.961+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:55:29.991+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:29.991+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:55:30.015+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.261 seconds
[2024-02-10T18:55:35.986+0000] {processor.py:153} INFO - Started process (PID=119766) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:55:35.988+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:55:35.989+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:35.989+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:55:36.022+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:55:36.064+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:36.064+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:55:36.104+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:36.103+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:55:36.139+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.158 seconds
[2024-02-10T18:55:41.357+0000] {processor.py:153} INFO - Started process (PID=119801) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:55:41.358+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:55:41.358+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:41.358+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:55:41.387+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:55:41.421+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:41.421+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:55:41.461+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:41.461+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:55:41.636+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.284 seconds
[2024-02-10T18:55:47.766+0000] {processor.py:153} INFO - Started process (PID=119824) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:55:47.768+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:55:47.769+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:47.769+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:55:47.805+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:55:47.859+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:47.859+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:55:47.922+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:47.922+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:55:48.132+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.371 seconds
[2024-02-10T18:55:53.296+0000] {processor.py:153} INFO - Started process (PID=119858) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:55:53.297+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:55:53.298+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:53.298+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:55:53.337+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:55:53.386+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:53.386+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:55:53.595+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:53.595+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:55:53.620+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.331 seconds
[2024-02-10T18:55:59.157+0000] {processor.py:153} INFO - Started process (PID=119880) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:55:59.159+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:55:59.160+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:59.160+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:55:59.199+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:55:59.245+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:59.244+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:55:59.453+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:55:59.452+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:55:59.475+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.323 seconds
[2024-02-10T18:56:05.090+0000] {processor.py:153} INFO - Started process (PID=119905) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:56:05.091+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:56:05.093+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:56:05.092+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:56:05.131+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:56:05.178+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:56:05.178+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:56:05.379+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:56:05.378+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:56:05.408+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.324 seconds
[2024-02-10T18:56:10.836+0000] {processor.py:153} INFO - Started process (PID=119941) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:56:10.837+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:56:10.838+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:56:10.838+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:56:10.872+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:56:11.079+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:56:11.079+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:56:11.108+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:56:11.108+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:56:11.134+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.303 seconds
[2024-02-10T18:56:16.471+0000] {processor.py:153} INFO - Started process (PID=119962) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:56:16.474+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:56:16.475+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:56:16.475+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:56:16.499+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:56:16.712+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:56:16.712+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:56:16.753+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:56:16.752+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:56:16.786+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.321 seconds
[2024-02-10T18:56:21.855+0000] {processor.py:153} INFO - Started process (PID=119999) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:56:21.856+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:56:21.857+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:56:21.857+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:56:21.879+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:56:22.046+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:56:22.046+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:56:22.080+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:56:22.080+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:56:22.108+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.258 seconds
[2024-02-10T18:56:27.206+0000] {processor.py:153} INFO - Started process (PID=120024) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:56:27.208+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:56:27.209+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:56:27.209+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:56:27.422+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:56:27.448+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:56:27.448+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:56:27.482+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:56:27.482+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:56:27.510+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.311 seconds
[2024-02-10T18:56:32.555+0000] {processor.py:153} INFO - Started process (PID=120049) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:56:32.556+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:56:32.557+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:56:32.557+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:56:32.580+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:56:32.620+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:56:32.620+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:56:32.672+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:56:32.672+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:56:32.707+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.156 seconds
[2024-02-10T18:56:37.813+0000] {processor.py:153} INFO - Started process (PID=120082) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:56:37.815+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:56:37.816+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:56:37.816+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:56:37.842+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:56:37.870+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:56:37.870+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:56:37.908+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:56:37.907+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:56:37.930+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.122 seconds
[2024-02-10T18:56:44.627+0000] {processor.py:153} INFO - Started process (PID=120101) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:56:44.628+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:56:44.628+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:56:44.628+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:56:44.650+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:56:44.679+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:56:44.679+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:56:44.711+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:56:44.711+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:56:44.864+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.241 seconds
[2024-02-10T18:56:50.074+0000] {processor.py:153} INFO - Started process (PID=120133) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:56:50.078+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:56:50.079+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:56:50.079+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:56:50.111+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:56:50.154+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:56:50.154+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:56:50.211+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:56:50.211+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:56:50.441+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.373 seconds
[2024-02-10T18:56:55.733+0000] {processor.py:153} INFO - Started process (PID=120157) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:56:55.735+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:56:55.736+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:56:55.736+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:56:55.760+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:56:55.792+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:56:55.792+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:56:56.004+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:56:56.003+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:56:56.033+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.307 seconds
[2024-02-10T18:57:01.586+0000] {processor.py:153} INFO - Started process (PID=120190) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:57:01.587+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:57:01.588+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:01.587+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:57:01.611+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:57:01.654+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:01.654+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:57:01.845+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:01.844+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:57:01.878+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.296 seconds
[2024-02-10T18:57:07.019+0000] {processor.py:153} INFO - Started process (PID=120214) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:57:07.020+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:57:07.021+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:07.021+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:57:07.046+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:57:07.082+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:07.082+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:57:07.270+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:07.270+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:57:07.293+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.278 seconds
[2024-02-10T18:57:12.904+0000] {processor.py:153} INFO - Started process (PID=120239) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:57:12.906+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:57:12.907+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:12.907+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:57:12.942+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:57:13.174+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:13.173+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:57:13.221+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:13.220+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:57:13.246+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.346 seconds
[2024-02-10T18:57:18.575+0000] {processor.py:153} INFO - Started process (PID=120272) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:57:18.576+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:57:18.577+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:18.577+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:57:18.672+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:57:18.949+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:18.948+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:57:19.091+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:19.089+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:57:19.236+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.667 seconds
[2024-02-10T18:57:25.013+0000] {processor.py:153} INFO - Started process (PID=120297) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:57:25.014+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:57:25.015+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:25.015+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:57:25.037+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:57:25.202+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:25.202+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:57:25.230+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:25.230+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:57:25.253+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.243 seconds
[2024-02-10T18:57:30.372+0000] {processor.py:153} INFO - Started process (PID=120322) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:57:30.373+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:57:30.374+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:30.374+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:57:30.564+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:57:30.595+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:30.595+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:57:30.639+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:30.639+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:57:30.661+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.294 seconds
[2024-02-10T18:57:35.779+0000] {processor.py:153} INFO - Started process (PID=120358) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:57:35.780+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:57:35.781+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:35.781+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:57:35.852+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:57:35.943+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:35.943+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:57:36.034+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:36.034+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:57:36.107+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.345 seconds
[2024-02-10T18:57:41.159+0000] {processor.py:153} INFO - Started process (PID=120380) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:57:41.160+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:57:41.161+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:41.161+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:57:41.194+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:57:41.241+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:41.241+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:57:41.296+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:41.296+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:57:41.492+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.339 seconds
[2024-02-10T18:57:47.266+0000] {processor.py:153} INFO - Started process (PID=120407) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:57:47.273+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:57:47.275+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:47.275+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:57:47.358+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:57:47.421+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:47.420+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:57:47.497+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:47.497+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:57:47.767+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.512 seconds
[2024-02-10T18:57:52.987+0000] {processor.py:153} INFO - Started process (PID=120430) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:57:52.988+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:57:52.990+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:52.989+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:57:53.021+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:57:53.072+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:53.072+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:57:53.293+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:53.293+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:57:53.314+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.332 seconds
[2024-02-10T18:57:58.658+0000] {processor.py:153} INFO - Started process (PID=120454) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:57:58.659+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:57:58.660+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:58.660+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:57:58.684+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:57:58.715+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:58.715+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:57:58.895+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:57:58.895+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:57:58.922+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.268 seconds
[2024-02-10T18:58:04.037+0000] {processor.py:153} INFO - Started process (PID=120487) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:58:04.038+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:58:04.039+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:58:04.039+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:58:04.071+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:58:04.114+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:58:04.113+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:58:04.390+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:58:04.390+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:58:04.478+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.448 seconds
[2024-02-10T18:58:10.290+0000] {processor.py:153} INFO - Started process (PID=120514) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:58:10.292+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:58:10.293+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:58:10.293+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:58:10.330+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:58:10.380+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:58:10.379+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:58:10.601+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:58:10.601+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:58:10.626+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.341 seconds
[2024-02-10T18:58:16.715+0000] {processor.py:153} INFO - Started process (PID=120539) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:58:16.716+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:58:16.718+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:58:16.717+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:58:16.763+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:58:17.035+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:58:17.034+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:58:17.114+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:58:17.113+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:58:17.151+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.444 seconds
[2024-02-10T18:58:24.423+0000] {processor.py:153} INFO - Started process (PID=120572) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:58:24.425+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:58:24.426+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:58:24.426+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:58:24.469+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:58:24.730+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:58:24.730+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:58:24.837+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:58:24.837+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:58:24.951+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.535 seconds
[2024-02-10T18:58:30.308+0000] {processor.py:153} INFO - Started process (PID=120596) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:58:30.310+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:58:30.311+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:58:30.311+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:58:30.538+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:58:30.573+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:58:30.572+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:58:30.639+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:58:30.638+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:58:30.665+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.373 seconds
[2024-02-10T18:58:36.388+0000] {processor.py:153} INFO - Started process (PID=120621) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:58:36.391+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:58:36.397+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:58:36.397+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:58:36.707+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:58:36.778+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:58:36.778+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:58:36.880+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:58:36.879+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:58:36.968+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.599 seconds
[2024-02-10T18:58:42.754+0000] {processor.py:153} INFO - Started process (PID=120644) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:58:42.758+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:58:42.760+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:58:42.759+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:58:43.063+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:58:43.108+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:58:43.108+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:58:43.164+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:58:43.164+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:58:43.196+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.450 seconds
[2024-02-10T18:58:48.456+0000] {processor.py:153} INFO - Started process (PID=120666) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:58:48.457+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:58:48.458+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:58:48.458+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:58:48.504+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:58:48.570+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:58:48.570+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:58:48.646+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:58:48.645+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:58:48.693+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.249 seconds
[2024-02-10T18:58:54.909+0000] {processor.py:153} INFO - Started process (PID=120701) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:58:54.911+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:58:54.912+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:58:54.911+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:58:54.945+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:58:55.015+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:58:55.015+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:58:55.100+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:58:55.100+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:58:55.400+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.496 seconds
[2024-02-10T18:59:00.530+0000] {processor.py:153} INFO - Started process (PID=120727) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:59:00.533+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:59:00.534+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:59:00.534+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:59:00.571+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:59:00.619+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:59:00.618+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:59:00.828+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:59:00.828+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:59:00.864+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.343 seconds
[2024-02-10T18:59:06.501+0000] {processor.py:153} INFO - Started process (PID=120748) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:59:06.502+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:59:06.503+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:59:06.502+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:59:06.538+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:59:06.593+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:59:06.593+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:59:06.875+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:59:06.875+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:59:06.921+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.427 seconds
[2024-02-10T18:59:14.956+0000] {processor.py:153} INFO - Started process (PID=120785) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:59:14.973+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:59:14.975+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:59:14.975+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:59:15.032+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:59:15.132+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:59:15.131+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:59:15.504+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:59:15.504+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:59:15.548+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.603 seconds
[2024-02-10T18:59:21.726+0000] {processor.py:153} INFO - Started process (PID=120810) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:59:21.727+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:59:21.728+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:59:21.728+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:59:21.760+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:59:21.808+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:59:21.808+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:59:22.029+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:59:22.029+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:59:22.058+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.338 seconds
[2024-02-10T18:59:28.228+0000] {processor.py:153} INFO - Started process (PID=120844) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:59:28.230+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:59:28.235+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:59:28.235+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:59:28.298+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:59:28.565+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:59:28.565+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:59:28.618+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:59:28.617+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:59:28.655+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.438 seconds
[2024-02-10T18:59:34.306+0000] {processor.py:153} INFO - Started process (PID=120870) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:59:34.308+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:59:34.309+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:59:34.309+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:59:34.592+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:59:34.636+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:59:34.636+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:59:34.696+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:59:34.695+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:59:34.742+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.443 seconds
[2024-02-10T18:59:40.081+0000] {processor.py:153} INFO - Started process (PID=120895) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:59:40.083+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:59:40.085+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:59:40.084+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:59:40.344+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:59:40.385+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:59:40.385+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:59:40.435+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:59:40.434+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:59:40.473+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.397 seconds
[2024-02-10T18:59:48.553+0000] {processor.py:153} INFO - Started process (PID=120932) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:59:48.554+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:59:48.556+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:59:48.555+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:59:48.833+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:59:48.931+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:59:48.931+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:59:48.995+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:59:48.994+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:59:49.053+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.539 seconds
[2024-02-10T18:59:54.555+0000] {processor.py:153} INFO - Started process (PID=120956) to work on /opt/airflow/dags/sensors.py
[2024-02-10T18:59:54.557+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T18:59:54.562+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:59:54.561+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T18:59:54.610+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T18:59:54.665+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:59:54.664+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T18:59:54.721+0000] {logging_mixin.py:137} INFO - [2024-02-10T18:59:54.720+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T18:59:54.938+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.391 seconds
[2024-02-10T19:00:00.872+0000] {processor.py:153} INFO - Started process (PID=120991) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:00:00.873+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:00:00.875+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:00:00.875+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:00:00.912+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:00:00.956+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:00:00.956+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:00:01.038+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:00:01.038+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:00:01.386+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.520 seconds
[2024-02-10T19:00:07.598+0000] {processor.py:153} INFO - Started process (PID=121010) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:00:07.599+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:00:07.600+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:00:07.600+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:00:07.632+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:00:07.679+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:00:07.678+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:00:07.926+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:00:07.926+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:00:07.964+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.372 seconds
[2024-02-10T19:00:13.620+0000] {processor.py:153} INFO - Started process (PID=121035) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:00:13.622+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:00:13.623+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:00:13.623+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:00:13.671+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:00:13.726+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:00:13.726+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:00:13.985+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:00:13.985+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:00:14.020+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.406 seconds
[2024-02-10T19:00:19.098+0000] {processor.py:153} INFO - Started process (PID=121066) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:00:19.099+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:00:19.100+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:00:19.100+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:00:19.144+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:00:19.197+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:00:19.196+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:00:19.489+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:00:19.489+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:00:19.528+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.439 seconds
[2024-02-10T19:00:26.364+0000] {processor.py:153} INFO - Started process (PID=121093) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:00:26.365+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:00:26.366+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:00:26.366+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:00:26.404+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:00:26.461+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:00:26.461+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:00:26.728+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:00:26.728+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:00:26.778+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.420 seconds
[2024-02-10T19:00:32.138+0000] {processor.py:153} INFO - Started process (PID=121114) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:00:32.139+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:00:32.140+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:00:32.140+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:00:32.185+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:00:32.392+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:00:32.392+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:00:32.443+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:00:32.443+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:00:32.491+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.360 seconds
[2024-02-10T19:00:39.050+0000] {processor.py:153} INFO - Started process (PID=121148) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:00:39.051+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:00:39.056+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:00:39.056+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:00:39.109+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:00:39.434+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:00:39.434+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:00:39.494+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:00:39.494+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:00:39.530+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.486 seconds
[2024-02-10T19:00:45.433+0000] {processor.py:153} INFO - Started process (PID=121173) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:00:45.437+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:00:45.438+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:00:45.438+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:00:45.659+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:00:45.696+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:00:45.696+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:00:45.739+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:00:45.739+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:00:45.769+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.343 seconds
[2024-02-10T19:00:51.456+0000] {processor.py:153} INFO - Started process (PID=121208) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:00:51.469+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:00:51.470+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:00:51.470+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:00:51.754+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:00:51.794+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:00:51.793+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:00:51.845+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:00:51.844+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:00:51.878+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.427 seconds
[2024-02-10T19:00:57.132+0000] {processor.py:153} INFO - Started process (PID=121236) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:00:57.133+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:00:57.134+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:00:57.134+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:00:57.373+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:00:57.403+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:00:57.403+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:00:57.486+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:00:57.486+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:00:57.531+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.405 seconds
[2024-02-10T19:01:02.787+0000] {processor.py:153} INFO - Started process (PID=121261) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:01:02.788+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:01:02.790+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:01:02.790+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:01:03.072+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:01:03.111+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:01:03.111+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:01:03.184+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:01:03.183+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:01:03.281+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.499 seconds
[2024-02-10T19:01:08.826+0000] {processor.py:153} INFO - Started process (PID=121292) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:01:08.827+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:01:08.829+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:01:08.828+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:01:08.869+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:01:08.916+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:01:08.916+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:01:08.978+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:01:08.978+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:01:09.022+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.201 seconds
[2024-02-10T19:01:14.690+0000] {processor.py:153} INFO - Started process (PID=121320) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:01:14.692+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:01:14.693+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:01:14.693+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:01:14.745+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:01:14.804+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:01:14.803+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:01:14.864+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:01:14.863+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:01:14.897+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.213 seconds
[2024-02-10T19:01:20.988+0000] {processor.py:153} INFO - Started process (PID=121341) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:01:21.360+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:01:21.364+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:01:21.363+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:01:21.441+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:01:21.483+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:01:21.483+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:01:21.526+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:01:21.526+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:01:22.346+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 1.361 seconds
[2024-02-10T19:01:28.215+0000] {processor.py:153} INFO - Started process (PID=121378) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:01:28.225+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:01:28.227+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:01:28.227+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:01:28.261+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:01:28.330+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:01:28.330+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:01:28.480+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:01:28.480+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:01:28.550+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.340 seconds
[2024-02-10T19:01:34.041+0000] {processor.py:153} INFO - Started process (PID=121403) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:01:34.042+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:01:34.043+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:01:34.043+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:01:34.082+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:01:34.126+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:01:34.125+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:01:34.167+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:01:34.167+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:01:34.200+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.166 seconds
[2024-02-10T19:01:40.584+0000] {processor.py:153} INFO - Started process (PID=121428) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:01:40.587+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:01:40.590+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:01:40.589+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:01:40.630+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:01:40.667+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:01:40.667+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:01:40.710+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:01:40.710+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:01:40.957+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.385 seconds
[2024-02-10T19:01:46.890+0000] {processor.py:153} INFO - Started process (PID=121462) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:01:46.894+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:01:46.895+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:01:46.895+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:01:46.943+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:01:47.067+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:01:47.067+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:01:47.136+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:01:47.136+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:01:47.179+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.296 seconds
[2024-02-10T19:01:53.932+0000] {processor.py:153} INFO - Started process (PID=121485) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:01:53.934+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:01:53.936+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:01:53.935+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:01:53.977+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:01:54.038+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:01:54.038+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:01:54.109+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:01:54.108+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:01:54.142+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.220 seconds
[2024-02-10T19:02:01.552+0000] {processor.py:153} INFO - Started process (PID=121510) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:02:01.554+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:02:01.555+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:01.554+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:02:01.593+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:02:01.654+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:01.653+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:02:01.715+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:01.715+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:02:01.771+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.225 seconds
[2024-02-10T19:02:07.118+0000] {processor.py:153} INFO - Started process (PID=121544) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:02:07.119+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:02:07.122+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:07.120+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:02:07.282+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:02:07.392+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:07.392+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:02:07.545+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:07.545+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:02:07.603+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.507 seconds
[2024-02-10T19:02:12.932+0000] {processor.py:153} INFO - Started process (PID=121571) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:02:12.933+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:02:12.935+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:12.934+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:02:12.972+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:02:13.019+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:13.018+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:02:13.075+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:13.074+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:02:13.122+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.197 seconds
[2024-02-10T19:02:18.237+0000] {processor.py:153} INFO - Started process (PID=121583) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:02:18.238+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:02:18.244+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:18.239+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:02:18.288+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:02:18.352+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:18.352+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:02:18.421+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:18.420+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:02:18.462+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.238 seconds
[2024-02-10T19:02:24.939+0000] {processor.py:153} INFO - Started process (PID=121620) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:02:24.940+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:02:24.942+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:24.942+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:02:24.976+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:02:25.021+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:25.021+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:02:25.077+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:25.077+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:02:25.114+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.180 seconds
[2024-02-10T19:02:30.582+0000] {processor.py:153} INFO - Started process (PID=121641) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:02:30.583+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:02:30.584+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:30.584+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:02:30.628+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:02:30.673+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:30.673+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:02:30.709+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:30.709+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:02:30.739+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.171 seconds
[2024-02-10T19:02:35.938+0000] {processor.py:153} INFO - Started process (PID=121669) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:02:35.939+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:02:35.940+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:35.940+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:02:35.974+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:02:36.011+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:36.010+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:02:36.064+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:36.064+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:02:36.094+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.162 seconds
[2024-02-10T19:02:42.703+0000] {processor.py:153} INFO - Started process (PID=121697) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:02:42.704+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:02:42.705+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:42.705+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:02:42.745+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:02:42.792+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:42.792+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:02:42.845+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:42.845+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:02:42.879+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.183 seconds
[2024-02-10T19:02:48.423+0000] {processor.py:153} INFO - Started process (PID=121719) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:02:48.424+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:02:48.426+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:48.426+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:02:48.449+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:02:48.479+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:48.478+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:02:48.512+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:48.512+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:02:48.534+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.118 seconds
[2024-02-10T19:02:53.862+0000] {processor.py:153} INFO - Started process (PID=121744) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:02:53.864+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:02:53.865+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:53.865+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:02:53.898+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:02:53.938+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:53.938+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:02:53.989+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:53.989+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:02:54.023+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.168 seconds
[2024-02-10T19:02:59.593+0000] {processor.py:153} INFO - Started process (PID=121781) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:02:59.594+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:02:59.596+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:59.595+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:02:59.695+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:02:59.752+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:59.752+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:02:59.805+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:02:59.805+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:02:59.873+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.306 seconds
[2024-02-10T19:03:05.251+0000] {processor.py:153} INFO - Started process (PID=121806) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:03:05.252+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:03:05.253+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:03:05.253+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:03:05.287+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:03:05.331+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:03:05.330+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:03:05.388+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:03:05.388+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:03:05.422+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.177 seconds
[2024-02-10T19:03:10.959+0000] {processor.py:153} INFO - Started process (PID=121840) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:03:10.966+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:03:10.970+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:03:10.969+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:03:11.035+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:03:11.089+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:03:11.089+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:03:11.193+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:03:11.193+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:03:11.255+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.302 seconds
[2024-02-10T19:03:16.590+0000] {processor.py:153} INFO - Started process (PID=121862) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:03:16.594+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:03:16.596+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:03:16.596+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:03:16.639+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:03:16.717+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:03:16.716+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:03:16.804+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:03:16.804+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:03:16.868+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.292 seconds
[2024-02-10T19:03:22.280+0000] {processor.py:153} INFO - Started process (PID=121885) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:03:22.281+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:03:22.282+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:03:22.282+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:03:22.310+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:03:22.351+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:03:22.351+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:03:22.412+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:03:22.408+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:03:22.445+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.174 seconds
[2024-02-10T19:03:28.437+0000] {processor.py:153} INFO - Started process (PID=121917) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:03:28.438+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:03:28.440+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:03:28.439+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:03:28.513+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:03:28.594+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:03:28.594+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:03:28.655+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:03:28.654+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:03:28.707+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.275 seconds
[2024-02-10T19:03:34.011+0000] {processor.py:153} INFO - Started process (PID=121945) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:03:34.013+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:03:34.015+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:03:34.015+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:03:34.076+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:03:34.143+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:03:34.143+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:03:34.210+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:03:34.210+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:03:34.267+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.270 seconds
[2024-02-10T19:03:39.749+0000] {processor.py:153} INFO - Started process (PID=121967) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:03:39.750+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:03:39.751+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:03:39.751+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:03:39.791+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:03:39.832+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:03:39.831+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:03:39.877+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:03:39.876+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:03:39.905+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.161 seconds
[2024-02-10T19:03:45.564+0000] {processor.py:153} INFO - Started process (PID=122001) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:03:45.565+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:03:45.566+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:03:45.566+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:03:45.602+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:03:45.665+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:03:45.665+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:03:45.749+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:03:45.748+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:03:45.784+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.237 seconds
[2024-02-10T19:03:51.541+0000] {processor.py:153} INFO - Started process (PID=122027) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:03:51.542+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:03:51.543+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:03:51.543+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:03:51.577+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:03:51.629+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:03:51.628+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:03:51.692+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:03:51.692+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:03:51.730+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.195 seconds
[2024-02-10T19:03:57.291+0000] {processor.py:153} INFO - Started process (PID=122047) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:03:57.292+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:03:57.293+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:03:57.293+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:03:57.334+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:03:57.383+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:03:57.383+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:03:57.435+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:03:57.435+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:03:57.468+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.185 seconds
[2024-02-10T19:04:02.520+0000] {processor.py:153} INFO - Started process (PID=122084) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:04:02.521+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:04:02.522+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:04:02.522+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:04:02.546+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:04:02.587+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:04:02.587+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:04:02.624+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:04:02.624+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:04:02.648+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.132 seconds
[2024-02-10T19:04:07.778+0000] {processor.py:153} INFO - Started process (PID=122109) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:04:07.779+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:04:07.779+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:04:07.779+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:04:07.807+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:04:07.845+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:04:07.845+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:04:07.899+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:04:07.898+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:04:07.936+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.163 seconds
[2024-02-10T19:04:13.754+0000] {processor.py:153} INFO - Started process (PID=122134) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:04:13.755+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:04:13.758+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:04:13.757+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:04:13.801+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:04:13.854+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:04:13.854+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:04:13.951+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:04:13.951+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:04:14.152+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.406 seconds
[2024-02-10T19:04:19.926+0000] {processor.py:153} INFO - Started process (PID=122165) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:04:19.928+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:04:19.929+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:04:19.929+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:04:19.963+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:04:20.006+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:04:20.006+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:04:20.064+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:04:20.063+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:04:20.115+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.195 seconds
[2024-02-10T19:04:25.531+0000] {processor.py:153} INFO - Started process (PID=122187) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:04:25.536+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:04:25.537+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:04:25.537+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:04:25.574+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:04:25.624+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:04:25.623+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:04:25.677+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:04:25.677+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:04:25.712+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.189 seconds
[2024-02-10T19:04:31.430+0000] {processor.py:153} INFO - Started process (PID=122212) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:04:31.431+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:04:31.433+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:04:31.432+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:04:31.464+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:04:31.510+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:04:31.509+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:04:31.562+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:04:31.561+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:04:31.595+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.170 seconds
[2024-02-10T19:04:38.267+0000] {processor.py:153} INFO - Started process (PID=122249) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:04:38.269+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:04:38.270+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:04:38.270+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:04:38.312+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:04:38.363+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:04:38.363+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:04:38.427+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:04:38.427+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:04:38.459+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.197 seconds
[2024-02-10T19:04:44.302+0000] {processor.py:153} INFO - Started process (PID=122271) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:04:44.303+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:04:44.304+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:04:44.304+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:04:44.344+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:04:44.444+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:04:44.444+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:04:44.565+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:04:44.565+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:04:44.646+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.363 seconds
[2024-02-10T19:04:49.791+0000] {processor.py:153} INFO - Started process (PID=122305) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:04:49.792+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:04:49.794+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:04:49.793+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:04:49.830+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:04:49.913+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:04:49.913+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:04:50.006+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:04:50.005+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:04:50.061+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.275 seconds
[2024-02-10T19:04:56.044+0000] {processor.py:153} INFO - Started process (PID=122329) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:04:56.045+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:04:56.045+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:04:56.045+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:04:56.075+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:04:56.113+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:04:56.113+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:04:56.163+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:04:56.162+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:04:56.194+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.154 seconds
[2024-02-10T19:05:01.379+0000] {processor.py:153} INFO - Started process (PID=122347) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:05:01.380+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:05:01.381+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:01.381+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:05:01.420+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:05:01.469+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:01.468+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:05:01.525+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:01.524+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:05:01.559+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.188 seconds
[2024-02-10T19:05:07.001+0000] {processor.py:153} INFO - Started process (PID=122382) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:05:07.002+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:05:07.003+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:07.003+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:05:07.038+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:05:07.086+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:07.086+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:05:07.148+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:07.147+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:05:07.217+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.221 seconds
[2024-02-10T19:05:12.560+0000] {processor.py:153} INFO - Started process (PID=122407) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:05:12.562+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:05:12.564+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:12.564+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:05:12.607+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:05:12.664+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:12.663+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:05:12.726+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:12.726+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:05:12.770+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.217 seconds
[2024-02-10T19:05:17.959+0000] {processor.py:153} INFO - Started process (PID=122432) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:05:17.960+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:05:17.961+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:17.961+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:05:17.992+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:05:18.036+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:18.036+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:05:18.083+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:18.083+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:05:18.116+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.163 seconds
[2024-02-10T19:05:23.597+0000] {processor.py:153} INFO - Started process (PID=122464) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:05:23.598+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:05:23.599+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:23.599+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:05:23.652+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:05:23.719+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:23.719+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:05:23.798+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:23.798+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:05:23.839+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.251 seconds
[2024-02-10T19:05:29.530+0000] {processor.py:153} INFO - Started process (PID=122492) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:05:29.531+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:05:29.533+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:29.532+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:05:29.567+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:05:29.616+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:29.616+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:05:29.670+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:29.670+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:05:29.712+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.187 seconds
[2024-02-10T19:05:35.294+0000] {processor.py:153} INFO - Started process (PID=122517) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:05:35.296+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:05:35.298+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:35.297+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:05:35.332+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:05:35.374+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:35.374+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:05:35.419+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:35.419+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:05:35.451+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.162 seconds
[2024-02-10T19:05:41.356+0000] {processor.py:153} INFO - Started process (PID=122548) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:05:41.357+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:05:41.359+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:41.359+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:05:41.406+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:05:41.474+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:41.474+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:05:41.569+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:41.569+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:05:41.609+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.260 seconds
[2024-02-10T19:05:46.854+0000] {processor.py:153} INFO - Started process (PID=122573) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:05:46.855+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:05:46.856+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:46.855+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:05:46.880+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:05:46.912+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:46.912+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:05:46.949+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:46.949+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:05:46.976+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.127 seconds
[2024-02-10T19:05:52.442+0000] {processor.py:153} INFO - Started process (PID=122597) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:05:52.443+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:05:52.444+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:52.444+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:05:52.480+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:05:52.526+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:52.526+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:05:52.583+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:52.583+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:05:52.619+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.183 seconds
[2024-02-10T19:05:57.891+0000] {processor.py:153} INFO - Started process (PID=122629) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:05:57.892+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:05:57.899+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:57.899+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:05:57.966+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:05:58.057+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:58.056+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:05:58.273+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:05:58.272+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:05:58.470+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.590 seconds
[2024-02-10T19:06:03.576+0000] {processor.py:153} INFO - Started process (PID=122652) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:06:03.577+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:06:03.578+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:03.578+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:06:03.612+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:06:03.656+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:03.655+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:06:03.704+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:03.704+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:06:03.740+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.172 seconds
[2024-02-10T19:06:09.344+0000] {processor.py:153} INFO - Started process (PID=122677) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:06:09.345+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:06:09.346+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:09.346+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:06:09.381+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:06:09.427+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:09.427+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:06:09.491+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:09.491+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:06:09.528+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.191 seconds
[2024-02-10T19:06:14.637+0000] {processor.py:153} INFO - Started process (PID=122712) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:06:14.638+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:06:14.639+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:14.639+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:06:14.681+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:06:14.728+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:14.728+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:06:14.798+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:14.798+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:06:14.828+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.198 seconds
[2024-02-10T19:06:20.049+0000] {processor.py:153} INFO - Started process (PID=122736) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:06:20.050+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:06:20.051+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:20.051+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:06:20.083+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:06:20.118+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:20.118+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:06:20.170+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:20.170+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:06:20.203+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.160 seconds
[2024-02-10T19:06:25.343+0000] {processor.py:153} INFO - Started process (PID=122757) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:06:25.344+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:06:25.352+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:25.352+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:06:25.446+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:06:25.601+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:25.601+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:06:25.674+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:25.674+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:06:25.709+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.381 seconds
[2024-02-10T19:06:30.765+0000] {processor.py:153} INFO - Started process (PID=122792) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:06:30.766+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:06:30.767+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:30.767+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:06:30.798+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:06:30.837+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:30.836+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:06:30.886+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:30.886+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:06:30.911+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.155 seconds
[2024-02-10T19:06:36.068+0000] {processor.py:153} INFO - Started process (PID=122817) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:06:36.069+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:06:36.070+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:36.070+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:06:36.095+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:06:36.129+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:36.129+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:06:36.165+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:36.164+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:06:36.205+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.140 seconds
[2024-02-10T19:06:41.856+0000] {processor.py:153} INFO - Started process (PID=122848) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:06:41.857+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:06:41.858+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:41.858+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:06:41.899+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:06:41.949+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:41.949+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:06:42.014+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:42.014+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:06:42.054+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.205 seconds
[2024-02-10T19:06:47.379+0000] {processor.py:153} INFO - Started process (PID=122876) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:06:47.386+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:06:47.390+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:47.389+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:06:47.454+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:06:47.497+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:47.496+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:06:47.537+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:47.537+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:06:47.565+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.206 seconds
[2024-02-10T19:06:53.154+0000] {processor.py:153} INFO - Started process (PID=122901) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:06:53.155+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:06:53.156+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:53.156+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:06:53.190+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:06:53.233+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:53.232+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:06:53.286+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:53.286+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:06:53.320+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.172 seconds
[2024-02-10T19:06:58.371+0000] {processor.py:153} INFO - Started process (PID=122938) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:06:58.373+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:06:58.374+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:58.374+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:06:58.411+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:06:58.447+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:58.447+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:06:58.492+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:06:58.492+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:06:58.525+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.160 seconds
[2024-02-10T19:07:04.158+0000] {processor.py:153} INFO - Started process (PID=122963) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:07:04.159+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:07:04.161+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:04.160+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:07:04.188+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:07:04.232+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:04.232+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:07:04.269+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:04.269+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:07:04.304+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.150 seconds
[2024-02-10T19:07:09.789+0000] {processor.py:153} INFO - Started process (PID=122981) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:07:09.790+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:07:09.791+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:09.791+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:07:09.829+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:07:09.874+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:09.874+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:07:09.929+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:09.929+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:07:09.967+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.183 seconds
[2024-02-10T19:07:15.174+0000] {processor.py:153} INFO - Started process (PID=123017) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:07:15.175+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:07:15.176+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:15.176+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:07:15.230+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:07:15.295+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:15.295+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:07:15.365+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:15.365+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:07:15.405+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.238 seconds
[2024-02-10T19:07:21.432+0000] {processor.py:153} INFO - Started process (PID=123040) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:07:21.436+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:07:21.445+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:21.445+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:07:21.484+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:07:21.533+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:21.533+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:07:21.589+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:21.589+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:07:21.631+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.204 seconds
[2024-02-10T19:07:27.087+0000] {processor.py:153} INFO - Started process (PID=123074) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:07:27.110+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:07:27.111+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:27.111+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:07:27.198+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:07:27.329+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:27.329+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:07:27.503+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:27.503+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:07:27.576+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.499 seconds
[2024-02-10T19:07:32.675+0000] {processor.py:153} INFO - Started process (PID=123102) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:07:32.678+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:07:32.679+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:32.679+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:07:32.720+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:07:32.767+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:32.767+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:07:32.816+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:32.815+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:07:32.846+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.180 seconds
[2024-02-10T19:07:38.284+0000] {processor.py:153} INFO - Started process (PID=123127) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:07:38.286+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:07:38.295+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:38.288+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:07:38.349+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:07:38.394+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:38.394+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:07:38.465+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:38.465+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:07:38.539+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.271 seconds
[2024-02-10T19:07:43.709+0000] {processor.py:153} INFO - Started process (PID=123164) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:07:43.710+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:07:43.712+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:43.711+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:07:43.747+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:07:43.793+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:43.792+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:07:43.874+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:43.874+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:07:43.921+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.218 seconds
[2024-02-10T19:07:48.980+0000] {processor.py:153} INFO - Started process (PID=123189) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:07:48.981+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:07:48.982+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:48.982+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:07:49.012+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:07:49.057+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:49.057+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:07:49.112+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:49.112+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:07:49.148+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.173 seconds
[2024-02-10T19:07:54.222+0000] {processor.py:153} INFO - Started process (PID=123212) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:07:54.224+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:07:54.226+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:54.225+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:07:54.259+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:07:54.306+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:54.306+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:07:54.384+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:54.384+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:07:54.443+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.229 seconds
[2024-02-10T19:07:59.734+0000] {processor.py:153} INFO - Started process (PID=123246) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:07:59.735+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:07:59.736+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:59.736+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:07:59.758+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:07:59.792+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:59.792+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:07:59.830+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:07:59.830+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:07:59.865+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.136 seconds
[2024-02-10T19:08:05.400+0000] {processor.py:153} INFO - Started process (PID=123269) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:08:05.401+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:08:05.403+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:08:05.402+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:08:05.437+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:08:05.487+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:08:05.486+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:08:05.544+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:08:05.544+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:08:05.589+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.196 seconds
[2024-02-10T19:08:10.763+0000] {processor.py:153} INFO - Started process (PID=123291) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:08:10.765+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:08:10.767+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:08:10.767+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:08:10.803+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:08:10.872+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:08:10.872+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:08:10.983+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:08:10.983+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:08:11.030+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.272 seconds
[2024-02-10T19:08:16.530+0000] {processor.py:153} INFO - Started process (PID=123326) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:08:16.531+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:08:16.533+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:08:16.532+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:08:16.567+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:08:16.615+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:08:16.615+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:08:16.665+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:08:16.664+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:08:16.696+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.172 seconds
[2024-02-10T19:08:22.411+0000] {processor.py:153} INFO - Started process (PID=123351) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:08:22.428+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:08:22.431+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:08:22.430+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:08:22.544+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:08:22.697+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:08:22.697+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:08:22.832+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:08:22.832+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:08:22.898+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.503 seconds
[2024-02-10T19:08:28.608+0000] {processor.py:153} INFO - Started process (PID=123387) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:08:28.612+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:08:28.613+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:08:28.613+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:08:28.646+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:08:28.678+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:08:28.677+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:08:28.719+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:08:28.718+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:08:28.765+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.161 seconds
[2024-02-10T19:08:34.016+0000] {processor.py:153} INFO - Started process (PID=123412) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:08:34.018+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:08:34.019+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:08:34.019+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:08:34.044+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:08:34.074+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:08:34.073+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:08:34.115+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:08:34.115+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:08:34.158+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.151 seconds
[2024-02-10T19:08:39.518+0000] {processor.py:153} INFO - Started process (PID=123437) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:08:39.519+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:08:39.520+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:08:39.520+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:08:39.543+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:08:39.579+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:08:39.578+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:08:39.624+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:08:39.624+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:08:39.660+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.148 seconds
[2024-02-10T19:08:44.971+0000] {processor.py:153} INFO - Started process (PID=123474) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:08:44.973+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:08:44.974+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:08:44.974+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:08:44.999+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:08:45.030+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:08:45.029+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:08:45.062+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:08:45.062+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:08:45.092+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.127 seconds
[2024-02-10T19:08:50.597+0000] {processor.py:153} INFO - Started process (PID=123499) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:08:50.601+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:08:50.602+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:08:50.602+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:08:50.640+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:08:50.686+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:08:50.686+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:08:50.737+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:08:50.737+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:08:50.770+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.180 seconds
[2024-02-10T19:08:55.995+0000] {processor.py:153} INFO - Started process (PID=123537) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:08:55.996+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:08:55.997+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:08:55.997+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:08:56.029+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:08:56.067+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:08:56.067+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:08:56.116+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:08:56.115+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:08:56.148+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.156 seconds
[2024-02-10T19:09:01.363+0000] {processor.py:153} INFO - Started process (PID=123561) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:09:01.365+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:09:01.366+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:01.365+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:09:01.395+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:09:01.425+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:01.425+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:09:01.457+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:01.457+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:09:01.480+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.122 seconds
[2024-02-10T19:09:06.725+0000] {processor.py:153} INFO - Started process (PID=123582) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:09:06.726+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:09:06.727+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:06.727+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:09:06.761+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:09:06.805+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:06.805+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:09:06.858+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:06.858+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:09:06.902+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.183 seconds
[2024-02-10T19:09:12.232+0000] {processor.py:153} INFO - Started process (PID=123619) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:09:12.232+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:09:12.233+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:12.233+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:09:12.261+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:09:12.292+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:12.292+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:09:12.324+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:12.324+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:09:12.345+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.119 seconds
[2024-02-10T19:09:17.439+0000] {processor.py:153} INFO - Started process (PID=123643) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:09:17.440+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:09:17.441+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:17.441+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:09:17.473+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:09:17.513+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:17.513+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:09:17.565+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:17.564+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:09:17.600+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.167 seconds
[2024-02-10T19:09:22.652+0000] {processor.py:153} INFO - Started process (PID=123678) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:09:22.653+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:09:22.654+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:22.654+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:09:22.676+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:09:22.704+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:22.703+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:09:22.735+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:22.735+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:09:22.761+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.115 seconds
[2024-02-10T19:09:28.337+0000] {processor.py:153} INFO - Started process (PID=123703) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:09:28.338+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:09:28.339+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:28.339+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:09:28.370+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:09:28.412+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:28.412+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:09:28.460+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:28.460+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:09:28.482+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.149 seconds
[2024-02-10T19:09:34.485+0000] {processor.py:153} INFO - Started process (PID=123734) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:09:34.497+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:09:34.499+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:34.498+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:09:34.569+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:09:34.643+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:34.643+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:09:34.740+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:34.740+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:09:34.801+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.322 seconds
[2024-02-10T19:09:39.950+0000] {processor.py:153} INFO - Started process (PID=123759) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:09:39.951+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:09:39.951+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:39.951+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:09:39.974+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:09:40.003+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:40.003+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:09:40.034+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:40.034+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:09:40.056+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.111 seconds
[2024-02-10T19:09:45.120+0000] {processor.py:153} INFO - Started process (PID=123784) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:09:45.121+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:09:45.122+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:45.122+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:09:45.143+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:09:45.171+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:45.171+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:09:45.206+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:45.206+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:09:45.266+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.149 seconds
[2024-02-10T19:09:50.345+0000] {processor.py:153} INFO - Started process (PID=123819) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:09:50.346+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:09:50.348+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:50.347+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:09:50.382+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:09:50.423+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:50.423+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:09:50.474+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:50.474+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:09:50.505+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.164 seconds
[2024-02-10T19:09:55.588+0000] {processor.py:153} INFO - Started process (PID=123844) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:09:55.589+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:09:55.589+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:55.589+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:09:55.611+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:09:55.639+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:55.639+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:09:55.670+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:09:55.669+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:09:55.691+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.107 seconds
[2024-02-10T19:10:01.320+0000] {processor.py:153} INFO - Started process (PID=123867) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:10:01.329+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:10:01.331+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:01.330+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:10:01.388+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:10:01.431+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:01.431+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:10:01.486+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:01.486+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:10:01.525+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.232 seconds
[2024-02-10T19:10:06.686+0000] {processor.py:153} INFO - Started process (PID=123904) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:10:06.687+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:10:06.688+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:06.688+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:10:06.711+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:10:06.739+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:06.739+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:10:06.770+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:06.770+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:10:06.803+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.126 seconds
[2024-02-10T19:10:12.042+0000] {processor.py:153} INFO - Started process (PID=123929) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:10:12.042+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:10:12.043+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:12.043+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:10:12.064+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:10:12.092+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:12.091+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:10:12.130+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:12.130+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:10:12.162+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.124 seconds
[2024-02-10T19:10:19.549+0000] {processor.py:153} INFO - Started process (PID=123966) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:10:19.551+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:10:19.552+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:19.551+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:10:19.575+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:10:19.618+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:19.618+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:10:19.664+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:19.664+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:10:19.700+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.166 seconds
[2024-02-10T19:10:24.764+0000] {processor.py:153} INFO - Started process (PID=123987) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:10:24.765+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:10:24.766+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:24.766+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:10:24.797+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:10:24.838+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:24.838+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:10:24.875+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:24.875+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:10:24.906+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.150 seconds
[2024-02-10T19:10:30.619+0000] {processor.py:153} INFO - Started process (PID=124012) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:10:30.620+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:10:30.622+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:30.622+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:10:30.659+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:10:30.708+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:30.708+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:10:30.763+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:30.761+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:10:30.798+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.184 seconds
[2024-02-10T19:10:36.428+0000] {processor.py:153} INFO - Started process (PID=124049) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:10:36.429+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:10:36.430+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:36.430+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:10:36.454+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:10:36.486+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:36.485+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:10:36.518+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:36.518+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:10:36.541+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.121 seconds
[2024-02-10T19:10:42.096+0000] {processor.py:153} INFO - Started process (PID=124074) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:10:42.097+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:10:42.098+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:42.098+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:10:42.129+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:10:42.171+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:42.171+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:10:42.236+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:42.236+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:10:42.289+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.198 seconds
[2024-02-10T19:10:47.857+0000] {processor.py:153} INFO - Started process (PID=124107) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:10:47.858+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:10:47.859+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:47.859+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:10:47.893+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:10:47.936+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:47.936+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:10:47.987+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:47.986+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:10:48.016+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.163 seconds
[2024-02-10T19:10:53.519+0000] {processor.py:153} INFO - Started process (PID=124130) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:10:53.520+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:10:53.521+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:53.521+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:10:53.546+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:10:53.587+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:53.587+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:10:53.637+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:53.636+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:10:53.668+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.154 seconds
[2024-02-10T19:10:59.096+0000] {processor.py:153} INFO - Started process (PID=124154) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:10:59.097+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:10:59.098+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:59.098+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:10:59.130+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:10:59.173+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:59.172+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:10:59.226+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:10:59.225+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:10:59.262+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.172 seconds
[2024-02-10T19:11:04.371+0000] {processor.py:153} INFO - Started process (PID=124186) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:11:04.372+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:11:04.375+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:11:04.374+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:11:04.409+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:11:04.455+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:11:04.455+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:11:04.510+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:11:04.510+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:11:04.549+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.184 seconds
[2024-02-10T19:11:09.756+0000] {processor.py:153} INFO - Started process (PID=124211) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:11:09.758+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:11:09.760+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:11:09.759+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:11:09.790+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:11:09.841+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:11:09.840+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:11:09.886+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:11:09.886+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:11:09.909+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.158 seconds
[2024-02-10T19:11:15.400+0000] {processor.py:153} INFO - Started process (PID=124248) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:11:15.401+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:11:15.401+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:11:15.401+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:11:15.424+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:11:15.456+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:11:15.456+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:11:15.491+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:11:15.491+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:11:15.521+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.125 seconds
[2024-02-10T19:11:20.657+0000] {processor.py:153} INFO - Started process (PID=124273) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:11:20.659+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:11:20.660+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:11:20.660+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:11:20.687+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:11:20.720+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:11:20.720+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:11:20.754+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:11:20.753+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:11:20.777+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.125 seconds
[2024-02-10T19:11:26.255+0000] {processor.py:153} INFO - Started process (PID=124298) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:11:26.260+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:11:26.261+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:11:26.261+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:11:26.295+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:11:26.329+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:11:26.329+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:11:26.363+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:11:26.363+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:11:26.388+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.140 seconds
[2024-02-10T19:11:32.395+0000] {processor.py:153} INFO - Started process (PID=124330) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:11:32.404+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:11:32.405+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:11:32.405+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:11:32.466+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:11:32.514+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:11:32.514+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:11:32.578+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:11:32.578+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:11:32.629+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.240 seconds
[2024-02-10T19:11:38.554+0000] {processor.py:153} INFO - Started process (PID=124355) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:11:38.555+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:11:38.556+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:11:38.556+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:11:38.591+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:11:38.640+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:11:38.640+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:11:38.702+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:11:38.702+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:11:38.739+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.188 seconds
[2024-02-10T19:11:45.057+0000] {processor.py:153} INFO - Started process (PID=124390) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:11:45.059+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:11:45.060+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:11:45.060+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:11:45.100+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:11:45.147+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:11:45.147+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:11:45.199+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:11:45.199+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:11:45.231+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.178 seconds
[2024-02-10T19:11:50.824+0000] {processor.py:153} INFO - Started process (PID=124418) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:11:50.828+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:11:50.831+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:11:50.830+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:11:50.885+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:11:50.930+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:11:50.930+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:11:50.967+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:11:50.967+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:11:50.988+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.176 seconds
[2024-02-10T19:11:56.505+0000] {processor.py:153} INFO - Started process (PID=124442) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:11:56.506+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:11:56.508+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:11:56.507+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:11:56.542+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:11:56.588+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:11:56.588+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:11:56.645+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:11:56.645+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:11:56.683+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.185 seconds
[2024-02-10T19:12:01.906+0000] {processor.py:153} INFO - Started process (PID=124475) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:12:01.907+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:12:01.907+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:01.907+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:12:01.928+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:12:01.962+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:01.962+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:12:02.002+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:02.002+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:12:02.032+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.130 seconds
[2024-02-10T19:12:07.146+0000] {processor.py:153} INFO - Started process (PID=124498) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:12:07.147+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:12:07.148+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:07.148+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:12:07.169+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:12:07.197+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:07.197+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:12:07.231+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:07.230+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:12:07.252+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.110 seconds
[2024-02-10T19:12:12.307+0000] {processor.py:153} INFO - Started process (PID=124520) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:12:12.309+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:12:12.310+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:12.310+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:12:12.348+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:12:12.393+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:12.392+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:12:12.449+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:12.449+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:12:12.481+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.180 seconds
[2024-02-10T19:12:17.701+0000] {processor.py:153} INFO - Started process (PID=124548) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:12:17.702+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:12:17.703+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:17.702+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:12:17.727+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:12:17.766+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:17.765+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:12:17.819+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:17.819+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:12:17.871+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.174 seconds
[2024-02-10T19:12:22.945+0000] {processor.py:153} INFO - Started process (PID=124573) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:12:22.946+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:12:22.947+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:22.947+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:12:22.977+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:12:23.008+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:23.008+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:12:23.043+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:23.043+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:12:23.071+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.132 seconds
[2024-02-10T19:12:28.819+0000] {processor.py:153} INFO - Started process (PID=124610) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:12:28.820+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:12:28.821+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:28.821+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:12:28.853+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:12:28.893+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:28.893+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:12:28.944+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:28.944+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:12:28.979+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.165 seconds
[2024-02-10T19:12:34.064+0000] {processor.py:153} INFO - Started process (PID=124635) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:12:34.065+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:12:34.066+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:34.066+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:12:34.095+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:12:34.135+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:34.134+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:12:34.185+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:34.185+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:12:34.206+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.146 seconds
[2024-02-10T19:12:39.567+0000] {processor.py:153} INFO - Started process (PID=124660) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:12:39.568+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:12:39.569+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:39.569+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:12:39.592+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:12:39.621+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:39.621+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:12:39.651+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:39.651+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:12:39.671+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.107 seconds
[2024-02-10T19:12:45.410+0000] {processor.py:153} INFO - Started process (PID=124692) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:12:45.411+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:12:45.413+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:45.412+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:12:45.439+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:12:45.467+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:45.467+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:12:45.505+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:45.505+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:12:45.533+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.128 seconds
[2024-02-10T19:12:50.656+0000] {processor.py:153} INFO - Started process (PID=124714) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:12:50.657+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:12:50.658+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:50.658+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:12:50.690+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:12:50.741+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:50.741+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:12:50.797+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:50.797+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:12:50.830+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.179 seconds
[2024-02-10T19:12:56.648+0000] {processor.py:153} INFO - Started process (PID=124751) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:12:56.649+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:12:56.650+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:56.650+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:12:56.704+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:12:56.753+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:56.752+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:12:56.809+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:12:56.809+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:12:56.846+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.204 seconds
[2024-02-10T19:13:02.086+0000] {processor.py:153} INFO - Started process (PID=124774) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:13:02.087+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:13:02.088+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:02.088+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:13:02.118+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:13:02.160+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:02.160+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:13:02.209+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:02.209+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:13:02.234+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.154 seconds
[2024-02-10T19:13:07.796+0000] {processor.py:153} INFO - Started process (PID=124797) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:13:07.797+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:13:07.798+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:07.798+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:13:07.839+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:13:07.893+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:07.892+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:13:07.948+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:07.948+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:13:07.979+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.189 seconds
[2024-02-10T19:13:13.008+0000] {processor.py:153} INFO - Started process (PID=124824) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:13:13.009+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:13:13.011+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:13.010+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:13:13.054+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:13:13.120+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:13.120+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:13:13.178+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:13.178+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:13:13.210+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.208 seconds
[2024-02-10T19:13:18.638+0000] {processor.py:153} INFO - Started process (PID=124849) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:13:18.639+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:13:18.640+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:18.640+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:13:18.665+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:13:18.697+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:18.696+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:13:18.735+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:18.734+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:13:18.762+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.128 seconds
[2024-02-10T19:13:23.822+0000] {processor.py:153} INFO - Started process (PID=124874) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:13:23.823+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:13:23.824+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:23.823+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:13:23.846+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:13:23.882+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:23.881+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:13:23.916+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:23.915+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:13:23.941+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.124 seconds
[2024-02-10T19:13:29.346+0000] {processor.py:153} INFO - Started process (PID=124911) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:13:29.349+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:13:29.351+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:29.351+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:13:29.390+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:13:29.446+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:29.445+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:13:29.516+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:29.515+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:13:29.549+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.209 seconds
[2024-02-10T19:13:34.782+0000] {processor.py:153} INFO - Started process (PID=124936) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:13:34.783+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:13:34.783+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:34.783+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:13:34.807+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:13:34.843+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:34.843+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:13:34.877+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:34.876+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:13:34.897+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.119 seconds
[2024-02-10T19:13:41.420+0000] {processor.py:153} INFO - Started process (PID=124959) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:13:41.423+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:13:41.426+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:41.425+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:13:41.494+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:13:41.575+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:41.575+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:13:41.656+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:41.656+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:13:41.707+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.298 seconds
[2024-02-10T19:13:47.597+0000] {processor.py:153} INFO - Started process (PID=124986) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:13:47.598+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:13:47.599+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:47.599+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:13:47.622+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:13:47.652+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:47.652+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:13:47.685+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:47.684+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:13:47.708+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.115 seconds
[2024-02-10T19:13:52.797+0000] {processor.py:153} INFO - Started process (PID=125011) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:13:52.798+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:13:52.799+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:52.799+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:13:52.823+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:13:52.856+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:52.855+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:13:52.888+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:52.888+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:13:52.913+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.122 seconds
[2024-02-10T19:13:58.054+0000] {processor.py:153} INFO - Started process (PID=125041) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:13:58.056+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:13:58.057+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:58.057+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:13:58.092+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:13:58.135+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:58.135+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:13:58.186+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:13:58.185+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:13:58.220+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.172 seconds
[2024-02-10T19:14:03.516+0000] {processor.py:153} INFO - Started process (PID=125061) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:14:03.517+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:14:03.518+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:14:03.518+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:14:03.541+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:14:03.574+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:14:03.574+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:14:03.609+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:14:03.609+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:14:03.641+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.132 seconds
[2024-02-10T19:14:09.146+0000] {processor.py:153} INFO - Started process (PID=125093) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:14:09.147+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:14:09.148+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:14:09.148+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:14:09.170+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:14:09.199+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:14:09.199+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:14:09.248+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:14:09.248+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:14:09.282+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.142 seconds
[2024-02-10T19:14:14.827+0000] {processor.py:153} INFO - Started process (PID=125123) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:14:14.829+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:14:14.830+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:14:14.830+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:14:14.858+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:14:14.889+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:14:14.889+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:14:14.938+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:14:14.937+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:14:14.959+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.140 seconds
[2024-02-10T19:14:20.017+0000] {processor.py:153} INFO - Started process (PID=125148) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:14:20.018+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:14:20.019+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:14:20.019+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:14:20.040+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:14:20.070+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:14:20.070+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:14:20.104+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:14:20.104+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:14:20.127+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.114 seconds
[2024-02-10T19:14:25.651+0000] {processor.py:153} INFO - Started process (PID=125185) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:14:25.663+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:14:25.665+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:14:25.665+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:14:25.740+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:14:25.781+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:14:25.781+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:14:25.834+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:14:25.834+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:14:25.869+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.228 seconds
[2024-02-10T19:14:31.307+0000] {processor.py:153} INFO - Started process (PID=125210) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:14:31.311+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:14:31.311+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:14:31.311+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:14:31.333+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:14:31.370+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:14:31.370+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:14:31.417+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:14:31.416+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:14:31.449+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.146 seconds
[2024-02-10T19:14:37.336+0000] {processor.py:153} INFO - Started process (PID=125235) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:14:37.337+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:14:37.338+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:14:37.337+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:14:37.359+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:14:37.391+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:14:37.391+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:14:37.429+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:14:37.429+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:14:37.452+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.122 seconds
[2024-02-10T19:14:43.089+0000] {processor.py:153} INFO - Started process (PID=125272) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:14:43.090+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:14:43.092+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:14:43.091+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:14:43.157+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:14:43.242+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:14:43.241+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:14:43.301+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:14:43.301+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:14:43.340+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.258 seconds
[2024-02-10T19:14:48.491+0000] {processor.py:153} INFO - Started process (PID=125297) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:14:48.492+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:14:48.493+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:14:48.493+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:14:48.521+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:14:48.553+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:14:48.552+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:14:48.606+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:14:48.606+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:14:48.640+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.153 seconds
[2024-02-10T19:14:53.884+0000] {processor.py:153} INFO - Started process (PID=125326) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:14:53.885+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:14:53.886+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:14:53.886+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:14:53.919+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:14:53.970+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:14:53.970+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:14:54.023+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:14:54.023+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:14:54.065+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.187 seconds
[2024-02-10T19:15:02.419+0000] {processor.py:153} INFO - Started process (PID=125351) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:15:02.420+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:15:02.422+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:15:02.421+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:15:02.459+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:15:02.511+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:15:02.510+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:15:02.566+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:15:02.566+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:15:02.608+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.195 seconds
[2024-02-10T19:15:07.671+0000] {processor.py:153} INFO - Started process (PID=125372) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:15:07.673+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:15:07.674+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:15:07.674+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:15:07.716+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:15:07.769+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:15:07.769+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:15:07.849+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:15:07.849+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:15:07.901+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.236 seconds
[2024-02-10T19:15:13.499+0000] {processor.py:153} INFO - Started process (PID=125397) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:15:13.509+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:15:13.510+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:15:13.510+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:15:13.663+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:15:13.884+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:15:13.883+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:15:14.027+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:15:14.027+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:15:14.078+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.588 seconds
[2024-02-10T19:15:26.187+0000] {processor.py:153} INFO - Started process (PID=125434) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:15:26.188+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:15:26.189+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:15:26.188+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:15:26.492+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:15:26.722+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:15:26.722+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:15:26.846+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:15:26.846+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:15:26.944+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.792 seconds
[2024-02-10T19:15:32.518+0000] {processor.py:153} INFO - Started process (PID=125458) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:15:32.520+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:15:32.522+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:15:32.521+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:15:32.561+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:15:32.620+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:15:32.620+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:15:32.680+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:15:32.679+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:15:32.734+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.227 seconds
[2024-02-10T19:15:41.045+0000] {processor.py:153} INFO - Started process (PID=125490) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:15:41.048+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:15:41.051+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:15:41.050+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:15:41.108+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:15:41.152+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:15:41.152+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:15:41.202+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:15:41.201+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:15:41.745+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.717 seconds
[2024-02-10T19:15:48.252+0000] {processor.py:153} INFO - Started process (PID=125514) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:15:48.262+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:15:48.265+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:15:48.265+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:15:48.414+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:15:48.511+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:15:48.510+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:15:48.658+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:15:48.658+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:15:48.784+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.554 seconds
[2024-02-10T19:15:54.546+0000] {processor.py:153} INFO - Started process (PID=125542) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:15:54.547+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:15:54.548+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:15:54.548+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:15:54.574+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:15:54.608+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:15:54.608+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:15:54.642+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:15:54.642+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:15:54.674+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.138 seconds
[2024-02-10T19:16:00.523+0000] {processor.py:153} INFO - Started process (PID=125576) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:16:00.524+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:16:00.525+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:00.525+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:16:00.629+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:16:00.874+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:00.873+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:16:01.121+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:01.120+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:16:01.283+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.766 seconds
[2024-02-10T19:16:06.419+0000] {processor.py:153} INFO - Started process (PID=125604) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:16:06.423+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:16:06.427+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:06.426+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:16:06.512+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:16:06.560+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:06.560+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:16:06.609+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:06.608+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:16:06.631+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.230 seconds
[2024-02-10T19:16:12.325+0000] {processor.py:153} INFO - Started process (PID=125629) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:16:12.326+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:16:12.327+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:12.327+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:16:12.364+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:16:12.410+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:12.410+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:16:12.459+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:12.459+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:16:12.489+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.172 seconds
[2024-02-10T19:16:17.560+0000] {processor.py:153} INFO - Started process (PID=125653) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:16:17.561+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:16:17.563+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:17.563+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:16:17.642+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:16:17.685+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:17.685+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:16:17.737+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:17.737+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:16:17.805+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.258 seconds
[2024-02-10T19:16:22.929+0000] {processor.py:153} INFO - Started process (PID=125679) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:16:22.930+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:16:22.931+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:22.931+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:16:22.954+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:16:22.987+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:22.986+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:16:23.021+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:23.021+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:16:23.066+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.140 seconds
[2024-02-10T19:16:28.669+0000] {processor.py:153} INFO - Started process (PID=125702) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:16:28.671+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:16:28.672+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:28.672+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:16:28.699+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:16:28.733+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:28.732+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:16:28.794+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:28.794+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:16:28.856+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.193 seconds
[2024-02-10T19:16:34.621+0000] {processor.py:153} INFO - Started process (PID=125739) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:16:34.622+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:16:34.623+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:34.623+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:16:34.704+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:16:34.761+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:34.760+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:16:34.813+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:34.813+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:16:34.864+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.250 seconds
[2024-02-10T19:16:40.447+0000] {processor.py:153} INFO - Started process (PID=125764) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:16:40.448+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:16:40.450+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:40.449+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:16:40.483+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:16:40.534+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:40.534+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:16:40.601+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:40.601+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:16:40.678+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.236 seconds
[2024-02-10T19:16:46.072+0000] {processor.py:153} INFO - Started process (PID=125789) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:16:46.077+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:16:46.079+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:46.078+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:16:46.134+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:16:46.174+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:46.174+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:16:46.237+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:46.237+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:16:46.277+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.223 seconds
[2024-02-10T19:16:51.567+0000] {processor.py:153} INFO - Started process (PID=125824) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:16:51.573+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:16:51.574+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:51.573+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:16:51.644+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:16:51.691+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:51.691+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:16:51.748+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:51.747+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:16:51.783+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.233 seconds
[2024-02-10T19:16:56.884+0000] {processor.py:153} INFO - Started process (PID=125849) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:16:56.885+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:16:56.886+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:56.885+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:16:56.909+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:16:56.950+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:56.949+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:16:57.000+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:16:57.000+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:16:57.034+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.154 seconds
[2024-02-10T19:17:02.180+0000] {processor.py:153} INFO - Started process (PID=125877) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:17:02.182+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:17:02.183+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:02.183+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:17:02.216+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:17:02.258+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:02.258+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:17:02.309+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:02.309+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:17:02.343+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.168 seconds
[2024-02-10T19:17:08.264+0000] {processor.py:153} INFO - Started process (PID=125907) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:17:08.265+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:17:08.266+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:08.266+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:17:08.300+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:17:08.335+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:08.335+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:17:08.368+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:08.368+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:17:08.394+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.137 seconds
[2024-02-10T19:17:13.748+0000] {processor.py:153} INFO - Started process (PID=125932) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:17:13.749+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:17:13.750+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:13.750+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:17:13.771+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:17:13.802+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:13.802+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:17:13.836+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:13.835+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:17:13.867+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.124 seconds
[2024-02-10T19:17:19.078+0000] {processor.py:153} INFO - Started process (PID=125969) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:17:19.079+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:17:19.080+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:19.080+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:17:19.111+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:17:19.155+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:19.154+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:17:19.207+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:19.207+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:17:19.240+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.168 seconds
[2024-02-10T19:17:24.667+0000] {processor.py:153} INFO - Started process (PID=125993) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:17:24.669+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:17:24.669+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:24.669+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:17:24.692+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:17:24.725+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:24.725+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:17:24.775+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:24.775+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:17:24.808+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.144 seconds
[2024-02-10T19:17:29.935+0000] {processor.py:153} INFO - Started process (PID=126013) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:17:29.936+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:17:29.938+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:29.938+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:17:29.986+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:17:30.043+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:30.043+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:17:30.118+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:30.117+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:17:30.162+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.239 seconds
[2024-02-10T19:17:35.391+0000] {processor.py:153} INFO - Started process (PID=126049) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:17:35.392+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:17:35.392+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:35.392+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:17:35.417+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:17:35.446+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:35.446+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:17:35.481+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:35.481+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:17:35.521+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.134 seconds
[2024-02-10T19:17:41.416+0000] {processor.py:153} INFO - Started process (PID=126074) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:17:41.417+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:17:41.418+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:41.417+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:17:41.445+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:17:41.490+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:41.489+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:17:41.544+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:41.544+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:17:41.578+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.166 seconds
[2024-02-10T19:17:46.889+0000] {processor.py:153} INFO - Started process (PID=126105) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:17:46.890+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:17:46.892+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:46.891+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:17:46.953+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:17:47.002+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:47.002+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:17:47.110+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:47.110+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:17:47.166+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.284 seconds
[2024-02-10T19:17:52.335+0000] {processor.py:153} INFO - Started process (PID=126130) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:17:52.336+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:17:52.337+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:52.337+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:17:52.366+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:17:52.409+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:52.409+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:17:52.451+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:52.451+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:17:52.483+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.153 seconds
[2024-02-10T19:17:57.576+0000] {processor.py:153} INFO - Started process (PID=126155) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:17:57.580+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:17:57.582+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:57.581+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:17:57.610+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:17:57.640+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:57.640+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:17:57.672+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:17:57.672+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:17:57.699+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.137 seconds
[2024-02-10T19:18:04.042+0000] {processor.py:153} INFO - Started process (PID=126192) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:18:04.046+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:18:04.048+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:18:04.048+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:18:04.098+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:18:04.147+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:18:04.147+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:18:04.203+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:18:04.201+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:18:04.251+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.227 seconds
[2024-02-10T19:18:09.369+0000] {processor.py:153} INFO - Started process (PID=126216) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:18:09.370+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:18:09.370+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:18:09.370+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:18:09.403+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:18:09.442+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:18:09.442+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:18:09.488+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:18:09.487+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:18:09.520+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.157 seconds
[2024-02-10T19:18:15.083+0000] {processor.py:153} INFO - Started process (PID=126234) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:18:15.084+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:18:15.086+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:18:15.086+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:18:15.130+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:18:15.185+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:18:15.184+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:18:15.248+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:18:15.248+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:18:15.287+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.212 seconds
[2024-02-10T19:18:21.265+0000] {processor.py:153} INFO - Started process (PID=126270) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:18:21.271+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:18:21.272+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:18:21.272+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:18:21.317+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:18:21.377+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:18:21.377+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:18:21.436+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:18:21.436+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:18:21.476+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.217 seconds
[2024-02-10T19:18:26.557+0000] {processor.py:153} INFO - Started process (PID=126292) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:18:26.558+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:18:26.559+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:18:26.559+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:18:26.581+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:18:26.610+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:18:26.609+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:18:26.644+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:18:26.643+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:18:26.665+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.113 seconds
[2024-02-10T19:18:33.544+0000] {processor.py:153} INFO - Started process (PID=126325) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:18:33.546+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:18:33.549+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:18:33.549+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:18:33.583+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:18:33.628+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:18:33.627+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:18:33.684+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:18:33.684+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:18:33.722+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.183 seconds
[2024-02-10T19:18:38.835+0000] {processor.py:153} INFO - Started process (PID=126351) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:18:38.836+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:18:38.838+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:18:38.837+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:18:38.864+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:18:38.899+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:18:38.899+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:18:38.943+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:18:38.942+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:18:38.980+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.151 seconds
[2024-02-10T19:18:44.447+0000] {processor.py:153} INFO - Started process (PID=126372) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:18:44.448+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:18:44.450+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:18:44.449+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:18:44.486+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:18:44.540+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:18:44.539+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:18:44.602+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:18:44.602+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:18:44.638+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.197 seconds
[2024-02-10T19:18:50.055+0000] {processor.py:153} INFO - Started process (PID=126404) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:18:50.056+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:18:50.058+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:18:50.057+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:18:50.096+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:18:50.140+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:18:50.140+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:18:50.215+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:18:50.215+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:18:50.279+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.232 seconds
[2024-02-10T19:18:55.713+0000] {processor.py:153} INFO - Started process (PID=126429) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:18:55.714+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:18:55.715+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:18:55.715+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:18:55.742+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:18:55.776+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:18:55.776+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:18:55.818+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:18:55.818+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:18:55.846+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.140 seconds
[2024-02-10T19:19:01.727+0000] {processor.py:153} INFO - Started process (PID=126454) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:19:01.728+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:19:01.729+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:01.729+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:19:01.758+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:19:01.795+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:01.795+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:19:01.850+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:01.849+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:19:01.888+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.168 seconds
[2024-02-10T19:19:07.449+0000] {processor.py:153} INFO - Started process (PID=126491) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:19:07.451+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:19:07.452+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:07.452+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:19:07.487+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:19:07.534+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:07.533+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:19:07.605+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:07.605+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:19:07.669+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.226 seconds
[2024-02-10T19:19:13.376+0000] {processor.py:153} INFO - Started process (PID=126516) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:19:13.379+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:19:13.380+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:13.380+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:19:13.410+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:19:13.458+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:13.458+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:19:13.504+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:13.504+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:19:13.539+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.170 seconds
[2024-02-10T19:19:18.647+0000] {processor.py:153} INFO - Started process (PID=126551) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:19:18.648+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:19:18.657+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:18.657+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:19:18.710+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:19:18.770+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:18.770+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:19:18.830+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:18.829+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:19:18.873+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.235 seconds
[2024-02-10T19:19:24.191+0000] {processor.py:153} INFO - Started process (PID=126579) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:19:24.196+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:19:24.200+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:24.199+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:19:24.244+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:19:24.288+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:24.287+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:19:24.344+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:24.344+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:19:24.378+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.205 seconds
[2024-02-10T19:19:30.245+0000] {processor.py:153} INFO - Started process (PID=126604) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:19:30.247+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:19:30.248+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:30.248+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:19:30.284+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:19:30.334+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:30.333+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:19:30.399+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:30.399+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:19:30.430+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.193 seconds
[2024-02-10T19:19:35.736+0000] {processor.py:153} INFO - Started process (PID=126637) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:19:35.737+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:19:35.738+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:35.738+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:19:35.772+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:19:35.819+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:35.819+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:19:35.880+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:35.880+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:19:35.921+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.191 seconds
[2024-02-10T19:19:41.195+0000] {processor.py:153} INFO - Started process (PID=126660) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:19:41.196+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:19:41.197+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:41.197+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:19:41.240+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:19:41.289+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:41.289+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:19:41.343+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:41.343+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:19:41.376+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.190 seconds
[2024-02-10T19:19:46.585+0000] {processor.py:153} INFO - Started process (PID=126685) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:19:46.587+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:19:46.588+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:46.588+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:19:46.622+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:19:46.666+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:46.665+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:19:46.712+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:46.712+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:19:46.759+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.180 seconds
[2024-02-10T19:19:51.840+0000] {processor.py:153} INFO - Started process (PID=126719) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:19:51.842+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:19:51.843+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:51.843+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:19:51.877+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:19:51.914+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:51.914+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:19:51.958+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:51.957+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:19:51.993+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.161 seconds
[2024-02-10T19:19:57.099+0000] {processor.py:153} INFO - Started process (PID=126740) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:19:57.108+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:19:57.112+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:57.109+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:19:57.173+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:19:57.221+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:57.221+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:19:57.264+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:19:57.264+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:19:57.303+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.211 seconds
[2024-02-10T19:20:02.380+0000] {processor.py:153} INFO - Started process (PID=126765) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:20:02.381+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:20:02.382+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:02.382+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:20:02.423+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:20:02.473+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:02.473+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:20:02.527+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:02.527+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:20:02.579+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.207 seconds
[2024-02-10T19:20:08.106+0000] {processor.py:153} INFO - Started process (PID=126802) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:20:08.107+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:20:08.108+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:08.108+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:20:08.136+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:20:08.173+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:08.173+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:20:08.218+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:08.218+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:20:08.262+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.163 seconds
[2024-02-10T19:20:13.851+0000] {processor.py:153} INFO - Started process (PID=126825) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:20:13.856+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:20:13.868+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:13.868+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:20:13.911+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:20:13.955+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:13.955+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:20:14.028+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:14.028+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:20:14.084+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.239 seconds
[2024-02-10T19:20:19.436+0000] {processor.py:153} INFO - Started process (PID=126859) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:20:19.442+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:20:19.445+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:19.445+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:20:19.480+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:20:19.519+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:19.519+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:20:19.555+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:19.555+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:20:19.592+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.156 seconds
[2024-02-10T19:20:24.846+0000] {processor.py:153} INFO - Started process (PID=126887) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:20:24.847+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:20:24.848+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:24.848+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:20:24.876+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:20:24.919+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:24.919+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:20:24.964+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:24.964+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:20:25.004+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.165 seconds
[2024-02-10T19:20:30.936+0000] {processor.py:153} INFO - Started process (PID=126908) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:20:30.938+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:20:30.939+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:30.939+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:20:30.978+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:20:31.080+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:31.079+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:20:31.177+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:31.176+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:20:31.266+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.336 seconds
[2024-02-10T19:20:36.915+0000] {processor.py:153} INFO - Started process (PID=126943) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:20:36.917+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:20:36.918+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:36.918+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:20:36.965+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:20:37.011+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:37.011+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:20:37.068+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:37.067+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:20:37.099+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.190 seconds
[2024-02-10T19:20:42.626+0000] {processor.py:153} INFO - Started process (PID=126966) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:20:42.627+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:20:42.628+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:42.628+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:20:42.664+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:20:42.706+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:42.706+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:20:42.746+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:42.745+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:20:42.775+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.156 seconds
[2024-02-10T19:20:48.296+0000] {processor.py:153} INFO - Started process (PID=126999) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:20:48.298+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:20:48.299+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:48.299+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:20:48.334+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:20:48.379+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:48.378+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:20:48.436+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:48.436+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:20:48.477+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.189 seconds
[2024-02-10T19:20:53.886+0000] {processor.py:153} INFO - Started process (PID=127023) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:20:53.888+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:20:53.889+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:53.889+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:20:53.921+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:20:53.953+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:53.953+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:20:54.006+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:54.006+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:20:54.049+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.170 seconds
[2024-02-10T19:20:59.746+0000] {processor.py:153} INFO - Started process (PID=127048) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:20:59.750+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:20:59.751+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:59.751+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:20:59.777+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:20:59.816+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:59.816+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:20:59.861+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:20:59.861+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:20:59.908+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.169 seconds
[2024-02-10T19:21:05.767+0000] {processor.py:153} INFO - Started process (PID=127085) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:21:05.769+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:21:05.770+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:21:05.770+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:21:05.805+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:21:05.850+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:21:05.850+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:21:05.902+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:21:05.902+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:21:05.937+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.177 seconds
[2024-02-10T19:21:11.376+0000] {processor.py:153} INFO - Started process (PID=127110) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:21:11.379+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:21:11.380+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:21:11.380+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:21:11.410+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:21:11.450+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:21:11.450+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:21:11.526+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:21:11.526+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:21:11.557+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.186 seconds
[2024-02-10T19:21:17.533+0000] {processor.py:153} INFO - Started process (PID=127144) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:21:17.534+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:21:17.535+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:21:17.535+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:21:17.558+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:21:17.598+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:21:17.598+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:21:17.648+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:21:17.648+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:21:18.061+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.532 seconds
[2024-02-10T19:21:23.123+0000] {processor.py:153} INFO - Started process (PID=127171) to work on /opt/airflow/dags/sensors.py
[2024-02-10T19:21:23.124+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/sensors.py for tasks to queue
[2024-02-10T19:21:23.134+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:21:23.133+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/sensors.py
[2024-02-10T19:21:23.209+0000] {processor.py:753} INFO - DAG(s) dict_keys(['httpsensor']) retrieved from /opt/airflow/dags/sensors.py
[2024-02-10T19:21:23.255+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:21:23.254+0000] {dag.py:2690} INFO - Sync 1 DAGs
[2024-02-10T19:21:23.322+0000] {logging_mixin.py:137} INFO - [2024-02-10T19:21:23.322+0000] {dag.py:3441} INFO - Setting next_dagrun for httpsensor to None, run_after=None
[2024-02-10T19:21:23.360+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/sensors.py took 0.243 seconds
